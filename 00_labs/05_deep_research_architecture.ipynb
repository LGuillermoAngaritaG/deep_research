{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7a6c87e",
   "metadata": {},
   "source": [
    "# Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c12d98",
   "metadata": {},
   "source": [
    "Create a deep research agent using a planner, a researcher and a writer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8fcf9b",
   "metadata": {},
   "source": [
    "# Web"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b600ccf1",
   "metadata": {},
   "source": [
    "Use Tavily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c53afff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic_settings import BaseSettings\n",
    "import os\n",
    "from pydantic_ai import RunContext\n",
    "from pydantic_ai.mcp import MCPServerStreamableHTTP\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"pydantic\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"asyncio\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "logger = logging.getLogger(\"web_deep_researcher\")\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    GOOGLE_API_KEY: str\n",
    "    MODEL_NAME: str = \"google-gla:gemini-2.5-pro\"\n",
    "    CONTEXT7_API_KEY: str\n",
    "    TAVILY_API_KEY: str\n",
    "    class Config:\n",
    "        #ignore extra fields\n",
    "        extra = \"ignore\"\n",
    "        env_file = \".env\"\n",
    "        \n",
    "class PlanOutput(BaseModel):\n",
    "    outline: str = Field(description=\"The outline of the report\")\n",
    "    \n",
    "class GetHumanInTheLoopInput(BaseModel):\n",
    "    \"\"\"Input for getting more information\"\"\"\n",
    "    questions: str\n",
    "\n",
    "class GetHumanInTheLoopOutput(BaseModel):\n",
    "    \"\"\"Response for getting more information\"\"\"\n",
    "    answer: str\n",
    "    \n",
    "class GetPlanInput(BaseModel):\n",
    "    \"\"\"Input for getting a plan\"\"\"\n",
    "    instructions_for_plan: str\n",
    "\n",
    "class GetPlanOutput(BaseModel):\n",
    "    \"\"\"Response for getting a plan\"\"\"\n",
    "    plan: str\n",
    "    \n",
    "class reportOutput(BaseModel):\n",
    "    report: str\n",
    "    \n",
    "class Section(BaseModel):\n",
    "    title: str = Field(description=\"The title/subtitle of the section\")\n",
    "    content: str = Field(description=\"The content of the section, that answers the title of the section\")\n",
    "    references: str = Field(description=\"The references of the section, that are used to answer the title of the section\")\n",
    "\n",
    "class documentationOutput(BaseModel):\n",
    "    sections: list[Section] = Field(description=\"The sections of the report, that are used to answer the outline\")\n",
    "    \n",
    "class web_deep_researcher:\n",
    "    def __init__(self):\n",
    "        self.settings = Settings()\n",
    "        os.environ[\"GOOGLE_API_KEY\"] = self.settings.GOOGLE_API_KEY\n",
    "    \n",
    "    async def execute(self, question:str):\n",
    "        # Create the plan\n",
    "        logger.info(f\"Creating the plan for the question: {question}\")\n",
    "        plan = await self.plan(question)\n",
    "        # Get the information\n",
    "        logger.info(f\"Getting the information for the plan: {plan}\")\n",
    "        information = await self.get_information(plan)\n",
    "        # Get the report\n",
    "        logger.info(f\"Getting the report for the information: {information}\")\n",
    "        report = await self.get_report(information, plan)\n",
    "        return report\n",
    "    # Define the Planning Agent\n",
    "    async def plan(self, question:str):\n",
    "        planning_agent = Agent(\n",
    "            model=self.settings.MODEL_NAME,\n",
    "            system_prompt=\"\"\"You are a planning assistant. Your Job is to create an outline for a report for an user. You have access to tools to help you answer questions.\n",
    "1. Assess which tool you should use to answer the question.\n",
    "2. Use get_human_in_the_loop to get user input in case of ambiguity. Provide your question.\n",
    "3. Use create_plan to create a plan based on what you understand about what the user is asking for. The default outline is the following:\n",
    "    - Summary \n",
    "    - Introduction\n",
    "    - Sections and subsections\n",
    "    - Conclusion\n",
    "    - References\n",
    "4. If you think the question is too complex or not relevant, first use get_human_in_the_loop to get user input. if user input is ambiguous, then respond with 'I don't know how to help you with that'.\n",
    "5. After making the plan, use human_in_the_loop to ask the user if the plan looks good, if the user asks for changes, use_create_plan with the changes made by the user. If the user is happy with the plan, respond with the final plan.\"\"\",\n",
    "            output_type=PlanOutput\n",
    "        )\n",
    "        # Tools\n",
    "        @planning_agent.tool\n",
    "        async def get_plan(_: RunContext[GetPlanInput], instructions_for_plan: str) -> GetPlanOutput:\n",
    "            outline_agent = Agent(self.settings.MODEL_NAME,\n",
    "                        instructions=\"You are an experienced planner for a research project, you will be given a query from the user and you need to make an outline of the report to give the user about it, and what to search for in the web in order to create the report\",\n",
    "                        output_type=GetPlanOutput)\n",
    "            result = await outline_agent.run(instructions_for_plan)\n",
    "            return GetPlanOutput(plan=result.output.plan)\n",
    "        \n",
    "        @planning_agent.tool\n",
    "        def get_human_in_the_loop(_: RunContext[GetHumanInTheLoopInput], question: str) -> GetHumanInTheLoopOutput:\n",
    "            input_str = input(f\"Please provide your input for the question > '{question}': \")\n",
    "            return GetHumanInTheLoopOutput(answer=input_str)\n",
    "        \n",
    "        nodes = []\n",
    "        # Run the Planning Agent\n",
    "        async with planning_agent.iter(\n",
    "            question,\n",
    "        ) as agent_run:\n",
    "            async for node in agent_run:\n",
    "                # Each node represents a step in the agent's execution\n",
    "                logger.info(f\"Node: {node}\")\n",
    "                nodes.append(node)\n",
    "                \n",
    "        result = nodes[-1].data.output.outline\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    # Define the Research Agent\n",
    "    async def get_information(self, plan:str):\n",
    "        tavily_server = MCPServerStreamableHTTP(\n",
    "            url =f'https://mcp.tavily.com/mcp/?tavilyApiKey={self.settings.TAVILY_API_KEY}'\n",
    "        )\n",
    "        mcp_agent = Agent(self.settings.MODEL_NAME, toolsets=[tavily_server], \n",
    "            instructions=\"\"\"You are a helpful AI assistant tasked with getting content for a comprehensive report based on a given outline. Your goal is to research and compile current information for each section and subsection of the outline using the DuckDuckGo MCP server. Follow these steps to complete the task:\n",
    "\n",
    "                1. Review the provided outline\n",
    "\n",
    "                2. For each section and subsection in the outline:\n",
    "                a. Use the DuckDuckGo MCP server to search for current information related to the topic.\n",
    "                b. Review the search results and identify the most relevant and reliable sources.\n",
    "                c. After fetching the content, analyze and summarize the information relevant to the outline section.\n",
    "\n",
    "                3. Write the report section by section, following the structure of the outline. For each section:\n",
    "                a. Provide a brief introduction to the topic.\n",
    "                b. Include relevant information gathered from your research.\n",
    "                c. Ensure that the content is well-organized and flows logically.\n",
    "                d. Use appropriate transitions between subsections and main sections.\n",
    "\n",
    "                4. Throughout the writing process, keep track of the sources you've used. For each source, note:\n",
    "                - The title of the webpage or article\n",
    "                - The author (if available)\n",
    "                - The website name\n",
    "                - The URL\n",
    "                - The date you accessed the information\n",
    "                \"\"\",\n",
    "                    output_type=documentationOutput\n",
    "            )\n",
    "        # Run the Research Agent\n",
    "        async with mcp_agent:  \n",
    "            nodes = []\n",
    "            async with mcp_agent.iter(\n",
    "                f\"The outline of the report is: {plan}\",\n",
    "            ) as agent_run:\n",
    "                async for node in agent_run:\n",
    "                    # Each node represents a step in the agent's execution\n",
    "                    logger.info(f\"Node: {node}\")\n",
    "                    nodes.append(node)\n",
    "        result = nodes[-1].data.output.sections\n",
    "        return result\n",
    "    \n",
    "    # Run the Writer Agent\n",
    "    async def get_report(self, information:list[Section], plan:str):\n",
    "        writer_agent = Agent(self.settings.MODEL_NAME,\n",
    "                instructions=\"You are a writer, you write a report about a given outline, you use the information provided as well as your own knowledge to write the report\",\n",
    "                        output_type=reportOutput\n",
    "        )\n",
    "        result = await writer_agent.run(f\"\"\"Write a report about the following outline: {plan}\n",
    "        A Researcher has found the following information for each section: {information}\n",
    "        Please use this to create a comprehesive report\"\"\")\n",
    "        return result.output.report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cd4b62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:web_deep_researcher:Creating the plan for the question: how can I use pydantic-ai\n",
      "INFO:web_deep_researcher:Node: UserPromptNode(user_prompt='how can I use pydantic-ai', instructions=None, instructions_functions=[], system_prompts=(\"You are a planning assistant. Your Job is to create an outline for a report for an user. You have access to tools to help you answer questions.\\n        1. Assess which tool you should use to answer the question.\\n        2. Use get_human_in_the_loop to get user input in case of ambiguity. Provide your question.\\n        3. Use create_plan to create a plan based on what you understand about what the user is asking for. The default outline is the following:\\n            - Summary \\n            - Introduction\\n            - Sections and subsections\\n            - Conclusion\\n            - References\\n        4. If you think the question is too complex or not relevant, first use get_human_in_the_loop to get user input. if user input is ambiguous, then respond with 'I don't know how to help you with that'.\\n        Finally, respond once you have a final answer.\",), system_prompt_functions=[], system_prompt_dynamic_functions={})\n",
      "INFO:web_deep_researcher:Node: ModelRequestNode(request=ModelRequest(parts=[SystemPromptPart(content=\"You are a planning assistant. Your Job is to create an outline for a report for an user. You have access to tools to help you answer questions.\\n        1. Assess which tool you should use to answer the question.\\n        2. Use get_human_in_the_loop to get user input in case of ambiguity. Provide your question.\\n        3. Use create_plan to create a plan based on what you understand about what the user is asking for. The default outline is the following:\\n            - Summary \\n            - Introduction\\n            - Sections and subsections\\n            - Conclusion\\n            - References\\n        4. If you think the question is too complex or not relevant, first use get_human_in_the_loop to get user input. if user input is ambiguous, then respond with 'I don't know how to help you with that'.\\n        Finally, respond once you have a final answer.\", timestamp=datetime.datetime(2025, 9, 3, 22, 28, 38, 967974, tzinfo=datetime.timezone.utc)), UserPromptPart(content='how can I use pydantic-ai', timestamp=datetime.datetime(2025, 9, 3, 22, 28, 38, 967983, tzinfo=datetime.timezone.utc))]))\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:web_deep_researcher:Node: CallToolsNode(model_response=ModelResponse(parts=[ToolCallPart(tool_name='get_human_in_the_loop', args={'question': \"It looks like you're asking a question about how to use 'pydantic-ai'. As a planning assistant, I can help you create an outline for a report. Would you like me to create an outline for a report about 'pydantic-ai'?\"}, tool_call_id='pyd_ai_d7e04a568a4947c9b75229e42bd96336')], usage=RequestUsage(input_tokens=326, output_tokens=308, details={'thoughts_tokens': 231, 'text_prompt_tokens': 326}), model_name='gemini-2.5-flash', timestamp=datetime.datetime(2025, 9, 3, 22, 28, 41, 320928, tzinfo=datetime.timezone.utc), provider_name='google-gla', provider_details={'finish_reason': 'STOP'}, provider_response_id='GcG4aLWgHbXUz7IPwp30-Ac'))\n",
      "INFO:web_deep_researcher:Node: ModelRequestNode(request=ModelRequest(parts=[ToolReturnPart(tool_name='get_human_in_the_loop', content=GetHumanInTheLoopOutput(answer='yes'), tool_call_id='pyd_ai_d7e04a568a4947c9b75229e42bd96336', timestamp=datetime.datetime(2025, 9, 3, 22, 28, 48, 783594, tzinfo=datetime.timezone.utc))]))\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:web_deep_researcher:Node: CallToolsNode(model_response=ModelResponse(parts=[ToolCallPart(tool_name='get_plan', args={'instructions_for_plan': 'Create an outline for a report about pydantic-ai. The outline should include a Summary, Introduction, Sections and subsections, Conclusion, and References.'}, tool_call_id='pyd_ai_d077cacabbfe4346ace49f782e059ffa')], usage=RequestUsage(input_tokens=430, output_tokens=211, details={'thoughts_tokens': 163, 'text_prompt_tokens': 430}), model_name='gemini-2.5-flash', timestamp=datetime.datetime(2025, 9, 3, 22, 28, 50, 599710, tzinfo=datetime.timezone.utc), provider_name='google-gla', provider_details={'finish_reason': 'STOP'}, provider_response_id='IsG4aM-CLdqeqtsPvOPayQ8'))\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:web_deep_researcher:Node: ModelRequestNode(request=ModelRequest(parts=[ToolReturnPart(tool_name='get_plan', content=GetPlanOutput(plan='**Report Outline: Pydantic in AI Applications**\\n\\n**1. Summary**\\n   * Brief overview of Pydantic\\'s role in modern AI development.\\n   * Key benefits of integrating Pydantic for data validation and structuring in AI systems.\\n   * Highlights of use cases and practical implications.\\n\\n**2. Introduction**\\n   * **2.1 What is Pydantic?**\\n     * Overview of Pydantic as a data validation and settings management library.\\n     * Core features: type hints, data parsing, serialization, runtime validation.\\n   * **2.2 The Challenge of Data in AI Systems**\\n     * Importance of data integrity and structure in AI/ML workflows.\\n     * Common issues: unstructured inputs/outputs, schema mismatches, debugging complexity.\\n   * **2.3 Why Pydantic for AI?**\\n     * Introduction to how Pydantic addresses these challenges.\\n     * Scope and objectives of the report.\\n\\n**3. Pydantic Fundamentals Relevant to AI**\\n   * **3.1 Data Models and Type Hinting**\\n     * Defining data structures with Python type hints.\\n     * Benefits for clarity and maintainability.\\n   * **3.2 Validation and Error Handling**\\n     * Automatic data validation on assignment and instantiation.\\n     * Custom validators and error reporting.\\n   * **3.3 Serialization and Deserialization**\\n     * Converting data models to/from JSON, dictionaries, etc.\\n     * Usefulness for API interactions and data persistence.\\n   * **3.4 Schema Generation (JSON Schema)**\\n     * Automatic generation of JSON schemas from Pydantic models.\\n     * Interoperability with other systems and documentation.\\n\\n**4. Pydantic\\'s Role in AI Data Management and Workflows**\\n   * **4.1 Input Validation for AI Models**\\n     * Ensuring quality and correctness of data fed into machine learning models.\\n     * Handling diverse data types (text, numerical, categorical) with structured validation.\\n   * **4.2 Structuring and Validating AI Model Outputs**\\n     * Enforcing specific output formats for predictions, classifications, and generated content.\\n     * Improving downstream processing and reliability of AI applications.\\n   * **4.3 Pydantic in Large Language Model (LLM) Applications**\\n     * **4.3.1 Enforcing Structured Output from LLMs:** How Pydantic helps LLMs generate reliable JSON or other structured data.\\n     * **4.3.2 Defining Tools and Functions for LLM Agents:** Using Pydantic to describe tool inputs/outputs for function calling (e.g., OpenAI API).\\n     * **4.3.3 Validating LLM Prompts and Responses:** Ensuring adherence to expected formats.\\n   * **4.4 Data Schemas for MLOps and Data Pipelines**\\n     * Defining clear contracts for data flow between different stages of an ML pipeline.\\n     * Improving data governance and reproducibility.\\n\\n**5. Practical Applications and Use Cases**\\n   * **5.1 Building Robust AI APIs**\\n     * Using Pydantic for request and response validation in FastAPI, Flask, etc.\\n   * **5.2 Enhancing Data Ingestion for Training Data**\\n     * Validating and cleaning raw data before model training.\\n   * **5.3 Developing Reliable AI Agents and Chatbots**\\n     * Structuring conversational states, user inputs, and agent actions.\\n   * **5.4 Integrating Pydantic with Popular AI Frameworks/Libraries**\\n     * Examples with LangChain, LlamaIndex, OpenAI Python client, etc.\\n\\n**6. Benefits and Challenges of Pydantic-AI Integration**\\n   * **6.1 Key Benefits**\\n     * Increased reliability and robustness of AI systems.\\n     * Improved developer experience and productivity.\\n     * Better code readability, maintainability, and collaboration.\\n     * Reduced debugging time and fewer runtime errors.\\n     * Enhanced API design and documentation.\\n   * **6.2 Potential Challenges and Considerations**\\n     * Learning curve for Pydantic (especially for those new to type hints).\\n     * Performance considerations for very high-throughput systems (if complex validation).\\n     * Keeping up with Pydantic\\'s evolving features (e.g., V1 vs. V2).\\n\\n**7. Conclusion**\\n   * Recap of Pydantic\\'s critical role in building robust, reliable, and maintainable AI applications.\\n   * Future outlook for structured data and type safety in the evolving AI landscape.\\n   * Final recommendations for developers leveraging AI.\\n\\n**8. References**\\n   * List of academic papers, articles, official documentation, and relevant code repositories consulted.\\n\\n**Web Search Queries for Report Creation:**\\n*   \"pydantic for AI applications\"\\n*   \"pydantic LLM structured output\"\\n*   \"pydantic function calling OpenAI\"\\n*   \"pydantic in LangChain\"\\n*   \"pydantic data validation machine learning\"\\n*   \"pydantic and AI agent tools\"\\n*   \"pydantic FastAPI AI APIs\"\\n*   \"benefits of pydantic in AI development\"\\n*   \"pydantic V2 AI features\"\\n*   \"how pydantic improves AI system reliability\"'), tool_call_id='pyd_ai_d077cacabbfe4346ace49f782e059ffa', timestamp=datetime.datetime(2025, 9, 3, 22, 29, 3, 324935, tzinfo=datetime.timezone.utc))]))\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:web_deep_researcher:Node: CallToolsNode(model_response=ModelResponse(parts=[ToolCallPart(tool_name='final_result', args={'outline': '**Report Outline: Pydantic in AI Applications**\\n\\n**1. Summary**\\n   * Brief overview of Pydantic\\'s role in modern AI development.\\n   * Key benefits of integrating Pydantic for data validation and structuring in AI systems.\\n   * Highlights of use cases and practical implications.\\n\\n**2. Introduction**\\n   * **2.1 What is Pydantic?**\\n     * Overview of Pydantic as a data validation and settings management library.\\n     * Core features: type hints, data parsing, serialization, runtime validation.\\n   * **2.2 The Challenge of Data in AI Systems**\\n     * Importance of data integrity and structure in AI/ML workflows.\\n     * Common issues: unstructured inputs/outputs, schema mismatches, debugging complexity.\\n   * **2.3 Why Pydantic for AI?**\\n     * Introduction to how Pydantic addresses these challenges.\\n     * Scope and objectives of the report.\\n\\n**3. Pydantic Fundamentals Relevant to AI**\\n   * **3.1 Data Models and Type Hinting**\\n     * Defining data structures with Python type hints.\\n     * Benefits for clarity and maintainability.\\n   * **3.2 Validation and Error Handling**\\n     * Automatic data validation on assignment and instantiation.\\n     * Custom validators and error reporting.\\n   * **3.3 Serialization and Deserialization**\\n     * Converting data models to/from JSON, dictionaries, etc.\\n     * Usefulness for API interactions and data persistence.\\n   * **3.4 Schema Generation (JSON Schema)**\\n     * Automatic generation of JSON schemas from Pydantic models.\\n     * Interoperability with other systems and documentation.\\n\\n**4. Pydantic\\'s Role in AI Data Management and Workflows**\\n   * **4.1 Input Validation for AI Models**\\n     * Ensuring quality and correctness of data fed into machine learning models.\\n     * Handling diverse data types (text, numerical, categorical) with structured validation.\\n   * **4.2 Structuring and Validating AI Model Outputs**\\n     * Enforcing specific output formats for predictions, classifications, and generated content.\\n     * Improving downstream processing and reliability of AI applications.\\n   * **4.3 Pydantic in Large Language Model (LLM) Applications**\\n     * **4.3.1 Enforcing Structured Output from LLMs:** How Pydantic helps LLMs generate reliable JSON or other structured data.\\n     * **4.3.2 Defining Tools and Functions for LLM Agents:** Using Pydantic to describe tool inputs/outputs for function calling (e.g., OpenAI API).\\n     * **4.3.3 Validating LLM Prompts and Responses:** Ensuring adherence to expected formats.\\n   * **4.4 Data Schemas for MLOps and Data Pipelines**\\n     * Defining clear contracts for data flow between different stages of an ML pipeline.\\n     * Improving data governance and reproducibility.\\n\\n**5. Practical Applications and Use Cases**\\n   * **5.1 Building Robust AI APIs**\\n     * Using Pydantic for request and response validation in FastAPI, Flask, etc.\\n   * **5.2 Enhancing Data Ingestion for Training Data**\\n     * Validating and cleaning raw data before model training.\\n   * **5.3 Developing Reliable AI Agents and Chatbots**\\n     * Structuring conversational states, user inputs, and agent actions.\\n   * **5.4 Integrating Pydantic with Popular AI Frameworks/Libraries**\\n     * Examples with LangChain, LlamaIndex, OpenAI Python client, etc.\\n\\n**6. Benefits and Challenges of Pydantic-AI Integration**\\n   * **6.1 Key Benefits**\\n     * Increased reliability and robustness of AI systems.\\n     * Improved developer experience and productivity.\\n     * Better code readability, maintainability, and collaboration.\\n     * Reduced debugging time and fewer runtime errors.\\n     * Enhanced API design and documentation.\\n   * **6.2 Potential Challenges and Considerations**\\n     * Learning curve for Pydantic (especially for those new to type hints).\\n     * Performance considerations for very high-throughput systems (if complex validation).\\n     * Keeping up with Pydantic\\'s evolving features (e.g., V1 vs. V2).\\n\\n**7. Conclusion**\\n   * Recap of Pydantic\\'s critical role in building robust, reliable, and maintainable AI applications.\\n   * Future outlook for structured data and type safety in the evolving AI landscape.\\n   * Final recommendations for developers leveraging AI.\\n\\n**8. References**\\n   * List of academic papers, articles, official documentation, and relevant code repositories consulted.\\n\\n**Web Search Queries for Report Creation:**\\n*   \"pydantic for AI applications\"\\n*   \"pydantic LLM structured output\"\\n*   \"pydantic function calling OpenAI\"\\n*   \"pydantic in LangChain\"\\n*   \"pydantic data validation machine learning\"\\n*   \"pydantic and AI agent tools\"\\n*   \"pydantic FastAPI AI APIs\"\\n*   \"benefits of pydantic in AI development\"\\n*   \"pydantic V2 AI features\"\\n*   \"how pydantic improves AI system reliability\"'}, tool_call_id='pyd_ai_834813d891224f01a4f4a6350cb7b899')], usage=RequestUsage(input_tokens=1667, output_tokens=1312, details={'thoughts_tokens': 129, 'text_prompt_tokens': 1667}), model_name='gemini-2.5-flash', timestamp=datetime.datetime(2025, 9, 3, 22, 29, 9, 125879, tzinfo=datetime.timezone.utc), provider_name='google-gla', provider_details={'finish_reason': 'STOP'}, provider_response_id='NcG4aLSbEfG0qtsP1NXE8Q8'))\n",
      "INFO:web_deep_researcher:Node: End(data=FinalResult(output=PlanOutput(outline='**Report Outline: Pydantic in AI Applications**\\n\\n**1. Summary**\\n   * Brief overview of Pydantic\\'s role in modern AI development.\\n   * Key benefits of integrating Pydantic for data validation and structuring in AI systems.\\n   * Highlights of use cases and practical implications.\\n\\n**2. Introduction**\\n   * **2.1 What is Pydantic?**\\n     * Overview of Pydantic as a data validation and settings management library.\\n     * Core features: type hints, data parsing, serialization, runtime validation.\\n   * **2.2 The Challenge of Data in AI Systems**\\n     * Importance of data integrity and structure in AI/ML workflows.\\n     * Common issues: unstructured inputs/outputs, schema mismatches, debugging complexity.\\n   * **2.3 Why Pydantic for AI?**\\n     * Introduction to how Pydantic addresses these challenges.\\n     * Scope and objectives of the report.\\n\\n**3. Pydantic Fundamentals Relevant to AI**\\n   * **3.1 Data Models and Type Hinting**\\n     * Defining data structures with Python type hints.\\n     * Benefits for clarity and maintainability.\\n   * **3.2 Validation and Error Handling**\\n     * Automatic data validation on assignment and instantiation.\\n     * Custom validators and error reporting.\\n   * **3.3 Serialization and Deserialization**\\n     * Converting data models to/from JSON, dictionaries, etc.\\n     * Usefulness for API interactions and data persistence.\\n   * **3.4 Schema Generation (JSON Schema)**\\n     * Automatic generation of JSON schemas from Pydantic models.\\n     * Interoperability with other systems and documentation.\\n\\n**4. Pydantic\\'s Role in AI Data Management and Workflows**\\n   * **4.1 Input Validation for AI Models**\\n     * Ensuring quality and correctness of data fed into machine learning models.\\n     * Handling diverse data types (text, numerical, categorical) with structured validation.\\n   * **4.2 Structuring and Validating AI Model Outputs**\\n     * Enforcing specific output formats for predictions, classifications, and generated content.\\n     * Improving downstream processing and reliability of AI applications.\\n   * **4.3 Pydantic in Large Language Model (LLM) Applications**\\n     * **4.3.1 Enforcing Structured Output from LLMs:** How Pydantic helps LLMs generate reliable JSON or other structured data.\\n     * **4.3.2 Defining Tools and Functions for LLM Agents:** Using Pydantic to describe tool inputs/outputs for function calling (e.g., OpenAI API).\\n     * **4.3.3 Validating LLM Prompts and Responses:** Ensuring adherence to expected formats.\\n   * **4.4 Data Schemas for MLOps and Data Pipelines**\\n     * Defining clear contracts for data flow between different stages of an ML pipeline.\\n     * Improving data governance and reproducibility.\\n\\n**5. Practical Applications and Use Cases**\\n   * **5.1 Building Robust AI APIs**\\n     * Using Pydantic for request and response validation in FastAPI, Flask, etc.\\n   * **5.2 Enhancing Data Ingestion for Training Data**\\n     * Validating and cleaning raw data before model training.\\n   * **5.3 Developing Reliable AI Agents and Chatbots**\\n     * Structuring conversational states, user inputs, and agent actions.\\n   * **5.4 Integrating Pydantic with Popular AI Frameworks/Libraries**\\n     * Examples with LangChain, LlamaIndex, OpenAI Python client, etc.\\n\\n**6. Benefits and Challenges of Pydantic-AI Integration**\\n   * **6.1 Key Benefits**\\n     * Increased reliability and robustness of AI systems.\\n     * Improved developer experience and productivity.\\n     * Better code readability, maintainability, and collaboration.\\n     * Reduced debugging time and fewer runtime errors.\\n     * Enhanced API design and documentation.\\n   * **6.2 Potential Challenges and Considerations**\\n     * Learning curve for Pydantic (especially for those new to type hints).\\n     * Performance considerations for very high-throughput systems (if complex validation).\\n     * Keeping up with Pydantic\\'s evolving features (e.g., V1 vs. V2).\\n\\n**7. Conclusion**\\n   * Recap of Pydantic\\'s critical role in building robust, reliable, and maintainable AI applications.\\n   * Future outlook for structured data and type safety in the evolving AI landscape.\\n   * Final recommendations for developers leveraging AI.\\n\\n**8. References**\\n   * List of academic papers, articles, official documentation, and relevant code repositories consulted.\\n\\n**Web Search Queries for Report Creation:**\\n*   \"pydantic for AI applications\"\\n*   \"pydantic LLM structured output\"\\n*   \"pydantic function calling OpenAI\"\\n*   \"pydantic in LangChain\"\\n*   \"pydantic data validation machine learning\"\\n*   \"pydantic and AI agent tools\"\\n*   \"pydantic FastAPI AI APIs\"\\n*   \"benefits of pydantic in AI development\"\\n*   \"pydantic V2 AI features\"\\n*   \"how pydantic improves AI system reliability\"'), tool_name='final_result', tool_call_id='pyd_ai_834813d891224f01a4f4a6350cb7b899'))\n",
      "INFO:web_deep_researcher:Getting the information for the plan: **Report Outline: Pydantic in AI Applications**\n",
      "\n",
      "**1. Summary**\n",
      "   * Brief overview of Pydantic's role in modern AI development.\n",
      "   * Key benefits of integrating Pydantic for data validation and structuring in AI systems.\n",
      "   * Highlights of use cases and practical implications.\n",
      "\n",
      "**2. Introduction**\n",
      "   * **2.1 What is Pydantic?**\n",
      "     * Overview of Pydantic as a data validation and settings management library.\n",
      "     * Core features: type hints, data parsing, serialization, runtime validation.\n",
      "   * **2.2 The Challenge of Data in AI Systems**\n",
      "     * Importance of data integrity and structure in AI/ML workflows.\n",
      "     * Common issues: unstructured inputs/outputs, schema mismatches, debugging complexity.\n",
      "   * **2.3 Why Pydantic for AI?**\n",
      "     * Introduction to how Pydantic addresses these challenges.\n",
      "     * Scope and objectives of the report.\n",
      "\n",
      "**3. Pydantic Fundamentals Relevant to AI**\n",
      "   * **3.1 Data Models and Type Hinting**\n",
      "     * Defining data structures with Python type hints.\n",
      "     * Benefits for clarity and maintainability.\n",
      "   * **3.2 Validation and Error Handling**\n",
      "     * Automatic data validation on assignment and instantiation.\n",
      "     * Custom validators and error reporting.\n",
      "   * **3.3 Serialization and Deserialization**\n",
      "     * Converting data models to/from JSON, dictionaries, etc.\n",
      "     * Usefulness for API interactions and data persistence.\n",
      "   * **3.4 Schema Generation (JSON Schema)**\n",
      "     * Automatic generation of JSON schemas from Pydantic models.\n",
      "     * Interoperability with other systems and documentation.\n",
      "\n",
      "**4. Pydantic's Role in AI Data Management and Workflows**\n",
      "   * **4.1 Input Validation for AI Models**\n",
      "     * Ensuring quality and correctness of data fed into machine learning models.\n",
      "     * Handling diverse data types (text, numerical, categorical) with structured validation.\n",
      "   * **4.2 Structuring and Validating AI Model Outputs**\n",
      "     * Enforcing specific output formats for predictions, classifications, and generated content.\n",
      "     * Improving downstream processing and reliability of AI applications.\n",
      "   * **4.3 Pydantic in Large Language Model (LLM) Applications**\n",
      "     * **4.3.1 Enforcing Structured Output from LLMs:** How Pydantic helps LLMs generate reliable JSON or other structured data.\n",
      "     * **4.3.2 Defining Tools and Functions for LLM Agents:** Using Pydantic to describe tool inputs/outputs for function calling (e.g., OpenAI API).\n",
      "     * **4.3.3 Validating LLM Prompts and Responses:** Ensuring adherence to expected formats.\n",
      "   * **4.4 Data Schemas for MLOps and Data Pipelines**\n",
      "     * Defining clear contracts for data flow between different stages of an ML pipeline.\n",
      "     * Improving data governance and reproducibility.\n",
      "\n",
      "**5. Practical Applications and Use Cases**\n",
      "   * **5.1 Building Robust AI APIs**\n",
      "     * Using Pydantic for request and response validation in FastAPI, Flask, etc.\n",
      "   * **5.2 Enhancing Data Ingestion for Training Data**\n",
      "     * Validating and cleaning raw data before model training.\n",
      "   * **5.3 Developing Reliable AI Agents and Chatbots**\n",
      "     * Structuring conversational states, user inputs, and agent actions.\n",
      "   * **5.4 Integrating Pydantic with Popular AI Frameworks/Libraries**\n",
      "     * Examples with LangChain, LlamaIndex, OpenAI Python client, etc.\n",
      "\n",
      "**6. Benefits and Challenges of Pydantic-AI Integration**\n",
      "   * **6.1 Key Benefits**\n",
      "     * Increased reliability and robustness of AI systems.\n",
      "     * Improved developer experience and productivity.\n",
      "     * Better code readability, maintainability, and collaboration.\n",
      "     * Reduced debugging time and fewer runtime errors.\n",
      "     * Enhanced API design and documentation.\n",
      "   * **6.2 Potential Challenges and Considerations**\n",
      "     * Learning curve for Pydantic (especially for those new to type hints).\n",
      "     * Performance considerations for very high-throughput systems (if complex validation).\n",
      "     * Keeping up with Pydantic's evolving features (e.g., V1 vs. V2).\n",
      "\n",
      "**7. Conclusion**\n",
      "   * Recap of Pydantic's critical role in building robust, reliable, and maintainable AI applications.\n",
      "   * Future outlook for structured data and type safety in the evolving AI landscape.\n",
      "   * Final recommendations for developers leveraging AI.\n",
      "\n",
      "**8. References**\n",
      "   * List of academic papers, articles, official documentation, and relevant code repositories consulted.\n",
      "\n",
      "**Web Search Queries for Report Creation:**\n",
      "*   \"pydantic for AI applications\"\n",
      "*   \"pydantic LLM structured output\"\n",
      "*   \"pydantic function calling OpenAI\"\n",
      "*   \"pydantic in LangChain\"\n",
      "*   \"pydantic data validation machine learning\"\n",
      "*   \"pydantic and AI agent tools\"\n",
      "*   \"pydantic FastAPI AI APIs\"\n",
      "*   \"benefits of pydantic in AI development\"\n",
      "*   \"pydantic V2 AI features\"\n",
      "*   \"how pydantic improves AI system reliability\"\n",
      "INFO:mcp.client.streamable_http:Negotiated protocol version: 2025-06-18\n",
      "INFO:web_deep_researcher:Node: UserPromptNode(user_prompt='The outline of the report is: **Report Outline: Pydantic in AI Applications**\\n\\n**1. Summary**\\n   * Brief overview of Pydantic\\'s role in modern AI development.\\n   * Key benefits of integrating Pydantic for data validation and structuring in AI systems.\\n   * Highlights of use cases and practical implications.\\n\\n**2. Introduction**\\n   * **2.1 What is Pydantic?**\\n     * Overview of Pydantic as a data validation and settings management library.\\n     * Core features: type hints, data parsing, serialization, runtime validation.\\n   * **2.2 The Challenge of Data in AI Systems**\\n     * Importance of data integrity and structure in AI/ML workflows.\\n     * Common issues: unstructured inputs/outputs, schema mismatches, debugging complexity.\\n   * **2.3 Why Pydantic for AI?**\\n     * Introduction to how Pydantic addresses these challenges.\\n     * Scope and objectives of the report.\\n\\n**3. Pydantic Fundamentals Relevant to AI**\\n   * **3.1 Data Models and Type Hinting**\\n     * Defining data structures with Python type hints.\\n     * Benefits for clarity and maintainability.\\n   * **3.2 Validation and Error Handling**\\n     * Automatic data validation on assignment and instantiation.\\n     * Custom validators and error reporting.\\n   * **3.3 Serialization and Deserialization**\\n     * Converting data models to/from JSON, dictionaries, etc.\\n     * Usefulness for API interactions and data persistence.\\n   * **3.4 Schema Generation (JSON Schema)**\\n     * Automatic generation of JSON schemas from Pydantic models.\\n     * Interoperability with other systems and documentation.\\n\\n**4. Pydantic\\'s Role in AI Data Management and Workflows**\\n   * **4.1 Input Validation for AI Models**\\n     * Ensuring quality and correctness of data fed into machine learning models.\\n     * Handling diverse data types (text, numerical, categorical) with structured validation.\\n   * **4.2 Structuring and Validating AI Model Outputs**\\n     * Enforcing specific output formats for predictions, classifications, and generated content.\\n     * Improving downstream processing and reliability of AI applications.\\n   * **4.3 Pydantic in Large Language Model (LLM) Applications**\\n     * **4.3.1 Enforcing Structured Output from LLMs:** How Pydantic helps LLMs generate reliable JSON or other structured data.\\n     * **4.3.2 Defining Tools and Functions for LLM Agents:** Using Pydantic to describe tool inputs/outputs for function calling (e.g., OpenAI API).\\n     * **4.3.3 Validating LLM Prompts and Responses:** Ensuring adherence to expected formats.\\n   * **4.4 Data Schemas for MLOps and Data Pipelines**\\n     * Defining clear contracts for data flow between different stages of an ML pipeline.\\n     * Improving data governance and reproducibility.\\n\\n**5. Practical Applications and Use Cases**\\n   * **5.1 Building Robust AI APIs**\\n     * Using Pydantic for request and response validation in FastAPI, Flask, etc.\\n   * **5.2 Enhancing Data Ingestion for Training Data**\\n     * Validating and cleaning raw data before model training.\\n   * **5.3 Developing Reliable AI Agents and Chatbots**\\n     * Structuring conversational states, user inputs, and agent actions.\\n   * **5.4 Integrating Pydantic with Popular AI Frameworks/Libraries**\\n     * Examples with LangChain, LlamaIndex, OpenAI Python client, etc.\\n\\n**6. Benefits and Challenges of Pydantic-AI Integration**\\n   * **6.1 Key Benefits**\\n     * Increased reliability and robustness of AI systems.\\n     * Improved developer experience and productivity.\\n     * Better code readability, maintainability, and collaboration.\\n     * Reduced debugging time and fewer runtime errors.\\n     * Enhanced API design and documentation.\\n   * **6.2 Potential Challenges and Considerations**\\n     * Learning curve for Pydantic (especially for those new to type hints).\\n     * Performance considerations for very high-throughput systems (if complex validation).\\n     * Keeping up with Pydantic\\'s evolving features (e.g., V1 vs. V2).\\n\\n**7. Conclusion**\\n   * Recap of Pydantic\\'s critical role in building robust, reliable, and maintainable AI applications.\\n   * Future outlook for structured data and type safety in the evolving AI landscape.\\n   * Final recommendations for developers leveraging AI.\\n\\n**8. References**\\n   * List of academic papers, articles, official documentation, and relevant code repositories consulted.\\n\\n**Web Search Queries for Report Creation:**\\n*   \"pydantic for AI applications\"\\n*   \"pydantic LLM structured output\"\\n*   \"pydantic function calling OpenAI\"\\n*   \"pydantic in LangChain\"\\n*   \"pydantic data validation machine learning\"\\n*   \"pydantic and AI agent tools\"\\n*   \"pydantic FastAPI AI APIs\"\\n*   \"benefits of pydantic in AI development\"\\n*   \"pydantic V2 AI features\"\\n*   \"how pydantic improves AI system reliability\"', instructions=\"You are a helpful AI assistant tasked with getting content for a comprehensive report based on a given outline. Your goal is to research and compile current information for each section and subsection of the outline using the DuckDuckGo MCP server. Follow these steps to complete the task:\\n\\n                1. Review the provided outline\\n\\n                2. For each section and subsection in the outline:\\n                a. Use the DuckDuckGo MCP server to search for current information related to the topic.\\n                b. Review the search results and identify the most relevant and reliable sources.\\n                c. After fetching the content, analyze and summarize the information relevant to the outline section.\\n\\n                3. Write the report section by section, following the structure of the outline. For each section:\\n                a. Provide a brief introduction to the topic.\\n                b. Include relevant information gathered from your research.\\n                c. Ensure that the content is well-organized and flows logically.\\n                d. Use appropriate transitions between subsections and main sections.\\n\\n                4. Throughout the writing process, keep track of the sources you've used. For each source, note:\\n                - The title of the webpage or article\\n                - The author (if available)\\n                - The website name\\n                - The URL\\n                - The date you accessed the information\", instructions_functions=[], system_prompts=(), system_prompt_functions=[], system_prompt_dynamic_functions={})\n",
      "INFO:web_deep_researcher:Node: ModelRequestNode(request=ModelRequest(parts=[UserPromptPart(content='The outline of the report is: **Report Outline: Pydantic in AI Applications**\\n\\n**1. Summary**\\n   * Brief overview of Pydantic\\'s role in modern AI development.\\n   * Key benefits of integrating Pydantic for data validation and structuring in AI systems.\\n   * Highlights of use cases and practical implications.\\n\\n**2. Introduction**\\n   * **2.1 What is Pydantic?**\\n     * Overview of Pydantic as a data validation and settings management library.\\n     * Core features: type hints, data parsing, serialization, runtime validation.\\n   * **2.2 The Challenge of Data in AI Systems**\\n     * Importance of data integrity and structure in AI/ML workflows.\\n     * Common issues: unstructured inputs/outputs, schema mismatches, debugging complexity.\\n   * **2.3 Why Pydantic for AI?**\\n     * Introduction to how Pydantic addresses these challenges.\\n     * Scope and objectives of the report.\\n\\n**3. Pydantic Fundamentals Relevant to AI**\\n   * **3.1 Data Models and Type Hinting**\\n     * Defining data structures with Python type hints.\\n     * Benefits for clarity and maintainability.\\n   * **3.2 Validation and Error Handling**\\n     * Automatic data validation on assignment and instantiation.\\n     * Custom validators and error reporting.\\n   * **3.3 Serialization and Deserialization**\\n     * Converting data models to/from JSON, dictionaries, etc.\\n     * Usefulness for API interactions and data persistence.\\n   * **3.4 Schema Generation (JSON Schema)**\\n     * Automatic generation of JSON schemas from Pydantic models.\\n     * Interoperability with other systems and documentation.\\n\\n**4. Pydantic\\'s Role in AI Data Management and Workflows**\\n   * **4.1 Input Validation for AI Models**\\n     * Ensuring quality and correctness of data fed into machine learning models.\\n     * Handling diverse data types (text, numerical, categorical) with structured validation.\\n   * **4.2 Structuring and Validating AI Model Outputs**\\n     * Enforcing specific output formats for predictions, classifications, and generated content.\\n     * Improving downstream processing and reliability of AI applications.\\n   * **4.3 Pydantic in Large Language Model (LLM) Applications**\\n     * **4.3.1 Enforcing Structured Output from LLMs:** How Pydantic helps LLMs generate reliable JSON or other structured data.\\n     * **4.3.2 Defining Tools and Functions for LLM Agents:** Using Pydantic to describe tool inputs/outputs for function calling (e.g., OpenAI API).\\n     * **4.3.3 Validating LLM Prompts and Responses:** Ensuring adherence to expected formats.\\n   * **4.4 Data Schemas for MLOps and Data Pipelines**\\n     * Defining clear contracts for data flow between different stages of an ML pipeline.\\n     * Improving data governance and reproducibility.\\n\\n**5. Practical Applications and Use Cases**\\n   * **5.1 Building Robust AI APIs**\\n     * Using Pydantic for request and response validation in FastAPI, Flask, etc.\\n   * **5.2 Enhancing Data Ingestion for Training Data**\\n     * Validating and cleaning raw data before model training.\\n   * **5.3 Developing Reliable AI Agents and Chatbots**\\n     * Structuring conversational states, user inputs, and agent actions.\\n   * **5.4 Integrating Pydantic with Popular AI Frameworks/Libraries**\\n     * Examples with LangChain, LlamaIndex, OpenAI Python client, etc.\\n\\n**6. Benefits and Challenges of Pydantic-AI Integration**\\n   * **6.1 Key Benefits**\\n     * Increased reliability and robustness of AI systems.\\n     * Improved developer experience and productivity.\\n     * Better code readability, maintainability, and collaboration.\\n     * Reduced debugging time and fewer runtime errors.\\n     * Enhanced API design and documentation.\\n   * **6.2 Potential Challenges and Considerations**\\n     * Learning curve for Pydantic (especially for those new to type hints).\\n     * Performance considerations for very high-throughput systems (if complex validation).\\n     * Keeping up with Pydantic\\'s evolving features (e.g., V1 vs. V2).\\n\\n**7. Conclusion**\\n   * Recap of Pydantic\\'s critical role in building robust, reliable, and maintainable AI applications.\\n   * Future outlook for structured data and type safety in the evolving AI landscape.\\n   * Final recommendations for developers leveraging AI.\\n\\n**8. References**\\n   * List of academic papers, articles, official documentation, and relevant code repositories consulted.\\n\\n**Web Search Queries for Report Creation:**\\n*   \"pydantic for AI applications\"\\n*   \"pydantic LLM structured output\"\\n*   \"pydantic function calling OpenAI\"\\n*   \"pydantic in LangChain\"\\n*   \"pydantic data validation machine learning\"\\n*   \"pydantic and AI agent tools\"\\n*   \"pydantic FastAPI AI APIs\"\\n*   \"benefits of pydantic in AI development\"\\n*   \"pydantic V2 AI features\"\\n*   \"how pydantic improves AI system reliability\"', timestamp=datetime.datetime(2025, 9, 3, 22, 29, 9, 752969, tzinfo=datetime.timezone.utc))], instructions=\"You are a helpful AI assistant tasked with getting content for a comprehensive report based on a given outline. Your goal is to research and compile current information for each section and subsection of the outline using the DuckDuckGo MCP server. Follow these steps to complete the task:\\n\\n                1. Review the provided outline\\n\\n                2. For each section and subsection in the outline:\\n                a. Use the DuckDuckGo MCP server to search for current information related to the topic.\\n                b. Review the search results and identify the most relevant and reliable sources.\\n                c. After fetching the content, analyze and summarize the information relevant to the outline section.\\n\\n                3. Write the report section by section, following the structure of the outline. For each section:\\n                a. Provide a brief introduction to the topic.\\n                b. Include relevant information gathered from your research.\\n                c. Ensure that the content is well-organized and flows logically.\\n                d. Use appropriate transitions between subsections and main sections.\\n\\n                4. Throughout the writing process, keep track of the sources you've used. For each source, note:\\n                - The title of the webpage or article\\n                - The author (if available)\\n                - The website name\\n                - The URL\\n                - The date you accessed the information\"))\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:web_deep_researcher:Node: CallToolsNode(model_response=ModelResponse(parts=[ToolCallPart(tool_name='tavily_search', args={'query': 'what is pydantic core features type hints data parsing serialization runtime validation', 'max_results': 5}, tool_call_id='pyd_ai_dc174c18fdd846d78331300a63faeecd')], usage=RequestUsage(input_tokens=3637, output_tokens=1579, details={'thoughts_tokens': 1542, 'text_prompt_tokens': 3637}), model_name='gemini-2.5-flash', timestamp=datetime.datetime(2025, 9, 3, 22, 29, 19, 880764, tzinfo=datetime.timezone.utc), provider_name='google-gla', provider_details={'finish_reason': 'STOP'}, provider_response_id='QMG4aPKCAoWqmtkPiMmxwA8'))\n",
      "INFO:web_deep_researcher:Node: ModelRequestNode(request=ModelRequest(parts=[ToolReturnPart(tool_name='tavily_search', content={'query': 'what is pydantic core features type hints data parsing serialization runtime validation', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://www.datagluons.io/blog/pydantic-data-contract-manager', 'title': 'Data Validation and Versioned Data Contracts with Pydantic', 'content': 'Pydantic is a data validation library for Python that uses type hints to define schemas. It validates input data at runtime and returns fully typed Python', 'score': 0.82082915, 'raw_content': None}, {'url': 'https://medium.com/@terabyte26/pydantic-v2-patterns-for-automation-validate-anything-everywhere-6d553504a25a', 'title': 'Pydantic v2 Patterns for Automation: Validate Anything, Everywhere', 'content': 'If you build automation in Python, you already know the secret sauce isn’t just writing scripts that “work.” The real win is writing scripts that keep working as inputs change, APIs evolve, and your data gets just a little messier every week. Pydantic v2 turns that superpower into a practical toolkit you can use anywhere data flows — API calls, HTML scrapes, CSV imports, CLI arguments, background jobs, you name it. In this article, we’ll take a human, practitioner’s look at Pydantic v2 patterns that make automation resilient: modeling heterogeneous inputs from APIs/HTML/CSV, building custom validators that read like business rules, leaning on strict types to eliminate “helpful” coercions that backfire, and squeezing performance with pydantic-core so validation never becomes your bottleneck.', 'score': 0.34979975, 'raw_content': None}, {'url': 'https://superjson.ai/blog/2025-08-14-pydantic-vs-zod-json-schema-generation-comparison', 'title': 'Pydantic vs Zod: Complete Comparison for JSON Schema Generation', 'content': 'Learn how to automatically generate Pydantic models from JSON data with validation, type hints, and schema generation. Includes tools, best practices, and', 'score': 0.33921495, 'raw_content': None}, {'url': 'https://www.reddit.com/r/Python/comments/1n0ng7f/whats_your_favorite_python_trick_or_lesser_known/', 'title': 'Whats your favorite Python trick or lesser known feature? - Reddit', 'content': 'Missing: core hints', 'score': 0.19392508, 'raw_content': None}, {'url': 'https://docs.langchain.com/oss/python/langgraph/use-graph-api', 'title': 'Use the graph API - Docs by LangChain', 'content': 'Missing: core hints', 'score': 0.18732621, 'raw_content': None}], 'response_time': 1.15, 'request_id': '7b4229be-ac28-4ccc-9d90-b06b097c8b0b'}, tool_call_id='pyd_ai_dc174c18fdd846d78331300a63faeecd', timestamp=datetime.datetime(2025, 9, 3, 22, 29, 21, 527660, tzinfo=datetime.timezone.utc))], instructions=\"You are a helpful AI assistant tasked with getting content for a comprehensive report based on a given outline. Your goal is to research and compile current information for each section and subsection of the outline using the DuckDuckGo MCP server. Follow these steps to complete the task:\\n\\n                1. Review the provided outline\\n\\n                2. For each section and subsection in the outline:\\n                a. Use the DuckDuckGo MCP server to search for current information related to the topic.\\n                b. Review the search results and identify the most relevant and reliable sources.\\n                c. After fetching the content, analyze and summarize the information relevant to the outline section.\\n\\n                3. Write the report section by section, following the structure of the outline. For each section:\\n                a. Provide a brief introduction to the topic.\\n                b. Include relevant information gathered from your research.\\n                c. Ensure that the content is well-organized and flows logically.\\n                d. Use appropriate transitions between subsections and main sections.\\n\\n                4. Throughout the writing process, keep track of the sources you've used. For each source, note:\\n                - The title of the webpage or article\\n                - The author (if available)\\n                - The website name\\n                - The URL\\n                - The date you accessed the information\"))\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:web_deep_researcher:Node: CallToolsNode(model_response=ModelResponse(parts=[ToolCallPart(tool_name='tavily_search', args={'max_results': 3, 'query': 'importance of data integrity and structure in AI/ML workflows common issues unstructured inputs outputs schema mismatches debugging complexity'}, tool_call_id='pyd_ai_a14cea2fd8b84494a09043b8974626a9')], usage=RequestUsage(input_tokens=4411, cache_read_tokens=2932, output_tokens=1080, details={'cached_content_tokens': 2932, 'thoughts_tokens': 1036, 'text_prompt_tokens': 4411, 'text_cache_tokens': 2932}), model_name='gemini-2.5-flash', timestamp=datetime.datetime(2025, 9, 3, 22, 29, 28, 151445, tzinfo=datetime.timezone.utc), provider_name='google-gla', provider_details={'finish_reason': 'STOP'}, provider_response_id='SMG4aLGhEou2qtsPstW5uQw'))\n",
      "INFO:web_deep_researcher:Node: ModelRequestNode(request=ModelRequest(parts=[ToolReturnPart(tool_name='tavily_search', content={'query': 'importance of data integrity and structure in AI/ML workflows common issues unstructured inputs outputs schema mismatches debugging complexity', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Understanding Data Integrity: Importance and Best Practices', 'url': 'https://www.cloudtech.com/resources/data-integrity-importance-best-practices', 'content': 'Aug 19, 2025 · Common indicators of data integrity problems and how to solve them before escalation? The absence of a structured approach to managing data integrity leads to mismatched records, duplicate entries, and reports that can’t be trusted.', 'score': 0.5920305, 'raw_content': None}, {'title': 'Data Integrity: The Key to Trust in AI Systems - IEEE Spectrum', 'url': 'https://spectrum.ieee.org/data-integrity', 'content': 'Aug 18, 2025 · As AI agents become more autonomous, data integrity is key. How can we ensure security for machine-to-machine interactions?', 'score': 0.5510187, 'raw_content': None}, {'title': 'AI and the Evolution of Relational Schemas | DataJoint ...', 'url': 'https://www.datajoint.com/news/ai-and-the-evolution-of-relational-schemas', 'content': 'Aug 26, 2025 · However, for systems demanding high data integrity , consistency, and provable relationships—especially when AI is involved in critical decision-making—the mathematical rigor and enforcement capabilities of well-defined schemas remain essential.', 'score': 0.52086246, 'raw_content': None}], 'response_time': 3.23, 'request_id': '08abcd99-7af2-4a77-9120-a3c940437680'}, tool_call_id='pyd_ai_a14cea2fd8b84494a09043b8974626a9', timestamp=datetime.datetime(2025, 9, 3, 22, 29, 31, 875801, tzinfo=datetime.timezone.utc))], instructions=\"You are a helpful AI assistant tasked with getting content for a comprehensive report based on a given outline. Your goal is to research and compile current information for each section and subsection of the outline using the DuckDuckGo MCP server. Follow these steps to complete the task:\\n\\n                1. Review the provided outline\\n\\n                2. For each section and subsection in the outline:\\n                a. Use the DuckDuckGo MCP server to search for current information related to the topic.\\n                b. Review the search results and identify the most relevant and reliable sources.\\n                c. After fetching the content, analyze and summarize the information relevant to the outline section.\\n\\n                3. Write the report section by section, following the structure of the outline. For each section:\\n                a. Provide a brief introduction to the topic.\\n                b. Include relevant information gathered from your research.\\n                c. Ensure that the content is well-organized and flows logically.\\n                d. Use appropriate transitions between subsections and main sections.\\n\\n                4. Throughout the writing process, keep track of the sources you've used. For each source, note:\\n                - The title of the webpage or article\\n                - The author (if available)\\n                - The website name\\n                - The URL\\n                - The date you accessed the information\"))\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:web_deep_researcher:Node: CallToolsNode(model_response=ModelResponse(parts=[ToolCallPart(tool_name='tavily_search', args={'query': 'why pydantic for AI addresses data integrity challenges', 'max_results': 3}, tool_call_id='pyd_ai_d0ac222920c149618218bd79e87ccc3b')], usage=RequestUsage(input_tokens=4902, cache_read_tokens=3918, output_tokens=293, details={'cached_content_tokens': 3918, 'thoughts_tokens': 260, 'text_prompt_tokens': 4902, 'text_cache_tokens': 3918}), model_name='gemini-2.5-flash', timestamp=datetime.datetime(2025, 9, 3, 22, 29, 34, 608074, tzinfo=datetime.timezone.utc), provider_name='google-gla', provider_details={'finish_reason': 'STOP'}, provider_response_id='TsG4aPm7La2mqtsP59SlwQ8'))\n",
      "INFO:web_deep_researcher:Node: ModelRequestNode(request=ModelRequest(parts=[ToolReturnPart(tool_name='tavily_search', content={'query': 'why pydantic for AI addresses data integrity challenges', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://latenode.com/blog/11-open-source-ai-agent-frameworks-that-will-transform-your-development-2025-complete-guide', 'title': '11 Open Source AI Agent Frameworks That Will Transform ...', 'content': 'Pydantic AI Agents excels at ensuring data reliability through its rigorous validation processes. However, implementing these capabilities often requires', 'score': 0.6512002, 'raw_content': None}, {'url': 'https://medium.com/@judeaugustinej/pydantic-for-llm-workflows-631a7c01fdba', 'title': 'Pydantic for LLM Workflows', 'content': 'Pydantic is the most widely used data validation library for Python. It brings structure , reliability & validation to all our LLM Applications.', 'score': 0.5463757, 'raw_content': None}, {'url': 'https://brightdata.com/blog/ai/pydantic-ai-with-web-mcp', 'title': \"Integrate Pydantic AI with Bright Data's Web MCP\", 'content': \"# Pydantic AI with Bright Data’s Web MCP for Agents with Data Access Discover how to combine Pydantic AI with Bright Data MCP to build Python AI agents that access and extract web data in real time. * Why Pydantic AI pairs well with Bright Data’s Web MCP server to build agents that can access the web. * How to integrate Pydantic with Bright Data’s Web MCP to create an AI agent backed by real data. # Initialize the AI agent with Gemini and Bright Data's Web MCP server integration In this article, you learned how to integrate Pydantic AI with the Bright Data’s Web MCP server to build an AI agent capable of accessing the web.\", 'score': 0.2876082, 'raw_content': None}], 'response_time': 1.23, 'request_id': 'a70723e2-337a-44ed-9478-325b174fadb5'}, tool_call_id='pyd_ai_d0ac222920c149618218bd79e87ccc3b', timestamp=datetime.datetime(2025, 9, 3, 22, 29, 36, 332257, tzinfo=datetime.timezone.utc))], instructions=\"You are a helpful AI assistant tasked with getting content for a comprehensive report based on a given outline. Your goal is to research and compile current information for each section and subsection of the outline using the DuckDuckGo MCP server. Follow these steps to complete the task:\\n\\n                1. Review the provided outline\\n\\n                2. For each section and subsection in the outline:\\n                a. Use the DuckDuckGo MCP server to search for current information related to the topic.\\n                b. Review the search results and identify the most relevant and reliable sources.\\n                c. After fetching the content, analyze and summarize the information relevant to the outline section.\\n\\n                3. Write the report section by section, following the structure of the outline. For each section:\\n                a. Provide a brief introduction to the topic.\\n                b. Include relevant information gathered from your research.\\n                c. Ensure that the content is well-organized and flows logically.\\n                d. Use appropriate transitions between subsections and main sections.\\n\\n                4. Throughout the writing process, keep track of the sources you've used. For each source, note:\\n                - The title of the webpage or article\\n                - The author (if available)\\n                - The website name\\n                - The URL\\n                - The date you accessed the information\"))\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:web_deep_researcher:Node: CallToolsNode(model_response=ModelResponse(parts=[ToolCallPart(tool_name='tavily_search', args={'query': 'pydantic role in modern AI development benefits use cases', 'max_results': 5}, tool_call_id='pyd_ai_ac368026c497417bb06d98b95422462a')], usage=RequestUsage(input_tokens=5465, cache_read_tokens=4909, output_tokens=292, details={'cached_content_tokens': 4909, 'thoughts_tokens': 258, 'text_prompt_tokens': 5465, 'text_cache_tokens': 4909}), model_name='gemini-2.5-flash', timestamp=datetime.datetime(2025, 9, 3, 22, 29, 39, 319693, tzinfo=datetime.timezone.utc), provider_name='google-gla', provider_details={'finish_reason': 'STOP'}, provider_response_id='U8G4aITqHPG0qtsP1NXE8Q8'))\n",
      "INFO:web_deep_researcher:Node: ModelRequestNode(request=ModelRequest(parts=[ToolReturnPart(tool_name='tavily_search', content={'query': 'pydantic role in modern AI development benefits use cases', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://www.geeky-gadgets.com/pydantic-ai-app-development/', 'title': 'Using Pydantic AI to Building Reliable AI Applications', 'content': 'Pydantic AI is a robust framework designed to streamline the development of AI-driven applications by focusing on **type safety** and **structured responses**. * Pydantic AI emphasizes type safety and structured responses, making sure data integrity and consistent outputs for AI-driven applications. * Compared to alternatives like LangChain and LangGraph, Pydantic AI excels in simplicity and type safety but may fall short for complex workflows requiring better visualization and debugging tools. These features make Pydantic AI particularly well-suited for applications that require **precision**, such as task management tools, data-driven workflows, or systems that demand high reliability. Pydantic AI offers several advantages that make it a compelling framework for developers aiming to build reliable and efficient AI-driven applications:', 'score': 0.7187726, 'raw_content': None}, {'url': 'https://towardsdatascience.com/applied-python-chronicles-a-gentle-intro-to-pydantic-a630f797e933/', 'title': 'Applied Python Chronicles: A Gentle Intro to Pydantic', 'content': 'Jul 25, 2024·There are quite a few use cases where Pydantic fits almost seamlessly. Data processing, among others, benefits from using Pydantic as well.', 'score': 0.67988104, 'raw_content': None}, {'url': 'https://www.civo.com/learn/simplifying-ai-development-with-pydanticai', 'title': 'Simplifying AI Development with PydanticAI and Serverless ...', 'content': \"Apr 16, 2025·Type Safety: By using Python's type annotations, PydanticAI ensures that inputs and outputs adhere to specified formats. This reduces bugs and\", 'score': 0.5726516, 'raw_content': None}, {'url': 'https://www.datacamp.com/tutorial/pydantic', 'title': 'Pydantic: A Guide With Practical Examples - DataCamp', 'content': 'Jun 25, 2025·Pydantic handles the rest: validating incoming data, converting types when appropriate, and providing clear error messages when validation fails', 'score': 0.4671622, 'raw_content': None}, {'url': 'https://ai.pydantic.dev/', 'title': 'Pydantic AI', 'content': \"*Agent Framework / shim to use Pydantic with LLMs* Pydantic AI is a Python agent framework designed to make it less painful to build production grade Similarly, virtually every agent framework and LLM library in Python uses Pydantic Validation, yet when we began to use LLMs in Pydantic Logfire, we couldn't find anything that gave us the same feeling. from pydantic_ai import Agent Here is a concise example using Pydantic AI to build a support agent for a bank: from pydantic_ai import Agent, RunContext To understand the flow of the above runs, we can watch the agent in action using Pydantic Logfire. from pydantic_ai import Agent, RunContext The Pydantic AI documentation is available in the llms.txt format.\", 'score': 0.46089602, 'raw_content': None}], 'response_time': 1.68, 'request_id': '575a41ce-91ce-4394-b10a-50e50e6b09f7'}, tool_call_id='pyd_ai_ac368026c497417bb06d98b95422462a', timestamp=datetime.datetime(2025, 9, 3, 22, 29, 41, 521313, tzinfo=datetime.timezone.utc))], instructions=\"You are a helpful AI assistant tasked with getting content for a comprehensive report based on a given outline. Your goal is to research and compile current information for each section and subsection of the outline using the DuckDuckGo MCP server. Follow these steps to complete the task:\\n\\n                1. Review the provided outline\\n\\n                2. For each section and subsection in the outline:\\n                a. Use the DuckDuckGo MCP server to search for current information related to the topic.\\n                b. Review the search results and identify the most relevant and reliable sources.\\n                c. After fetching the content, analyze and summarize the information relevant to the outline section.\\n\\n                3. Write the report section by section, following the structure of the outline. For each section:\\n                a. Provide a brief introduction to the topic.\\n                b. Include relevant information gathered from your research.\\n                c. Ensure that the content is well-organized and flows logically.\\n                d. Use appropriate transitions between subsections and main sections.\\n\\n                4. Throughout the writing process, keep track of the sources you've used. For each source, note:\\n                - The title of the webpage or article\\n                - The author (if available)\\n                - The website name\\n                - The URL\\n                - The date you accessed the information\"))\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:web_deep_researcher:Node: CallToolsNode(model_response=ModelResponse(parts=[ToolCallPart(tool_name='tavily_search', args={'max_results': 3, 'query': 'pydantic data models type hinting benefits clarity maintainability'}, tool_call_id='pyd_ai_01ed42c6d7a647849c04143ec9b4341f'), ToolCallPart(tool_name='tavily_search', args={'max_results': 3, 'query': 'pydantic validation error handling custom validators'}, tool_call_id='pyd_ai_5767c8c0869b468c91ff7ed559a8a61a'), ToolCallPart(tool_name='tavily_search', args={'max_results': 3, 'query': 'pydantic serialization deserialization JSON dictionaries API interactions data persistence'}, tool_call_id='pyd_ai_59af328752c6490a863f76b56fe32eed'), ToolCallPart(tool_name='tavily_search', args={'max_results': 3, 'query': 'pydantic schema generation JSON Schema interoperability documentation'}, tool_call_id='pyd_ai_d429081c12074cc99ff67eaebb441789')], usage=RequestUsage(input_tokens=6374, cache_read_tokens=4929, output_tokens=1072, details={'cached_content_tokens': 4929, 'thoughts_tokens': 939, 'text_prompt_tokens': 6374, 'text_cache_tokens': 4929}), model_name='gemini-2.5-flash', timestamp=datetime.datetime(2025, 9, 3, 22, 29, 47, 933796, tzinfo=datetime.timezone.utc), provider_name='google-gla', provider_details={'finish_reason': 'STOP'}, provider_response_id='XMG4aJT-BJykqtsPoO3z-Q8'))\n",
      "INFO:web_deep_researcher:Node: ModelRequestNode(request=ModelRequest(parts=[ToolReturnPart(tool_name='tavily_search', content={'query': 'pydantic data models type hinting benefits clarity maintainability', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://www.ireneburresi.dev/p/the-modern-python-toolbox-an-engineered', 'title': 'The Modern Python Toolbox: An Engineered Workflow for 2025', 'content': \"Pydantic solves this by using Python's type hints to enforce data validation at runtime. ... It ensures your type hints and Pydantic models are used\", 'score': 0.59824234, 'raw_content': None}, {'url': 'https://moldstud.com/articles/p-designing-python-rest-apis-with-openapi-specification-a-comprehensive-guide', 'title': 'Designing Python REST APIs with OpenAPI Specification', 'content': 'Defining Endpoints and Data Models\\u200b\\u200b Prioritize clarity and predictability in endpoint structure to enhance maintainability and onboarding speed. Use resource-', 'score': 0.5806618, 'raw_content': None}, {'url': 'https://www.reddit.com/r/Python/comments/1n4hc9e/python_type_system/', 'title': 'Python type system', 'content': 'They help so much with clarity, IDE hints, and defensive programming. Highly recommended.', 'score': 0.4627892, 'raw_content': None}], 'response_time': 1.15, 'request_id': '39b13100-9fd2-4343-8abb-a2919df882c5'}, tool_call_id='pyd_ai_01ed42c6d7a647849c04143ec9b4341f', timestamp=datetime.datetime(2025, 9, 3, 22, 29, 49, 594211, tzinfo=datetime.timezone.utc)), ToolReturnPart(tool_name='tavily_search', content={'query': 'pydantic validation error handling custom validators', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://dev.to/aws-builders/how-to-handle-form-data-in-aws-lambda-apis-with-powertools-openapi-support-4l40', 'title': 'How to Handle Form Data in AWS Lambda APIs with ...', 'content': 'Custom Validation You can add custom validation using Pydantic validators: from pydantic import field_validator class ContactRequest(BaseModel): name', 'score': 0.7316155, 'raw_content': None}, {'url': 'https://medium.com/@shouke.wei/the-power-of-pydantic-bringing-structure-and-validation-to-your-python-data-9de124362a6f', 'title': 'The Power of Pydantic: Bringing Structure and Validation to ...', 'content': 'Validators: For more complex, custom logic, you can use decorators like @field_validator to create your own validation functions that run on specific fields.', 'score': 0.6977124, 'raw_content': None}, {'url': 'https://medium.com/@terabyte26/pydantic-v2-patterns-for-automation-validate-anything-everywhere-6d553504a25a', 'title': 'Pydantic v2 Patterns for Automation: Validate Anything, ...', 'content': 'If you build automation in Python, you already know the secret sauce isn’t just writing scripts that “work.” The real win is writing scripts that keep working as inputs change, APIs evolve, and your data gets just a little messier every week. Pydantic v2 turns that superpower into a practical toolkit you can use anywhere data flows — API calls, HTML scrapes, CSV imports, CLI arguments, background jobs, you name it. In this article, we’ll take a human, practitioner’s look at Pydantic v2 patterns that make automation resilient: modeling heterogeneous inputs from APIs/HTML/CSV, building custom validators that read like business rules, leaning on strict types to eliminate “helpful” coercions that backfire, and squeezing performance with pydantic-core so validation never becomes your bottleneck.', 'score': 0.6824261, 'raw_content': None}], 'response_time': 1.02, 'request_id': '07e98c18-6890-4899-8d07-f477b5aaf512'}, tool_call_id='pyd_ai_5767c8c0869b468c91ff7ed559a8a61a', timestamp=datetime.datetime(2025, 9, 3, 22, 29, 49, 449195, tzinfo=datetime.timezone.utc)), ToolReturnPart(tool_name='tavily_search', content={'query': 'pydantic serialization deserialization JSON dictionaries API interactions data persistence', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://betterstack.com/community/guides/scaling-python/pydantic-explained/', 'title': 'A Complete Guide to Pydantic | Better Stack Community', 'content': 'Feb 17, 2025·Pydantic is a data validation and settings management library for Python that makes it easy to enforce data types, constraints, and serialization rules.', 'score': 0.52027756, 'raw_content': None}, {'url': 'https://docs.pydantic.dev/latest/concepts/serialization/', 'title': 'Serialization - Pydantic', 'content': \"print(m.model\\\\_dump(exclude={'foo', 'bar'})) print(m.model\\\\_dump(exclude={'foo', 'bar'})) from pydantic import BaseModel, ConfigDict, field\\\\_serializer, model\\\\_serializer print(m.model\\\\_dump()) # note: the password field is not included This behavior is different from how things worked in Pydantic V1, where we would always include all (subclass) fields when recursively dumping models to dicts. print(OuterModel(user=user).model\\\\_dump(serialize\\\\_as\\\\_any=True)) # (1)! print(OuterModel(user=user).model\\\\_dump(serialize\\\\_as\\\\_any=False)) # (2)! You can override the default setting for `serialize\\\\_as\\\\_any` by configuring a subclass of `BaseModel` that overrides the default for the `serialize\\\\_as\\\\_any` parameter to `model\\\\_dump()` and `model\\\\_dump\\\\_json()`, and then use that as the base class (instead of `pydantic.BaseModel`) for any model you want to have this default behavior. 1. By default, `model\\\\_dump\\\\_json` will use duck-typing serialization behavior, which means that the `password` field is included in the output. print(t.model\\\\_dump(exclude={'user', 'value'})) print(t.model\\\\_dump(exclude={'user': {'username', 'password'}, 'value': True})) print(user.model\\\\_dump(exclude={'hobbies': {'\\\\_\\\\_all\\\\_\\\\_': {'info'}}}))\", 'score': 0.5082762, 'raw_content': None}, {'url': 'https://stackoverflow.com/questions/65622045/pydantic-convert-to-jsonable-dict-not-full-json-string', 'title': 'pydantic convert to jsonable dict (not full json string) - Stack Overflow', 'content': '# pydantic convert to jsonable dict (not full json string) I\\'d like to use `pydantic` for handling data (bidirectionally) between an api and datastore due to it\\'s nice support for several types I care about that are not natively json-serializable. print(Model().model_dump(mode=\\'json\\')) * use `model_dump_json` directly now The official method in Pydantic 2 is using the `.model_dump()` method with `mode=\"json\"` argument: print(instance.model_dump(mode=\"json\")) We implemented a custom function to do this, descending the result of `.dict()` and converting types to jsonable - hopefully the above proposed functionality is added to pydantic in the future. * and what is the difference between `jsonable_encoder` and `model_dump_json`? New version provide to easy way to convert pydantic model to json. Please use `model_dump_json` method to convert to Json. `test_pydantic.model_dump_json()` json.loads(test_pydantic.model_dump_json())', 'score': 0.50564, 'raw_content': None}], 'response_time': 1.32, 'request_id': 'dce1ddf3-9bd5-4f2e-8003-8a6ef137ca42'}, tool_call_id='pyd_ai_59af328752c6490a863f76b56fe32eed', timestamp=datetime.datetime(2025, 9, 3, 22, 29, 49, 949277, tzinfo=datetime.timezone.utc)), ToolReturnPart(tool_name='tavily_search', content={'query': 'pydantic schema generation JSON Schema interoperability documentation', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://www.scaleway.com/en/docs/generative-apis/how-to/use-structured-outputs/', 'title': 'How to use structured outputs | Scaleway Documentation', 'content': 'Structured outputs allow users to get consistent, machine-readable JSON format responses from language models. By specifying a response format when using the Chat Completions API or Responses API, you can ensure that responses are returned in a JSON structure. There are two main modes for generating JSON: **Object Mode** (schemaless) and **Schema Mode** (deterministic, structured output). The Chat Completions API is focused on generating conversational responses, while the Responses API is a more general REST API for chat, structured outputs, tool use, and multimodal inputs. ### Using structured outputs with JSON schema (Pydantic) In JSON mode, you can prompt the model to output a JSON object without enforcing a strict schema. We recommend using structured outputs (`json_schema`) for most use cases.', 'score': 0.57894874, 'raw_content': None}, {'url': 'https://ai.pydantic.dev/api/tools/', 'title': 'pydantic_ai.tools - Pydantic AI', 'content': 'Creates a Pydantic tool from a function and a JSON schema. Parameters: Name, Type, Description, Default. function, Callable[..', 'score': 0.5775198, 'raw_content': None}, {'url': 'https://python.useinstructor.com/architecture/', 'title': 'Instructor Architecture', 'content': '[Skip to content](https://python.useinstructor.com/architecture/#instructor-architecture) Response Model Handling (handle_response_model)](https://python.useinstructor.com/architecture/#2-response-model-handling-handle_response_model) Provider-Specific Utilities](https://python.useinstructor.com/architecture/#5-provider-specific-utilities) *   [Mode System](https://python.useinstructor.com/architecture/#mode-system) *   [Adding a New Provider](https://python.useinstructor.com/architecture/#adding-a-new-provider) Response Model Handling (handle_response_model)](https://python.useinstructor.com/architecture/#2-response-model-handling-handle_response_model) Provider-Specific Utilities](https://python.useinstructor.com/architecture/#5-provider-specific-utilities) *   [Mode System](https://python.useinstructor.com/architecture/#mode-system) *   [Adding a New Provider](https://python.useinstructor.com/architecture/#adding-a-new-provider) Instructor Architecture[¶](https://python.useinstructor.com/architecture/#instructor-architecture \"Permanent link\") Client Initialization and Patching[¶](https://python.useinstructor.com/architecture/#1-client-initialization-and-patching \"Permanent link\") Response Model Handling (`handle_response_model`)[¶](https://python.useinstructor.com/architecture/#2-response-model-handling-handle_response_model \"Permanent link\") **Responsibilities:** - Converts Pydantic models to provider-specific schemas - Adds tool/function definitions for providers that support them - Injects system prompts for JSON-mode providers - Handles special cases like parallel tool calls Response Processing (`process_response`)[¶](https://python.useinstructor.com/architecture/#3-response-processing-process_response \"Permanent link\") Error Recovery (`handle_reask_kwargs`)[¶](https://python.useinstructor.com/architecture/#4-error-recovery-handle_reask_kwargs \"Permanent link\") Provider-Specific Utilities[¶](https://python.useinstructor.com/architecture/#5-provider-specific-utilities \"Permanent link\") Mode System[¶](https://python.useinstructor.com/architecture/#mode-system \"Permanent link\") Request Flow Timeline[¶](https://python.useinstructor.com/architecture/#request-flow-timeline \"Permanent link\") Request Flow Detail[¶](https://python.useinstructor.com/architecture/#request-flow-detail \"Permanent link\") ### Example: User Perspective[¶](https://python.useinstructor.com/architecture/#example-user-perspective \"Permanent link\") ### Adding a New Provider[¶](https://python.useinstructor.com/architecture/#adding-a-new-provider \"Permanent link\")', 'score': 0.5617203, 'raw_content': None}], 'response_time': 0.86, 'request_id': '03035f45-3923-4a3e-b878-3a019b9836fd'}, tool_call_id='pyd_ai_d429081c12074cc99ff67eaebb441789', timestamp=datetime.datetime(2025, 9, 3, 22, 29, 49, 304912, tzinfo=datetime.timezone.utc))], instructions=\"You are a helpful AI assistant tasked with getting content for a comprehensive report based on a given outline. Your goal is to research and compile current information for each section and subsection of the outline using the DuckDuckGo MCP server. Follow these steps to complete the task:\\n\\n                1. Review the provided outline\\n\\n                2. For each section and subsection in the outline:\\n                a. Use the DuckDuckGo MCP server to search for current information related to the topic.\\n                b. Review the search results and identify the most relevant and reliable sources.\\n                c. After fetching the content, analyze and summarize the information relevant to the outline section.\\n\\n                3. Write the report section by section, following the structure of the outline. For each section:\\n                a. Provide a brief introduction to the topic.\\n                b. Include relevant information gathered from your research.\\n                c. Ensure that the content is well-organized and flows logically.\\n                d. Use appropriate transitions between subsections and main sections.\\n\\n                4. Throughout the writing process, keep track of the sources you've used. For each source, note:\\n                - The title of the webpage or article\\n                - The author (if available)\\n                - The website name\\n                - The URL\\n                - The date you accessed the information\"))\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:web_deep_researcher:Node: CallToolsNode(model_response=ModelResponse(parts=[ToolCallPart(tool_name='tavily_search', args={'max_results': 3, 'query': 'Pydantic input validation AI models diverse data types structured validation'}, tool_call_id='pyd_ai_36d83dd0b2704bbcbe4ab9953c098f4a'), ToolCallPart(tool_name='tavily_search', args={'query': 'Pydantic structuring validating AI model outputs predictions classifications generated content', 'max_results': 3}, tool_call_id='pyd_ai_2046028444534b7e848e749c5fca6843'), ToolCallPart(tool_name='tavily_search', args={'query': 'Pydantic enforcing structured output from LLMs JSON reliable data', 'max_results': 3}, tool_call_id='pyd_ai_bc43d3968dc44d1ea434910a502f131b'), ToolCallPart(tool_name='tavily_search', args={'query': 'Pydantic defining tools functions LLM agents function calling OpenAI API', 'max_results': 3}, tool_call_id='pyd_ai_767dbf3c53db43fcbd4d4c8c0dcd7041'), ToolCallPart(tool_name='tavily_search', args={'max_results': 3, 'query': 'Pydantic validating LLM prompts responses adherence expected formats'}, tool_call_id='pyd_ai_cebd3985e2a14a6d956eaa53328db8e0'), ToolCallPart(tool_name='tavily_search', args={'query': 'Pydantic data schemas MLOps data pipelines data governance reproducibility', 'max_results': 3}, tool_call_id='pyd_ai_6b034e0526a24c6b8d488c0bf945fd85')], usage=RequestUsage(input_tokens=9542, cache_read_tokens=5978, output_tokens=430, details={'cached_content_tokens': 5978, 'thoughts_tokens': 219, 'text_prompt_tokens': 9542, 'text_cache_tokens': 5978}), model_name='gemini-2.5-flash', timestamp=datetime.datetime(2025, 9, 3, 22, 29, 53, 190683, tzinfo=datetime.timezone.utc), provider_name='google-gla', provider_details={'finish_reason': 'STOP'}, provider_response_id='YcG4aIH9FLe6qtsPlOqYoA0'))\n",
      "INFO:web_deep_researcher:Node: ModelRequestNode(request=ModelRequest(parts=[ToolReturnPart(tool_name='tavily_search', content={'query': 'Pydantic input validation AI models diverse data types structured validation', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://www.netguru.com/blog/data-validation-pydantic', 'title': 'Data Validation with Pydantic - Netguru', 'content': '# Data Validation with Pydantic Pydantic is a powerful Python library that provides robust data validation and settings management features. By leveraging Python type hints, Pydantic ensures that the data conforms to the specified types, making data validation straightforward and efficient. Class methods in Pydantic can be used to add custom validation logic, such as creating validators to ensure model fields meet specific criteria. Additionally, Pydantic allows the implementation of a default value when validation fails, ensuring that your program can handle unexpected data gracefully. >>> pydantic_alcedo = PydanticBird(**bird_data) >>> pydantic_alcedo = PydanticBird(**bird_data) >>> pydantic_alcedo = PydanticBird(**bird_data) >>> pydantic_alcedo = PydanticBird(**bird_data) In addition to Python types, thanks to Pydantic you can also validate a variety of other useful data types such as: >>>pydantic_alcedo = PydanticBird.construct(**bird_data)', 'score': 0.7334523, 'raw_content': None}, {'url': 'https://codilime.com/blog/declarative-data-validation-pydantic/', 'title': 'Effortless Declarative data validation with Pydantic: smarter, cleaner ...', 'content': \"Pydantic uses Python’s type annotations to define data validation rules. * **Type-based Validation:** Fields like int, str, and bool drive validation. * **Default Values:** Missing fields (id, device\\\\_color) are automatically populated in the validated output. Input should be a valid number, unable to parse string as a number [type=float_parsing, input_value='not int', input_type=str] To handle such hierarchical data, Pydantic models should also be organized hierarchically, with one model using other models as types in field annotations. EdgeComputingDevice(id=2025139, model='power edge', dimensions=[20, 30, 25.5], serial_no='ED-2025-139', active=True, device_color='white', network=DeviceNetwork(mac_addresses=['11:22:33:44:55:66', 'AA-BB-CC-DD-EE-FF'], interfaces=['eth0', 'eth1'], ip_addresses={'192.168.16.1', '192.168.0.1'})) * **Nested Models:** Enables the use of one model as a field type within another, simplifying validation of hierarchical data. EdgeComputingDevice(id=2025139, model='POWER EDGE', dimensions=[20, 30, 25.5], serial_no='ED-2025-139', active=True, device_color=Color('white', rgb=(255, 255, 255)), network=DeviceNetwork(mac_addresses=['11:22:33:44:55:66', 'aa:bb:cc:dd:ee:ff'], interfaces=['eth0', 'eth1'], ip_addresses={IPv4Address('192.168.0.1'), IPv4Address('192.168.16.1')}))\", 'score': 0.6922474, 'raw_content': None}, {'url': 'https://dev.to/mechcloud_academy/practical-pydantic-data-transformation-aliases-and-validators-16c2', 'title': 'Practical Pydantic: Data Transformation, Aliases, and Validators', 'content': '# Practical Pydantic: Data Transformation, Aliases, and Validators In our previous post, we explored how Pydantic handles nested models and structured data, enabling robust validation for complex APIs and configurations. This post dives into Pydantic’s advanced features: custom validators, field aliases, and model configuration. Pydantic’s custom validators allow you to define bespoke validation logic using the `@validator` or `@root_validator` decorators (or `@field_validator` and `@model_validator` in Pydantic V2). Let’s combine these features into a cohesive example: a user model with defaults, custom validation, and aliases for an external API. from pydantic import BaseModel, Field, validator, ConfigDict Custom validators, field aliases, and model configuration unlock Pydantic’s full potential for tailored data handling. * Use `@validator` and `@root_validator` for custom logic at field or model levels.', 'score': 0.6469293, 'raw_content': None}], 'response_time': 1.04, 'request_id': '557cb529-f3a8-4f80-8606-fc3d1133f33c'}, tool_call_id='pyd_ai_36d83dd0b2704bbcbe4ab9953c098f4a', timestamp=datetime.datetime(2025, 9, 3, 22, 29, 54, 736616, tzinfo=datetime.timezone.utc)), ToolReturnPart(tool_name='tavily_search', content={'query': 'Pydantic structuring validating AI model outputs predictions classifications generated content', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://pydantic.dev/articles/llm-intro', 'title': 'Steering Large Language Models with Pydantic', 'content': 'In this post, we will discuss validating structured outputs from language models using Pydantic and OpenAI. This schema is used to validate data, but also to generate documentation and even to generate a JSON schema, which is perfect for our use case of generating structured data with language models! Package.model_validate_json(resp.choices[0].message.content) In this example, instead of describing the desired output in plain text, we simply provide the JSON schema for the `Packages` class, which includes a list of `Package` objects: \"parameters\": Packages.model_json_schema(), Packages.model_validate_json( The example we provided above is somewhat contrived, but it illustrates how Pydantic can be utilized to generate structured data from language models. We can model this structured output in Pydantic using the instructor library', 'score': 0.70263255, 'raw_content': None}, {'url': 'https://dev.to/devasservice/a-practical-guide-on-structuring-llm-outputs-with-pydantic-50b4', 'title': 'A Practical Guide on Structuring LLM Outputs with Pydantic', 'content': 'With LLMs like MistralAI (and many others) supporting structured outputs via JSON schemas, combining these tools ensures AI-generated data adheres to strict schemas. Let\\'s consider the example of processing a CSV file containing data about users, which can have some incomplete data, into structured JSON representing user profiles. \"\"\"Call the Mistral API with a prompt and optional system message, expecting a JSON object response.\"\"\" Given the following CSV data, return a JSON array of objects with fields: {Person.model_json_schema()} Given the following CSV data, return a JSON array of objects with fields: {model_json_schema} last_response = call_mistral_json_mode(improved_prompt, system_message=system_message) After retrieving the initial response from the Mistral API, this section tries to parse and validate the LLM\\'s output against the `Person` Pydantic schema.', 'score': 0.6426349, 'raw_content': None}, {'url': 'https://medium.com/@sridevi.gogusetty/building-multi-agent-llm-systems-with-pydanticai-framework-a-step-by-step-guide-to-create-ai-595cfcdede92', 'title': 'Building Multi-Agent LLM Systems with PydanticAI Framework: A ...', 'content': 'Jan 5, 2025·Harnesses the power of Pydantic to validate and structure model outputs, ensuring responses are consistent across runs. Dependency Injection', 'score': 0.6366926, 'raw_content': None}], 'response_time': 1.06, 'request_id': 'ad5d5a6f-7b66-43b2-a0f2-f2f585eaa161'}, tool_call_id='pyd_ai_2046028444534b7e848e749c5fca6843', timestamp=datetime.datetime(2025, 9, 3, 22, 29, 54, 937941, tzinfo=datetime.timezone.utc)), ToolReturnPart(tool_name='tavily_search', content={'query': 'Pydantic enforcing structured output from LLMs JSON reliable data', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://codecut.ai/enforce-structured-outputs-from-llms-with-pydanticai/', 'title': 'Enforce Structured Outputs from LLMs with PydanticAI - CodeCut', 'content': 'If you’re expecting clean, structured data, such as a JSON object with keys like ‘first\\\\_name’, ‘last\\\\_name’, ‘experience’, and ‘primary\\\\_skill’, you may find the model returning values in an unstructured form. input=\"Khuyen Tran is a data scientist with 5 years of experience, skilled in Python and machine learning.\", * Enforce structured JSON outputs from LLMs using Pydantic models instead of parsing unreliable text First, define a `Pydantic` Model that describes the expected structure of your agent’s output: Now, use the `output_type` parameter to connect this model to your agent: result = agent.run_sync(\\'Khuyen Tran is a data scientist with 5 years of experience, skilled in Python and machine learning.\\') first_name=\\'Khuyen\\' last_name=\\'Tran\\' experience_years=5 primary_skill=[\\'Python\\', \\'machine learning\\'] df = pd.DataFrame([result.output.model_dump()])', 'score': 0.82994765, 'raw_content': None}, {'url': 'https://dev.to/devasservice/a-practical-guide-on-structuring-llm-outputs-with-pydantic-50b4', 'title': 'A Practical Guide on Structuring LLM Outputs with Pydantic', 'content': 'With LLMs like MistralAI (and many others) supporting structured outputs via JSON schemas, combining these tools ensures AI-generated data adheres to strict schemas. Let\\'s consider the example of processing a CSV file containing data about users, which can have some incomplete data, into structured JSON representing user profiles. \"\"\"Call the Mistral API with a prompt and optional system message, expecting a JSON object response.\"\"\" Given the following CSV data, return a JSON array of objects with fields: {Person.model_json_schema()} Given the following CSV data, return a JSON array of objects with fields: {model_json_schema} last_response = call_mistral_json_mode(improved_prompt, system_message=system_message) After retrieving the initial response from the Mistral API, this section tries to parse and validate the LLM\\'s output against the `Person` Pydantic schema.', 'score': 0.80014634, 'raw_content': None}, {'url': 'https://datachain.ai/blog/enforcing-json-outputs-in-commercial-llms', 'title': 'Enforcing JSON Outputs in Commercial LLMs - DataChain', 'content': 'Sep 6, 2024·Their “Structured Outputs API” can directly accept a Pydantic model, enabling it to read both the data model and field descriptions effortlessly', 'score': 0.7715858, 'raw_content': None}], 'response_time': 0.96, 'request_id': 'accfd268-3922-487e-a9f8-d39885930806'}, tool_call_id='pyd_ai_bc43d3968dc44d1ea434910a502f131b', timestamp=datetime.datetime(2025, 9, 3, 22, 29, 54, 658543, tzinfo=datetime.timezone.utc)), ToolReturnPart(tool_name='tavily_search', content={'query': 'Pydantic defining tools functions LLM agents function calling OpenAI API', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://www.reddit.com/r/LangChain/comments/1mynq4a/agents_are_just_llm_loop_tools_its_simpler_than/', 'title': \"Agents are just “LLM + loop + tools” (it's simpler than people make it)\", 'content': \"A lot of people overcomplicate AI agents. Strip away the buzzwords, and it's basically: LLM → Loop → Tools. That's it. Last weekend, I broke down a coding\", 'score': 0.67243975, 'raw_content': None}, {'url': 'https://ai.pydantic.dev/api/agent/', 'title': 'pydantic_ai.agent - Pydantic AI', 'content': 'This is the API to use if you want to consume the outputs coming from each LLM model response, or the stream of events coming from the execution of tools. The', 'score': 0.6439794, 'raw_content': None}, {'url': 'https://ai.pydantic.dev/api/tools/', 'title': 'pydantic_ai.tools - Pydantic AI', 'content': \"Tool calls that require approval or external execution. This can be used as an agent's output_type and will be used as the output of the agent run if the model\", 'score': 0.61695945, 'raw_content': None}], 'response_time': 1.01, 'request_id': '2e1ffa02-637a-4c9d-8758-4053bfbac3f8'}, tool_call_id='pyd_ai_767dbf3c53db43fcbd4d4c8c0dcd7041', timestamp=datetime.datetime(2025, 9, 3, 22, 29, 54, 729970, tzinfo=datetime.timezone.utc)), ToolReturnPart(tool_name='tavily_search', content={'query': 'Pydantic validating LLM prompts responses adherence expected formats', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://docs.databricks.com/aws/en/mlflow3/genai/tracing/integrations/pydantic-ai', 'title': 'Tracing PydanticAI | Databricks on AWS', 'content': 'It centers on Pydantic models to enforce structure and validation throughout agent workflows. ... LLM requests logging model name, prompt, parameters & response', 'score': 0.57035583, 'raw_content': None}, {'url': 'https://pub.towardsai.net/how-to-ensure-reliability-in-llm-applications-13302bd7521a', 'title': 'How to Ensure Reliability in LLM Applications', 'content': 'In my examples, I ensure to adhere to my own expected response format, using the <answer></answer tags>. Using markup tags, you can thus ensure a high level', 'score': 0.532839, 'raw_content': None}, {'url': 'https://medium.com/@albertoarrigoni/production-grade-ai-assisted-coding-python-first-patterns-for-data-ml-teams-6880bfc111a6', 'title': 'Production-Grade AI-Assisted Coding: Python-First ...', 'content': 'Prompt the LLM with schema introspection + few-shot exemplars to draft SQL; then verify with: AST parsing (reject disallowed constructs),; plan validation (', 'score': 0.3644614, 'raw_content': None}], 'response_time': 1.02, 'request_id': '504156e2-d802-431f-9e92-73cfc2e2ab13'}, tool_call_id='pyd_ai_cebd3985e2a14a6d956eaa53328db8e0', timestamp=datetime.datetime(2025, 9, 3, 22, 29, 54, 727876, tzinfo=datetime.timezone.utc)), ToolReturnPart(tool_name='tavily_search', content={'query': 'Pydantic data schemas MLOps data pipelines data governance reproducibility', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://www.linkedin.com/posts/shantanuladhwe_ive-set-up-10-ai-production-environments-activity-7368870388241510400-EzeU', 'title': \"Shantanu Ladhwe's Post - LinkedIn\", 'content': '(with a notebook to make sure you setup everything well) 🔹 𝗪𝗵𝗮𝘁’𝘀 𝗶𝗻𝘀𝗶𝗱𝗲 (𝗯𝘂𝗶𝗹𝗱𝗲𝗿-𝗳𝗶𝗿𝘀𝘁) 1️⃣ 𝗙𝗮𝘀𝘁𝗔𝗣𝗜 𝗮𝗽𝗽 → clean src/ with routers / services / repositories / schemas ↳ async-ready, typed Pydantic models, structured logging & config 2️⃣ 𝗗𝗮𝘁𝗮𝗯𝗮𝘀𝗲 → PostgreSQL + SQLAlchemy + Alembic ↳ migrations, seeds, env-driven settings 3️⃣ 𝗦𝗲𝗮𝗿𝗰𝗵 & 𝘃𝗲𝗰𝘁𝗼𝗿𝘀 → OpenSearch (BM25 + vector) wired for RAG ↳ index templates you can extend 4️⃣ 𝗟𝗟𝗠 𝗵𝗼𝗼𝗸𝘂𝗽 → Ollama local endpoints to start fast ↳ swap later for your hosted model 5️⃣ 𝗣𝗶𝗽𝗲𝗹𝗶𝗻𝗲𝘀 → Airflow project scaffold for data jobs ↳ optional; replace with your scheduler if you prefer 6️⃣ 𝗗𝗲𝘃𝗫 & 𝗼𝗽𝘀 → uv, ruff, pre-commit, Makefile, Docker/Compose ↳ reproducible installs, linting, one-command spin-up 7️⃣ 𝗤𝘂𝗮𝗹𝗶𝘁𝘆 & 𝗿𝗲𝗮𝗱𝗶𝗻𝗲𝘀𝘀 → health checks, tracing hooks, config management ↳ made for scaling, collaboration, and real-world debugging 🔹 𝗤𝘂𝗶𝗰𝗸 𝘀𝘁𝗮𝗿𝘁 1️⃣ Clone & copy .env.example → .env 2️⃣ uv sync to install deps 3️⃣ docker compose up -d (FastAPI, Airflow, Postgres, OpenSearch, Ollama) 🔹 𝗪𝗵𝘆 𝘁𝗵𝗶𝘀 𝗺𝗮𝘁𝘁𝗲𝗿𝘀 → Build RAG/Agents like a backend, not a demo. ➕ Follow me, Shantanu for production AI/ML/MLOps & careers ➕Join 20.000+ AI/ML builders here: https://lnkd.in/ds_SzEUH', 'score': 0.53531766, 'raw_content': None}, {'url': 'https://omyeole.com/work/house_price_predictor', 'title': 'House Price Predictor - Om Yeole', 'content': 'A production-grade ML pipeline for predicting house prices, featuring modular architecture, MLOps integration, and real-time model serving.', 'score': 0.15194441, 'raw_content': None}, {'url': 'https://huggingface.co/papers?q=modular%20collaborative%20framework', 'title': 'Daily Papers - Hugging Face', 'content': 'Its modular architecture ensures seamless integration of new data modalities while maintaining strict data governance, marking a significant advancement in', 'score': 0.07272241, 'raw_content': None}], 'response_time': 1.22, 'request_id': '20482ccf-0e6d-4fd4-8ad4-c4a8f124de01'}, tool_call_id='pyd_ai_6b034e0526a24c6b8d488c0bf945fd85', timestamp=datetime.datetime(2025, 9, 3, 22, 29, 54, 934881, tzinfo=datetime.timezone.utc))], instructions=\"You are a helpful AI assistant tasked with getting content for a comprehensive report based on a given outline. Your goal is to research and compile current information for each section and subsection of the outline using the DuckDuckGo MCP server. Follow these steps to complete the task:\\n\\n                1. Review the provided outline\\n\\n                2. For each section and subsection in the outline:\\n                a. Use the DuckDuckGo MCP server to search for current information related to the topic.\\n                b. Review the search results and identify the most relevant and reliable sources.\\n                c. After fetching the content, analyze and summarize the information relevant to the outline section.\\n\\n                3. Write the report section by section, following the structure of the outline. For each section:\\n                a. Provide a brief introduction to the topic.\\n                b. Include relevant information gathered from your research.\\n                c. Ensure that the content is well-organized and flows logically.\\n                d. Use appropriate transitions between subsections and main sections.\\n\\n                4. Throughout the writing process, keep track of the sources you've used. For each source, note:\\n                - The title of the webpage or article\\n                - The author (if available)\\n                - The website name\\n                - The URL\\n                - The date you accessed the information\"))\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:web_deep_researcher:Node: CallToolsNode(model_response=ModelResponse(parts=[ToolCallPart(tool_name='tavily_search', args={'query': 'pydantic for AI applications role benefits use cases modern AI development', 'max_results': 5}, tool_call_id='pyd_ai_6f84755e728d42bd8a4ede8b23051395')], usage=RequestUsage(input_tokens=14192, cache_read_tokens=9033, output_tokens=990, details={'cached_content_tokens': 9033, 'thoughts_tokens': 954, 'text_prompt_tokens': 14192, 'text_cache_tokens': 9033}), model_name='gemini-2.5-flash', timestamp=datetime.datetime(2025, 9, 3, 22, 30, 0, 988844, tzinfo=datetime.timezone.utc), provider_name='google-gla', provider_details={'finish_reason': 'STOP'}, provider_response_id='acG4aJ2cCJS7qtsP3ojJsQw'))\n",
      "INFO:web_deep_researcher:Node: ModelRequestNode(request=ModelRequest(parts=[ToolReturnPart(tool_name='tavily_search', content={'query': 'pydantic for AI applications role benefits use cases modern AI development', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://szeyusim.medium.com/a-comprehensive-guide-on-agent-development-with-pydantic-ai-beginner-to-advanced-12d90e0ba1a6', 'title': 'A Comprehensive Guide on Agent Development with Pydantic AI', 'content': 'Mar 2, 2025·Pydantic AI is a Python framework designed to simplify the development of production-grade applications utilizing Generative AI.', 'score': 0.6867278, 'raw_content': None}, {'url': 'https://www.civo.com/learn/simplifying-ai-development-with-pydanticai', 'title': 'Simplifying AI Development with PydanticAI and Serverless ...', 'content': 'PydanticAI is a powerful tool that provides type safety and structured outputs for AI models, ensuring that your application receives data in a predictable format . At the same time, serverless functions abstract away the intricacies of managing servers and infrastructure.', 'score': 0.68344116, 'raw_content': None}, {'url': 'https://towardsai.net/p/artificial-intelligence/build-smarter-agentic-ai-apps-with-pydantic-a-beginners-guide', 'title': \"Build Smarter Agentic AI Apps with Pydantic: A Beginner's Guide\", 'content': '6 days ago·This guide explains how Pydantic can be utilized for data validation and parsing in Python applications, emphasizing its ability to check and', 'score': 0.6141863, 'raw_content': None}, {'url': 'https://tech.appunite.com/posts/understanding-pydantic-ai-a-powerful-alternative-to-lang-chain-and-llama-index', 'title': 'Understanding Pydantic-AI: A Powerful Alternative to LangChain ...', 'content': \"Pydantic-AI is a Python agent framework designed to make it less painful to build production-grade applications with Generative AI. ## The Testing Challenge: Evaluating AI Agents Pydantic-AI's evaluation framework (pydantic\\\\_evals) helps address this challenge by providing tools to systematically test and evaluate your agents. Pydantic-AI provides a structured framework for evaluating your agents, centered around three main components: A powerful feature of Pydantic-AI's evaluation framework is the ability to define expected outputs for each test case. Pydantic-AI makes it easy to create custom evaluators for specific aspects of your agent's behavior. This is where LLM Judge comes in - it uses another language model to evaluate the outputs of your agent based on customized rubrics.\", 'score': 0.5106191, 'raw_content': None}, {'url': 'https://cuttlesoft.com/blog/2024/12/11/pydantic-ai-building-smarter-type-safe-ai-agents/', 'title': 'Pydantic.ai: Building Smarter, Type-Safe AI Agents - Cuttlesoft', 'content': \"# Pydantic.ai: Building Smarter, Type-Safe AI Agents Rather than just watching from the sidelines, they decided to tackle the challenges of AI development head-on, leveraging their deep expertise in data validation to build a framework that addresses the real-world complexities developers face when working with LLMs. The result is Pydantic.ai, a ground-up rethinking of how AI frameworks should work. First, let's build a recipe generator that showcases Pydantic.ai's structured output capabilities. from pydantic_ai import Agent Since Pydantic.ai handles all the validation and type-checking, you can focus on building features rather than wrestling with data structures. The AI isn't just making educated guesses anymore – it's making recommendations based on real-time weather data, filtered through business logic that we control.\", 'score': 0.49670476, 'raw_content': None}], 'response_time': 2.74, 'request_id': 'e3057ed2-a870-40b5-84d1-65cef1c54d7a'}, tool_call_id='pyd_ai_6f84755e728d42bd8a4ede8b23051395', timestamp=datetime.datetime(2025, 9, 3, 22, 30, 4, 324293, tzinfo=datetime.timezone.utc))], instructions=\"You are a helpful AI assistant tasked with getting content for a comprehensive report based on a given outline. Your goal is to research and compile current information for each section and subsection of the outline using the DuckDuckGo MCP server. Follow these steps to complete the task:\\n\\n                1. Review the provided outline\\n\\n                2. For each section and subsection in the outline:\\n                a. Use the DuckDuckGo MCP server to search for current information related to the topic.\\n                b. Review the search results and identify the most relevant and reliable sources.\\n                c. After fetching the content, analyze and summarize the information relevant to the outline section.\\n\\n                3. Write the report section by section, following the structure of the outline. For each section:\\n                a. Provide a brief introduction to the topic.\\n                b. Include relevant information gathered from your research.\\n                c. Ensure that the content is well-organized and flows logically.\\n                d. Use appropriate transitions between subsections and main sections.\\n\\n                4. Throughout the writing process, keep track of the sources you've used. For each source, note:\\n                - The title of the webpage or article\\n                - The author (if available)\\n                - The website name\\n                - The URL\\n                - The date you accessed the information\"))\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:web_deep_researcher:Node: CallToolsNode(model_response=ModelResponse(parts=[ToolCallPart(tool_name='tavily_search', args={'query': 'Pydantic data validation settings management library core features'}, tool_call_id='pyd_ai_db82dfef3af24b1393cc6bdd8ddf99a8')], usage=RequestUsage(input_tokens=15219, cache_read_tokens=14060, output_tokens=724, details={'cached_content_tokens': 14060, 'thoughts_tokens': 698, 'text_prompt_tokens': 15219, 'text_cache_tokens': 14060}), model_name='gemini-2.5-flash', timestamp=datetime.datetime(2025, 9, 3, 22, 30, 9, 181099, tzinfo=datetime.timezone.utc), provider_name='google-gla', provider_details={'finish_reason': 'STOP'}, provider_response_id='ccG4aMygFJqjqtsP3vyBwA8'))\n",
      "INFO:web_deep_researcher:Node: ModelRequestNode(request=ModelRequest(parts=[ToolReturnPart(tool_name='tavily_search', content={'query': 'Pydantic data validation settings management library core features', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://github.com/pydantic/pydantic-core', 'title': 'Core validation logic for pydantic written in rust - GitHub', 'content': '| python/pydantic\\\\_core | | python/pydantic\\\\_core | Add\") `exclude_computed_fields` serialization option (\")#1780)\") | Aug 11, 2025 | | .codecov.yml | | .codecov.yml | Default validator (  * WIP, default validator  * fixing tests  * use DefaultValidator in arguments  * simplifying schema for type-dict defaults  * support omit on lists  * improve coverage  * add benchmark  * fix tests  * test compatibility with pytest-benchmark  * fix tests  * supporting tuples  * small tweaks  * more tests and tweak codecov.yml  * fix tests  * support for dict omit\")#242)  * WIP, default validator  * fixing tests  * use DefaultValidator in arguments  * simplifying schema for type-dict defaults  * support omit on lists  * improve coverage  * add benchmark  * fix tests  * test compatibility with pytest-benchmark  * fix tests  * supporting tuples  * small tweaks  * more tests and tweak codecov.yml  * fix tests  * support for dict omit\") | Sep 22, 2022 |', 'score': 0.61112726, 'raw_content': None}, {'url': 'https://pypi.org/project/pydantic-client/', 'title': 'pydantic-client - PyPI', 'content': 'The library automatically validates responses against Pydantic models when specified as return types in the method definitions. Error Handling. HTTP errors are', 'score': 0.49553296, 'raw_content': None}, {'url': 'https://github.com/orgs/pydantic/repositories', 'title': 'Repositories - Pydantic - GitHub', 'content': '# Pydantic * #### logfire * #### datafusion •4•29•0•2•Updated Aug 28, 2025Aug 28, 2025 * #### pydantic •9•82•6•2•Updated Aug 26, 2025Aug 26, 2025 * #### logfire-rust •4•47•8•1•Updated Aug 21, 2025Aug 21, 2025 Core validation logic for pydantic written in rust •16•0•0•0•Updated Aug 21, 2025Aug 21, 2025 •4•46•7•1•Updated Aug 21, 2025Aug 21, 2025 •44•448•13•6•Updated Aug 20, 2025Aug 20, 2025 •75•280•34•2•Updated Aug 15, 2025Aug 15, 2025 •18•104•4•2•Updated Aug 8, 2025Aug 8, 2025 •1•10•0•0•Updated Aug 5, 2025Aug 5, 2025 •26•0•0•0•Updated Aug 4, 2025Aug 4, 2025 •49•0•0•0•Updated Aug 4, 2025Aug 4, 2025 •24•231•6•0•Updated Aug 4, 2025Aug 4, 2025 Talks about Pydantic, PydanticAI and Logfire •6•53•0•2•Updated Aug 2, 2025Aug 2, 2025 A Visual Studio Code extension for Pydantic Logfire A Language Server Protocol implementation for Pydantic Logfire', 'score': 0.44536126, 'raw_content': None}, {'url': 'https://latenode.com/blog/11-open-source-ai-agent-frameworks-that-will-transform-your-development-2025-complete-guide', 'title': '11 Open Source AI Agent Frameworks That Will Transform Your ...', 'content': 'The standout feature of Pydantic AI Agents is its ability to enforce structured, validated responses from language models. Instead of relying on chance for', 'score': 0.34448805, 'raw_content': None}, {'url': 'https://lawrencemcdaniel.com/data-science', 'title': 'Data Science - Lawrence McDaniel', 'content': 'A Python library for scientific computing, offering advanced functions for math, optimization, and signal processing. A Python library for data validation and', 'score': 0.29894704, 'raw_content': None}], 'response_time': 1.0, 'request_id': '59bee9a4-e961-4aba-9ed3-f5c73c20f39f'}, tool_call_id='pyd_ai_db82dfef3af24b1393cc6bdd8ddf99a8', timestamp=datetime.datetime(2025, 9, 3, 22, 30, 10, 652336, tzinfo=datetime.timezone.utc))], instructions=\"You are a helpful AI assistant tasked with getting content for a comprehensive report based on a given outline. Your goal is to research and compile current information for each section and subsection of the outline using the DuckDuckGo MCP server. Follow these steps to complete the task:\\n\\n                1. Review the provided outline\\n\\n                2. For each section and subsection in the outline:\\n                a. Use the DuckDuckGo MCP server to search for current information related to the topic.\\n                b. Review the search results and identify the most relevant and reliable sources.\\n                c. After fetching the content, analyze and summarize the information relevant to the outline section.\\n\\n                3. Write the report section by section, following the structure of the outline. For each section:\\n                a. Provide a brief introduction to the topic.\\n                b. Include relevant information gathered from your research.\\n                c. Ensure that the content is well-organized and flows logically.\\n                d. Use appropriate transitions between subsections and main sections.\\n\\n                4. Throughout the writing process, keep track of the sources you've used. For each source, note:\\n                - The title of the webpage or article\\n                - The author (if available)\\n                - The website name\\n                - The URL\\n                - The date you accessed the information\"))\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:web_deep_researcher:Node: CallToolsNode(model_response=ModelResponse(parts=[ToolCallPart(tool_name='tavily_search', args={'query': 'pydantic FastAPI Flask AI APIs request response validation'}, tool_call_id='pyd_ai_e9507226b54546f6abcc7f64f4e989d5'), ToolCallPart(tool_name='tavily_search', args={'query': 'pydantic data ingestion training data validation cleaning'}, tool_call_id='pyd_ai_67d9a0cd0be24f39ad4c92e500fb6763'), ToolCallPart(tool_name='tavily_search', args={'query': 'pydantic AI agents chatbots structuring conversational states user inputs agent actions'}, tool_call_id='pyd_ai_7ce39cc705aa4244aa02fcc2c0a6df85'), ToolCallPart(tool_name='tavily_search', args={'query': 'pydantic LangChain LlamaIndex OpenAI Python client integration'}, tool_call_id='pyd_ai_723e7558b8ac497fa7c6be3bdcf52702'), ToolCallPart(tool_name='tavily_search', args={'query': 'pydantic learning curve type hints'}, tool_call_id='pyd_ai_34a88ed574f541c5b55c0a1940af8997'), ToolCallPart(tool_name='tavily_search', args={'query': 'pydantic performance considerations complex validation high-throughput systems'}, tool_call_id='pyd_ai_0aeedbab5054484da569080817eb557a'), ToolCallPart(tool_name='tavily_search', args={'query': 'pydantic V1 vs V2 evolving features AI'}, tool_call_id='pyd_ai_6a171cf3b61d4aaf8c5ba3042105527d'), ToolCallPart(tool_name='tavily_search', args={'query': 'pydantic future outlook structured data type safety AI landscape'}, tool_call_id='pyd_ai_a7f29bb4075b48e3803af9fec9133584'), ToolCallPart(tool_name='tavily_search', args={'query': 'pydantic in AI applications benefits challenges'}, tool_call_id='pyd_ai_0932946b49534a8c8bffc2b6d1703b79')], usage=RequestUsage(input_tokens=16473, cache_read_tokens=15076, output_tokens=1616, details={'cached_content_tokens': 15076, 'thoughts_tokens': 1380, 'text_prompt_tokens': 16473, 'text_cache_tokens': 15076}), model_name='gemini-2.5-flash', timestamp=datetime.datetime(2025, 9, 3, 22, 30, 18, 745655, tzinfo=datetime.timezone.utc), provider_name='google-gla', provider_details={'finish_reason': 'STOP'}, provider_response_id='esG4aPi8Npu8qtsPrrD2sQw'))\n",
      "INFO:web_deep_researcher:Node: ModelRequestNode(request=ModelRequest(parts=[ToolReturnPart(tool_name='tavily_search', content={'query': 'pydantic FastAPI Flask AI APIs request response validation', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://medium.com/@hadiyolworld007/from-prototype-to-production-fastapi-in-the-era-of-ai-native-apps-faf7b0b566f1', 'title': 'From Prototype to Production: FastAPI in the Era of AI ...', 'content': 'Async by default → perfect for high-concurrency tasks like streaming LLM responses. Pydantic models → ensures reliable data validation between AI services.', 'score': 0.6729558, 'raw_content': None}, {'url': 'https://dev.to/atifwattoo/django-vs-flask-vs-fastapi-37n4', 'title': 'Django vs Flask vs FastAPI', 'content': \"FastAPI is a modern, high-performance web framework for building APIs with Python. It's built on Starlette (for the web parts) and Pydantic (for data validation)\", 'score': 0.6102915, 'raw_content': None}, {'url': 'https://www.geeksforgeeks.org/python/rest-api-with-fastapi/', 'title': 'REST API with FastAPI', 'content': 'FastAPI, a modern Python framework, makes building ****REST APIs**** easier with high performance, built-in validation and interactive documentation. FastAPI is a web framework for Python that makes building REST APIs simple. REST APIs can use multiple layers (like load balancers, security layers or proxy servers) without affecting how the client interacts with the service. Whenever an app communicates with the internet to fetch or send data, a REST API is usually at work behind the scenes. Python was created 1991 with focus on code readability and express concepts in fewer lines of code.Simple and readable syntax makes it beginner-friendly.Runs seamlessly on Windows, macOS and Linux.Includes libraries for tasks like web development, data analysis and machine learning.Variable types ar', 'score': 0.52130115, 'raw_content': None}, {'url': 'https://zenvanriel.nl/ai-engineer-blog/fastapi-vs-flask-for-ai-applications/', 'title': 'FastAPI vs Flask for AI Applications', 'content': 'FastAPI vs Flask for AI Applications FastAPI vs Flask for AI Applications When implementing AI solutions, the framework you choose for your API layer significantly impacts development speed, performance, and maintainability. These differences create distinct development experiences when implementing AI applications. Performance impact varies based on your specific implementation pattern, but FastAPI typically offers advantages for concurrent AI operations. Consider FastAPI for your AI implementation when: Consider Flask for your AI implementation when: Many teams standardize on FastAPI for new AI projects while maintaining existing Flask applications. While both frameworks can successfully implement AI applications, FastAPI’s modern features and performance advantages make it an increasingly popular choice for new AI projects. Want to learn more about implementing AI applications with FastAPI, Flask, or other frameworks?', 'score': 0.46089602, 'raw_content': None}, {'url': 'https://www.netguru.com/blog/python-flask-versus-fastapi', 'title': 'FastAPI vs Flask: Which Should You Choose?', 'content': 'Flask and FastAPI are popular Python micro-frameworks used to build small scale websites or applications based on data science and machine learning. While Flask has been a long-standing favorite for its simplicity and extensibility, FastAPI is gaining traction for its speed and modern features designed for building APIs. Understanding the nuances of each framework will empower developers to make an informed decision for their web development needs. In the realm of Python web development, FastAPI is often contrasted with Flask, highlighting their unique features, performance capabilities, and suitability for various project requirements. Flask and FastAPI are two prominent Python web frameworks, each with its own set of features and advantages. * **Micro Web Framework**: Flask is a lightweight and easy-to-use micro web framework that provides the essentials for web development.', 'score': 0.44102347, 'raw_content': None}], 'response_time': 1.04, 'request_id': '02269258-d0be-487a-a4c3-95a6624a5e7c'}, tool_call_id='pyd_ai_e9507226b54546f6abcc7f64f4e989d5', timestamp=datetime.datetime(2025, 9, 3, 22, 30, 20, 567731, tzinfo=datetime.timezone.utc)), ToolReturnPart(tool_name='tavily_search', content={'query': 'pydantic data ingestion training data validation cleaning', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://www.prefect.io/blog/what-is-pydantic-validating-data-in-python', 'title': 'What is Pydantic? Validating Data in Python - Prefect', 'content': 'In this article, I’ll talk about what Pydantic is, how to use it in your code, and how to couple it with Prefect to reduce application errors and improve data quality in your most critical workflows. **Pydantic** is a data validation package for Python. This code uses Pydantic’s BaseModel class to create a subclass - a\\xa0 model that represents your application’s data. Strictly speaking, do you *need* Pydantic to perform data validation? Once you load the JSON file from the URL, you can use the static method model\\\\_validate\\\\_json() on your Pydantic model to create a new Customer object. I’ve shown how easy it is to incorporate Pydantic into your Python code - and how you can use Pydantic and Prefect together to create robust workflows.', 'score': 0.98526, 'raw_content': None}, {'url': 'https://medium.com/@durgarao3d6/clean-and-reliable-data-handling-in-python-using-pydantic-3aef7c42d2f8', 'title': 'Clean and Reliable Data Handling in Python Using Pydantic - Medium', 'content': 'May 18, 2025·In Python, Pydantic is a powerful library that makes data validation and parsing easy, especially when working with APIs and structured data.', 'score': 0.98383, 'raw_content': None}, {'url': 'https://www.reddit.com/r/Python/comments/16xnhim/what_problems_does_pydantic_solves_and_how_should/', 'title': 'What problems does pydantic solves? and How should it be used', 'content': 'What problems does pydantic solves? and How should it be used : r/Python and How should it be used : r/Python Image 1: r/Python icon Go to Python r/Python Image 3: r/Python iconr/PythonImage 4: pythonLogoImage 5: pythonLogo The official Python community for Reddit! --- If you have questions or are new to Python use r/LearnPython What problems does pydantic solves? Asking this question, Because, in the first look pydantic looks helpful. Well, the purpose of my question is not to question the usuability of pydantic. New to Reddit? Top Posts *   Reddit reReddit: Top posts of October 2, 2023 * * * *   Reddit reReddit: Top posts of October 2023 * * * *   Reddit reReddit: Top posts of 2023 * * *', 'score': 0.97733, 'raw_content': None}, {'url': 'https://data-ai.theodo.com/en/technical-blog/boost-your-data-projects-with-pydantic-validation-and-efficiency', 'title': 'Boost Your Data Projects with Pydantic: Validation and Efficiency', 'content': '# Boost Your Data Projects with Pydantic: Validation and Efficiency **Pydantic**, a **Python** tool, defines **data models** with specific types and formats, providing a declarative way to structure data and facilitating validation and transformation. This snippet highlights how Pydantic ensures **data** **conformity** with the standardized contact object after mapping using the data model `PivotContact`. ### **How to validate data with Pydantic ?** The project data model requires data validation such as contact address. Pydantic base model enables us to build an `Address` class for the Addresses field and a **custom validator** for date type in contact data. pydantic data validation Here, Pydantic empowers **data accuracy** and **consistency** by enforcing validation protocols, ensuring the integrity of critical fields like addresses and dates.', 'score': 0.97525, 'raw_content': None}, {'url': 'https://medium.com/@CodeWithHannan/data-validation-and-schema-enforcement-in-python-using-pydantic-886ab64fecba', 'title': 'Data Validation and Schema Enforcement in Python Using Pydantic', 'content': 'Data Validation and Schema Enforcement in Python Using Pydantic | by Code With Hannan | Jun, 2025 | Medium Data Validation and Schema Enforcement in Python Using Pydantic **Pydantic** is a powerful Python library that provides **runtime data validation** and **schema enforcement** using Python type hints. This blog explores how Pydantic works, why it’s so effective for schema enforcement, and how to use it in **real-world Python systems**. Image 8: Python in Plain English Image 9: Advanced Python Type Hints for Scalable Codebases Image 15: 10 Python Workflows for High-Volume ETL Pipelines Image 19: How I Finally Stopped Fighting Git and Built a Fast, Clean Python Project in Minutes Image 24: Top Python Libraries Image 25: My Exact Setup for Building a Scalable Python Web App 🚀', 'score': 0.97086, 'raw_content': None}], 'response_time': 1.82, 'request_id': 'e1b6d2ed-2de8-49f1-8da9-7f3682ad6150'}, tool_call_id='pyd_ai_67d9a0cd0be24f39ad4c92e500fb6763', timestamp=datetime.datetime(2025, 9, 3, 22, 30, 21, 163700, tzinfo=datetime.timezone.utc)), ToolReturnPart(tool_name='tavily_search', content={'query': 'pydantic AI agents chatbots structuring conversational states user inputs agent actions', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://latenode.com/blog/11-open-source-ai-agent-frameworks-that-will-transform-your-development-2025-complete-guide', 'title': '11 Open Source AI Agent Frameworks That Will Transform ...', 'content': 'The standout feature of Pydantic AI Agents is its ability to enforce structured, validated responses from language models. Instead of relying on chance for', 'score': 0.54666615, 'raw_content': None}, {'url': 'https://medium.com/@tam.tamanna18/why-ai-frameworks-langchain-crewai-pydanticai-and-others-fail-in-production-c6292b98ee59', 'title': 'Why AI Frameworks (LangChain, CrewAI, PydanticAI and ...', 'content': 'In the fast-paced world of AI development, frameworks like LangChain, CrewAI, AutoGen, and even newer ones like PydanticAI promise to make building AI agents easy and quick. This article dives into why these popular frameworks are letting production teams down and introduces Atomic Agents as a fresh, developer-friendly alternative. These abstractions are like pre-built shortcuts that hide the complex code underneath, letting developers quickly create chatbots, question-answering systems, or other AI apps. ### Let me tell you\\u200a—\\u200amost AI agents are like that one coworker who talks a big game but delivers nothing. ## Building a Temporal AI Agent to Optimize Evolving Knowledge Bases in Modern RAG Systems ## Agentic AI vs. ## Building an Intelligent Deep Research Agent: From Query to Insights with AI-Powered Analysis using…', 'score': 0.27890983, 'raw_content': None}, {'url': 'https://devot.team/blog/agentic-ai', 'title': 'Agentic AI: From Chains to Autonomous, Multi ...', 'content': 'Back to blogs # Agentic AI: From Chains to Autonomous, Multi-Agent Frameworks 8 min read 8 min read This evolution is powered by frameworks like LangChain (available in both Python and TypeScript) and specialized tools like LangSmith, LangGraph, and Pydantic-AI that enable sophisticated multi-agent architectures. LangGraph extends the capabilities of LangChain by introducing a graph-based orchestration framework for agentic workflows. ### Pydantic-AI In **holonic MAS**, each “holon” behaves both as an autonomous agent and as a component of a larger agent, echoing biological systems like organs within a body. ### Networked & cooperative/competitive multi-agent systems Enterprises are already leveraging these tools to automate customer support, knowledge retrieval, and decision workflows, marking a fundamental shift in AI strategy and integration across business functions. Keep reading Similar blogs for further insights', 'score': 0.22167104, 'raw_content': None}, {'url': 'https://huggingface.co/papers?q=agile%20methodologies', 'title': 'Daily Papers', 'content': 'By interacting with a conversational agent, users can steer the creation of user stories and the extraction of competency questions, while receiving', 'score': 0.19333042, 'raw_content': None}, {'url': 'https://www.researchgate.net/publication/383032644_NLP_Chatbot_For_Order_Assistance_Using_Dialogflow', 'title': '(PDF) NLP Chatbot For Order Assistance Using Dialogflow', 'content': 'This paper explains the development of an order assistant conversational AI chatbot with Dialogflow. The chatbot will be created for an improved experience.', 'score': 0.10737215, 'raw_content': None}], 'response_time': 1.33, 'request_id': '9d75630f-ccc5-4025-a51f-c347acfab5f7'}, tool_call_id='pyd_ai_7ce39cc705aa4244aa02fcc2c0a6df85', timestamp=datetime.datetime(2025, 9, 3, 22, 30, 20, 562123, tzinfo=datetime.timezone.utc)), ToolReturnPart(tool_name='tavily_search', content={'query': 'pydantic LangChain LlamaIndex OpenAI Python client integration', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://docs.llamaindex.ai/en/stable/examples/output_parsing/openai_pydantic_program/', 'title': 'OpenAI Pydantic Program - LlamaIndex', 'content': 'output_cls=Album, prompt_template_str=prompt_template_str, verbose=True output\\\\_cls=Album, prompt\\\\_template\\\\_str=prompt\\\\_template\\\\_str, verbose=True output_cls=Album, prompt_template_str=prompt_template_str, verbose=True Instead of waiting for the Function Call to generate the entire JSON, we can use the `stream_partial_objects()` method of the `program` to stream valid intermediate instances of the Pydantic Output class as soon as they\\'re available 🔥 Function call: Album with args: {\"name\": \"Spring\", \"artist\": \"Various Artists\", \"songs\": [{\"title\": \"Blossom\", \"length_seconds\": 180}, {\"title\": \"Sunshine\", \"length_seconds\": 240}, {\"title\": \"Renewal\", \"length_seconds\": 200}]} Function call: Album with args: {\"name\": \"Fall\", \"artist\": \"Autumn Leaves\", \"songs\": [{\"title\": \"Golden Days\", \"length_seconds\": 210}, {\"title\": \"Harvest Moon\", \"length_seconds\": 240}, {\"title\": \"Crisp Air\", \"length_seconds\": 190}]} Function call: Album with args: {\"name\": \"Winter\", \"artist\": \"Snowflakes\", \"songs\": [{\"title\": \"Frosty Morning\", \"length_seconds\": 190}, {\"title\": \"Snowfall\", \"length_seconds\": 220}, {\"title\": \"Cozy Nights\", \"length_seconds\": 250}]}', 'score': 0.9855, 'raw_content': None}, {'url': 'https://ai.pydantic.dev/', 'title': 'Pydantic AI', 'content': \"*Agent Framework / shim to use Pydantic with LLMs* Pydantic AI is a Python agent framework designed to make it less painful to build production grade Similarly, virtually every agent framework and LLM library in Python uses Pydantic Validation, yet when we began to use LLMs in Pydantic Logfire, we couldn't find anything that gave us the same feeling. from pydantic_ai import Agent Here is a concise example using Pydantic AI to build a support agent for a bank: from pydantic_ai import Agent, RunContext To understand the flow of the above runs, we can watch the agent in action using Pydantic Logfire. from pydantic_ai import Agent, RunContext The Pydantic AI documentation is available in the llms.txt format.\", 'score': 0.98339, 'raw_content': None}, {'url': 'https://docs.llamaindex.ai/en/stable/examples/llm/openai/', 'title': 'OpenAI - LlamaIndex', 'content': 'OpenAI models have native support for function calling. This conveniently integrates with LlamaIndex tool abstractions, letting you plug in any arbitrary Python', 'score': 0.97958, 'raw_content': None}, {'url': 'https://logfire.pydantic.dev/docs/integrations/', 'title': 'Integrations - Pydantic Logfire Documentation', 'content': 'Integrations - Pydantic Logfire Documentation Pydantic Logfire Documentation  *   Why Logfire? *   Integrate Logfire *   Logfire logfire.instrument_fastapi() logfire.instrument_httpx() | Package | Type | Logfire Instrument Call / Notes | | Anthropic | AI | `logfire.instrument_anthropic()` | | ASGI | Web Framework Interface | `logfire.instrument_asgi()` | | Django | Web Framework | `logfire.instrument_django()` | | FastAPI | Web Framework | `logfire.instrument_fastapi()` | | Flask | Web Framework | `logfire.instrument_flask()` | | MySQL | Database | `logfire.instrument_mysql()` | | OpenAI | AI | `logfire.instrument_openai()` | | Starlette | Web Framework | `logfire.instrument_starlette()` | | WSGI | Web Framework Interface | `logfire.instrument_wsgi()` | Many of the integrations documented in the previous section are based upon the OpenTelemetry instrumentation packages with first-class support built into **Logfire**.', 'score': 0.97474, 'raw_content': None}, {'url': 'https://github.com/pydantic/pydantic-ai', 'title': 'Agent Framework / shim to use Pydantic with LLMs - GitHub', 'content': \"GitHub - pydantic/pydantic-ai: Agent Framework / shim to use Pydantic with LLMs Image 9: Pydantic AI Pydantic AI is a Python agent framework designed to make it less painful to build production grade applications with Generative AI. Similarly, virtually every agent framework and LLM library in Python uses Pydantic Validation, yet when we began to use LLMs in Pydantic Logfire, we couldn't find anything that gave us the same feeling. Why use Pydantic AI from pydantic\\\\_ai import Agent Here is a concise example using Pydantic AI to build a support agent for a bank: from pydantic\\\\_ai import Agent, RunContext # This pydantic model defines the structure of the output returned by the agent.\", 'score': 0.97085, 'raw_content': None}], 'response_time': 1.28, 'request_id': 'e3a43303-36ae-4746-9b96-c3ccdb6ced83'}, tool_call_id='pyd_ai_723e7558b8ac497fa7c6be3bdcf52702', timestamp=datetime.datetime(2025, 9, 3, 22, 30, 20, 558347, tzinfo=datetime.timezone.utc)), ToolReturnPart(tool_name='tavily_search', content={'query': 'pydantic learning curve type hints', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://docs.pydantic.dev/2.0/usage/types/types/', 'title': 'Types Overview - Pydantic', 'content': 'Data validation using Python type hints. ... Where possible Pydantic uses standard library types to define fields, thus smoothing the learning curve.', 'score': 0.8021998, 'raw_content': None}, {'url': 'https://docs.pydantic.dev/latest/concepts/types/', 'title': 'Types - Pydantic', 'content': 'Data validation using Python type hints. ... Where possible Pydantic uses standard library types to define fields, thus smoothing the learning curve.', 'score': 0.80070794, 'raw_content': None}, {'url': 'https://medium.com/@sulbha.jindal/pydantic-python-library-for-data-validation-cd8b9185fc4d', 'title': 'Pydantic — Python Library for Data Validation | by Sulbha Jain', 'content': 'Mar 12, 2025·Type-Driven Validation — Pydantic harnesses Python type hints for schema validation and serialization, reducing the learning curve and code', 'score': 0.79732054, 'raw_content': None}, {'url': 'https://docs.pydantic.dev/1.10/usage/types/', 'title': 'Field Types - Pydantic', 'content': 'Data validation using Python type hints. ... Where possible pydantic uses standard library types to define fields, thus smoothing the learning curve.', 'score': 0.77751994, 'raw_content': None}, {'url': 'https://stackoverflow.com/questions/75730322/type-check-pydantic-basemodel-as-type-hint-with-mypy', 'title': 'Type check Pydantic BaseModel as type hint with mypy', 'content': \"# Type check Pydantic BaseModel as type hint with mypy Simultaneously I'd like to use these models as type hints (because they contain more information than simply saying `dict`). I'd like mypy to type check the pydantic BaseModel and not give an error. Optionally I could create a TypedDict and use that as type hint, but then I lose the pydantic strict functionality. So the short answer is that you don't get to use the `[]` operator on instances of a class derived from the Pydantic `BaseModel`. Since you use `mypy` and seem to be a beginner with Pydantic I'm guessing you don't have the `mypy` plugin for Pydantic set up. Find the answer to your question by asking.\", 'score': 0.6496014, 'raw_content': None}], 'response_time': 0.93, 'request_id': 'acfa9d95-9409-488b-bc8a-e71944fc2d2e'}, tool_call_id='pyd_ai_34a88ed574f541c5b55c0a1940af8997', timestamp=datetime.datetime(2025, 9, 3, 22, 30, 20, 276090, tzinfo=datetime.timezone.utc)), ToolReturnPart(tool_name='tavily_search', content={'query': 'pydantic performance considerations complex validation high-throughput systems', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://arxiv.org/html/2508.14123v1', 'title': 'AI Agents for Photonic Integrated Circuit Design Automation', 'content': 'A Pydantic model is a Python data class that defines a strict schema for the expected output (field names and types) and automatically validates that the', 'score': 0.34442192, 'raw_content': None}, {'url': 'https://medium.com/@patriwala/ai-augmented-development-the-complete-guide-bd39fc2316bd', 'title': 'AI-Augmented Development: The Complete Guide', 'content': 'Pydantic models for request/response validation. SQLAlchemy ORM integration. Async database operations. Comprehensive error handling. OpenAPI documentation. 4', 'score': 0.2831199, 'raw_content': None}, {'url': 'https://www.tokenmetrics.com/blog/fastapi-high-performance-apis', 'title': 'Fast API Guide: FastAPI Performance & Best Practices', 'content': \"Learn how FastAPI leverages async Python, pydantic models, and ASGI servers to build fast, scalable APIs. Practical optimization, deployment, and security tips for production-ready services. This post breaks down pragmatic patterns for building, testing, and scaling FastAPI services, with concrete guidance on performance tuning, deployment choices, and observability so you can design robust APIs for real-world workloads. Whether you're building a dashboard, feeding data into an AI model, or fetching market prices for analytics, understanding what an API is and how it works is essential to designing reliable systems. * **WebSocket / Streaming APIs:** Provide persistent connections for real-time data flows, useful for live feeds like price updates or chat. Reliable APIs for feature data, model serving, and orchestration are key to building robust AI workflows.\", 'score': 0.15081534, 'raw_content': None}, {'url': 'https://www.tokenmetrics.com/blog/best-api-institutional-crypto-analytics?74e29fd5_page=2', 'title': 'Which API is Best for Institutional-Level Crypto Analytics?', 'content': 'Enforce strong input validation with pydantic, use HTTPS, validate and sanitize user data, implement authentication and authorization (OAuth2, JWT), and', 'score': 0.14288872, 'raw_content': None}, {'url': 'https://www.reddit.com/r/Python/comments/1mqf3e6/we_rewrote_our_ingest_pipeline_from_python_to_go/', 'title': 'We rewrote our ingest pipeline from Python to Go', 'content': 'It worked well for validation… until performance became the bottleneck. We were hitting 800% CPU spikes, crashes, and unpredictable behavior under load', 'score': 0.1321896, 'raw_content': None}], 'response_time': 1.21, 'request_id': '9da3f65e-37b7-4e25-8ea2-2a9f7a92e07f'}, tool_call_id='pyd_ai_0aeedbab5054484da569080817eb557a', timestamp=datetime.datetime(2025, 9, 3, 22, 30, 20, 559942, tzinfo=datetime.timezone.utc)), ToolReturnPart(tool_name='tavily_search', content={'query': 'pydantic V1 vs V2 evolving features AI', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://ai.pydantic.dev/changelog/', 'title': 'Upgrade Guide', 'content': '* @T-256 made their first contribution in #627 by @samuelcolvin in #592 * fix docs icons by @samuelcolvin in #549 * fix settings docs formatting by @samuelcolvin in #524 * fix formatting of `tests/test_tools.py` by @samuelcolvin in #229 * add model documentation by @samuelcolvin in #250 * fix agent name by @samuelcolvin in #156 * fix formatting of api docs by @samuelcolvin in #117 * add readme by @samuelcolvin in #108 * fix broken links and add type checking function by @samuelcolvin in #97 by @samuelcolvin in #87 * add docs stubs by @samuelcolvin in #46 * fixing agent docs and refactoring retriever return types by @samuelcolvin in #60 * Add back api docs by @samuelcolvin in #62', 'score': 0.98588, 'raw_content': None}, {'url': 'https://medium.com/@albertoarrigoni/production-grade-ai-assisted-coding-python-first-patterns-for-data-ml-teams-6880bfc111a6', 'title': 'Production-Grade AI-Assisted Coding: Python-First ...', 'content': 'This post collects 10 situational patterns where AI either supports or replaces traditional engineering tasks — with a focus on Python code and libraries.', 'score': 0.98107, 'raw_content': None}, {'url': 'https://news.ycombinator.com/item?id=45055439', 'title': 'Building your own CLI coding agent with Pydantic-AI', 'content': 'reply | | | |  |  |  | | --- | --- | --- | |  |  | siva7 3 days ago  | root | parent | next )   Thanks for working on pydantic-ai. [0]:   reply | | | |  |  |  | | --- | --- | --- | |  |  | DouweM 3 days ago  | parent | next )   > All I know is that with the same LLM models, `openai.client.chat.completions` + a custom prompt to pass in the pydantic JSON schema + post-processing to instantiate SomePydanticModel(\\\\*json) creates objects successfully whereas vanilla pydantic-ai rarely does, regardless of the number of retries.', 'score': 0.97915, 'raw_content': None}, {'url': 'https://martinfowler.com/articles/build-own-coding-agent.html', 'title': 'Building your own CLI Coding Agent with Pydantic-AI', 'content': 'Building your own CLI Coding Agent with Pydantic-AI. Learning by doing. CLI coding agents are a fundamentally different tool to chatbots or autocomplete', 'score': 0.97366, 'raw_content': None}, {'url': 'https://medium.com/@sharma31193/parallel-model-training-data-pipelines-on-databricks-foreach-tasks-asset-bundles-pydantic-991cfba11dc5', 'title': 'Parallel Model Training & Data Pipelines on Databricks ...', 'content': 'To make life easier, we built a config-driven, parallel setup using: Pydantic to keep our configs clean and validated. Databricks ForEach tasks to run things in', 'score': 0.96928, 'raw_content': None}], 'response_time': 1.64, 'request_id': 'febd3a3c-942e-4c3a-bac8-acb794ac51a8'}, tool_call_id='pyd_ai_6a171cf3b61d4aaf8c5ba3042105527d', timestamp=datetime.datetime(2025, 9, 3, 22, 30, 21, 57583, tzinfo=datetime.timezone.utc)), ToolReturnPart(tool_name='tavily_search', content={'query': 'pydantic future outlook structured data type safety AI landscape', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://medium.com/@01coder/pydantic-ai-the-type-safe-ai-agent-framework-for-building-robust-llm-applications-ec0ce3366862', 'title': 'Pydantic AI: The Type-Safe AI Agent Framework for Building Robust ...', 'content': '# Pydantic AI: The Type-Safe AI Agent Framework for Building Robust LLM Applications Following the recent release of Pydantic AI `v0.0.21`, which brings a host of new features and improvements, this powerful framework has gained significant traction in the AI development community. **Pydantic**, widely recognized as the most widely used data validation and parsing library for Python , now offers developers a robust tool to build type-safe applications powered by large language models (LLMs). `Pydantic AI` emerges as an innovative Python agent framework, simplifying the development of production-grade applications that leverage generative AI. [## [MCP Servers worth installation] Firecrawl\\u200a—\\u200aA Powerful LLM-friendly Open-source Web Crawler Today, I’d like to share a…](/@01coder/mcp-servers-worth-installation-firecrawl-a-powerful-llm-friendly-open-source-web-crawler-afc49b357098? ## Gemini CLI: The Ultimate Command-Line AI Agent with Native MCP Support', 'score': 0.98524, 'raw_content': None}, {'url': 'https://www.linkedin.com/pulse/leveraging-pydantic-robust-structured-outputs-from-large-amit-kohad-yrmkc', 'title': 'Leveraging Pydantic for Robust, Structured Outputs from Large ...', 'content': 'Pydantic is a powerful data validation library that uses Python type hints to define data models. 2. Define the Pydantic Response Model: Create a Pydantic class that precisely defines the structure, fields, types, and descriptions of the data you wish to extract. The scenario: We have unstructured text (like an email or message) describing an event, and we want to use an LLM to reliably extract the event details into a structured, validated Python object. 1. Data Validation & Type Coercion: The LLM might generate {\"event\\\\_date\": \"2025-08-15\"}. In the second example, the LLM correctly extracts the date \"May 1, 2023,\" but our @field\\\\_validator(\\'event\\\\_date\\') catches that this date is in the past and raises a ValueError, which is then wrapped in a ValidationError.', 'score': 0.98233, 'raw_content': None}, {'url': 'https://www.startuphub.ai/ai-news/ai-video/2025/pydantics-vision-for-robust-ai-type-safety-agents-and-observability/', 'title': \"Pydantic's Vision for Robust AI: Type Safety, Agents, and Observability\", 'content': \"Jul 25, 2025·Samuel Colvin discusses Pydantic's approach to reliable AI development, focusing on type safety, agentic loops, and observability with\", 'score': 0.97814, 'raw_content': None}, {'url': 'https://ai.pydantic.dev/llms-full.txt', 'title': 'llms-full.txt - Pydantic AI', 'content': '... Type safe by design Pydantic AI is designed to work well with static type checkers, like mypy and pyright. Typing is (somewhat) optional Pydantic AI is', 'score': 0.97378, 'raw_content': None}, {'url': 'https://codecut.ai/enforce-structured-outputs-from-llms-with-pydanticai/', 'title': 'Enforce Structured Outputs from LLMs with PydanticAI - CodeCut', 'content': 'If you’re expecting clean, structured data, such as a JSON object with keys like ‘first\\\\_name’, ‘last\\\\_name’, ‘experience’, and ‘primary\\\\_skill’, you may find the model returning values in an unstructured form. input=\"Khuyen Tran is a data scientist with 5 years of experience, skilled in Python and machine learning.\", * Enforce structured JSON outputs from LLMs using Pydantic models instead of parsing unreliable text First, define a `Pydantic` Model that describes the expected structure of your agent’s output: Now, use the `output_type` parameter to connect this model to your agent: result = agent.run_sync(\\'Khuyen Tran is a data scientist with 5 years of experience, skilled in Python and machine learning.\\') first_name=\\'Khuyen\\' last_name=\\'Tran\\' experience_years=5 primary_skill=[\\'Python\\', \\'machine learning\\'] df = pd.DataFrame([result.output.model_dump()])', 'score': 0.96767, 'raw_content': None}], 'response_time': 1.74, 'request_id': 'b2ac4007-35f7-4cd5-9753-64ed48e88781'}, tool_call_id='pyd_ai_a7f29bb4075b48e3803af9fec9133584', timestamp=datetime.datetime(2025, 9, 3, 22, 30, 21, 276884, tzinfo=datetime.timezone.utc)), ToolReturnPart(tool_name='tavily_search', content={'query': 'pydantic in AI applications benefits challenges', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://hrshdg8.medium.com/agno-vs-pydantic-ai-the-ultimate-showdown-for-building-ai-agents-79b2c975cbec', 'title': 'Agno vs. Pydantic AI: The Ultimate Showdown for Building AI Agents', 'content': '[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fhrshdg8.medium.com%2Fagno-vs-pydantic-ai-the-ultimate-showdown-for-building-ai-agents-79b2c975cbec&source=post_page---top_nav_layout_nav-----------------------global_nav------------------) [Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fhrshdg8.medium.com%2Fagno-vs-pydantic-ai-the-ultimate-showdown-for-building-ai-agents-79b2c975cbec&source=post_page---top_nav_layout_nav-----------------------global_nav------------------) [Building Three Pipelines to Select the Right LLMs for RAG, Multi-Agent Systems, and Vision ------------------------------------------------------------------------------------------ ### Agentic RAG, Multi-Agent and Vision with Reasoning](https://medium.com/@fareedkhandev/building-three-pipelines-to-select-the-right-llms-for-rag-multi-agent-systems-and-vision-3e47e0563b76?source=post_page---read_next_recirc--79b2c975cbec----0---------------------dd5c336d_cc4c_44ac_bfe9_067b2b511db0--------------) [703 10](https://medium.com/@fareedkhandev/building-three-pipelines-to-select-the-right-llms-for-rag-multi-agent-systems-and-vision-3e47e0563b76?source=post_page---read_next_recirc--79b2c975cbec----0---------------------dd5c336d_cc4c_44ac_bfe9_067b2b511db0--------------) [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3e47e0563b76&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fbuilding-three-pipelines-to-select-the-right-llms-for-rag-multi-agent-systems-and-vision-3e47e0563b76&source=---read_next_recirc--79b2c975cbec----0-----------------bookmark_preview----dd5c336d_cc4c_44ac_bfe9_067b2b511db0--------------) [![Image 17: Artificial Intelligence in Plain English](https://miro.medium.com/v2/resize:fill:20:20/1*9zAmnK08gUCmZX7q0McVKw@2x.png)](https://ai.plainenglish.io/?source=post_page---read_next_recirc--79b2c975cbec----1---------------------dd5c336d_cc4c_44ac_bfe9_067b2b511db0--------------) [Building Agentic RAG with LangGraph: Mastering Adaptive RAG for Production -------------------------------------------------------------------------- ### Build intelligent RAG systems that know when to retrieve documents, search the web, or generate responses directly](https://medium.com/@piyushagni5/building-agentic-rag-with-langgraph-mastering-adaptive-rag-for-production-c2c4578c836a?source=post_page---read_next_recirc--79b2c975cbec----1---------------------dd5c336d_cc4c_44ac_bfe9_067b2b511db0--------------) [1.7K 26](https://medium.com/@piyushagni5/building-agentic-rag-with-langgraph-mastering-adaptive-rag-for-production-c2c4578c836a?source=post_page---read_next_recirc--79b2c975cbec----1---------------------dd5c336d_cc4c_44ac_bfe9_067b2b511db0--------------) [![Image 19: AI Advances](https://miro.medium.com/v2/resize:fill:20:20/1*R8zEd59FDf0l8Re94ImV0Q.png)](https://ai.gopubby.com/?source=post_page---read_next_recirc--79b2c975cbec----0---------------------dd5c336d_cc4c_44ac_bfe9_067b2b511db0--------------) [Human-in-the-Loop Strategy for Agentic AI ----------------------------------------- ### Collaborative Human-assisted Execution Model for AI Agents](https://debmalyabiswas.medium.com/human-in-the-loop-strategy-for-agentic-ai-d9daa22c3204?source=post_page---read_next_recirc--79b2c975cbec----0---------------------dd5c336d_cc4c_44ac_bfe9_067b2b511db0--------------) [537 14](https://debmalyabiswas.medium.com/human-in-the-loop-strategy-for-agentic-ai-d9daa22c3204?source=post_page---read_next_recirc--79b2c975cbec----0---------------------dd5c336d_cc4c_44ac_bfe9_067b2b511db0--------------) [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd9daa22c3204&operation=register&redirect=https%3A%2F%2Fai.gopubby.com%2Fhuman-in-the-loop-strategy-for-agentic-ai-d9daa22c3204&source=---read_next_recirc--79b2c975cbec----0-----------------bookmark_preview----dd5c336d_cc4c_44ac_bfe9_067b2b511db0--------------) [Building AI Agent Teams: The Complete Guide to Multi-Agent LLM Architectures ---------------------------------------------------------------------------- ### How to orchestrate multiple AI agents to solve complex problems that single agents can’t handle](https://medium.com/@Micheal-Lanham/building-ai-agent-teams-the-complete-guide-to-multi-agent-llm-architectures-936dde66daf3?source=post_page---read_next_recirc--79b2c975cbec----1---------------------dd5c336d_cc4c_44ac_bfe9_067b2b511db0--------------) [8](https://medium.com/@Micheal-Lanham/building-ai-agent-teams-the-complete-guide-to-multi-agent-llm-architectures-936dde66daf3?source=post_page---read_next_recirc--79b2c975cbec----1---------------------dd5c336d_cc4c_44ac_bfe9_067b2b511db0--------------) [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F936dde66daf3&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40Micheal-Lanham%2Fbuilding-ai-agent-teams-the-complete-guide-to-multi-agent-llm-architectures-936dde66daf3&source=---read_next_recirc--79b2c975cbec----1-----------------bookmark_preview----dd5c336d_cc4c_44ac_bfe9_067b2b511db0--------------) [32](https://medium.com/@a-gilmore/quickly-build-a-react-agent-with-langgraph-and-mcp-828757e3bd69?source=post_page---read_next_recirc--79b2c975cbec----2---------------------dd5c336d_cc4c_44ac_bfe9_067b2b511db0--------------)', 'score': 0.9856, 'raw_content': None}, {'url': 'https://medium.com/@tam.tamanna18/why-ai-frameworks-langchain-crewai-pydanticai-and-others-fail-in-production-c6292b98ee59', 'title': 'Why AI Frameworks (LangChain, CrewAI, PydanticAI and Others ...', 'content': 'In the fast-paced world of AI development, frameworks like LangChain, CrewAI, AutoGen, and even newer ones like PydanticAI promise to make building AI agents easy and quick. This article dives into why these popular frameworks are letting production teams down and introduces Atomic Agents as a fresh, developer-friendly alternative. These abstractions are like pre-built shortcuts that hide the complex code underneath, letting developers quickly create chatbots, question-answering systems, or other AI apps. ### Let me tell you\\u200a—\\u200amost AI agents are like that one coworker who talks a big game but delivers nothing. ## Building a Temporal AI Agent to Optimize Evolving Knowledge Bases in Modern RAG Systems ## Agentic AI vs. ## Building an Intelligent Deep Research Agent: From Query to Insights with AI-Powered Analysis using…', 'score': 0.98231, 'raw_content': None}, {'url': 'https://skylarbpayne.com/posts/ai-reliability-patterns/', 'title': 'Patterns for AI Reliability - Skylar Payne', 'content': 'This prevents malformed data from causing errors in downstream processes. For Python, Pydantic is the standard. It uses Python type hints to define a data', 'score': 0.97947, 'raw_content': None}, {'url': 'https://ai.pydantic.dev/temporal/', 'title': 'Durable Execution with Temporal - Pydantic AI', 'content': '[Skip to content](https://ai.pydantic.dev/temporal/#durable-execution-with-temporal) *   [Introduction](https://ai.pydantic.dev/) *   [Agents](https://ai.pydantic.dev/agents/) *   - [x] [Models](https://ai.pydantic.dev/models/)  Models   *   [Groq](https://ai.pydantic.dev/models/groq/) *   - [x]  Durable Execution with Temporal  [Durable Execution with Temporal](https://ai.pydantic.dev/temporal/) Table of contents   *   [Durable Execution](https://ai.pydantic.dev/temporal/#durable-execution) *   [Durable Agent](https://ai.pydantic.dev/temporal/#durable-agent) *   [Streaming](https://ai.pydantic.dev/temporal/#streaming) *   [Activity Configuration](https://ai.pydantic.dev/temporal/#activity-configuration) *   [Activity Retries](https://ai.pydantic.dev/temporal/#activity-retries) *   [Pandas](https://ai.pydantic.dev/temporal/#pandas) *   [pydantic_ai.agent](https://ai.pydantic.dev/api/agent/) *   [pydantic_ai.mcp](https://ai.pydantic.dev/api/mcp/) *   [pydantic_ai.models](https://ai.pydantic.dev/api/models/base/) *   [pydantic_ai.models.function](https://ai.pydantic.dev/api/models/function/) *   [fasta2a](https://ai.pydantic.dev/api/fasta2a/) *   [Durable Execution](https://ai.pydantic.dev/temporal/#durable-execution) *   [Durable Agent](https://ai.pydantic.dev/temporal/#durable-agent) *   [Streaming](https://ai.pydantic.dev/temporal/#streaming) *   [Activity Configuration](https://ai.pydantic.dev/temporal/#activity-configuration) *   [Activity Retries](https://ai.pydantic.dev/temporal/#activity-retries) *   [Pandas](https://ai.pydantic.dev/temporal/#pandas) In the case of Pydantic AI agents, integration with Temporal means that [model requests](https://ai.pydantic.dev/models/), [tool calls](https://ai.pydantic.dev/tools/) that may require I/O, and [MCP server communication](https://ai.pydantic.dev/mcp/client/) all need to be offloaded to Temporal activities due to their I/O requirements, while the logic that coordinates them (i.e. the agent run) lives in the workflow.', 'score': 0.97415, 'raw_content': None}, {'url': 'https://latenode.com/blog/11-open-source-ai-agent-frameworks-that-will-transform-your-development-2025-complete-guide', 'title': '11 Open Source AI Agent Frameworks That Will Transform Your ...', 'content': 'The standout feature of Pydantic AI Agents is its ability to enforce structured, validated responses from language models. Instead of relying on chance for', 'score': 0.97034, 'raw_content': None}], 'response_time': 1.34, 'request_id': 'd0d27849-e937-429e-87d9-5f75ccc4b71d'}, tool_call_id='pyd_ai_0932946b49534a8c8bffc2b6d1703b79', timestamp=datetime.datetime(2025, 9, 3, 22, 30, 20, 825145, tzinfo=datetime.timezone.utc))], instructions=\"You are a helpful AI assistant tasked with getting content for a comprehensive report based on a given outline. Your goal is to research and compile current information for each section and subsection of the outline using the DuckDuckGo MCP server. Follow these steps to complete the task:\\n\\n                1. Review the provided outline\\n\\n                2. For each section and subsection in the outline:\\n                a. Use the DuckDuckGo MCP server to search for current information related to the topic.\\n                b. Review the search results and identify the most relevant and reliable sources.\\n                c. After fetching the content, analyze and summarize the information relevant to the outline section.\\n\\n                3. Write the report section by section, following the structure of the outline. For each section:\\n                a. Provide a brief introduction to the topic.\\n                b. Include relevant information gathered from your research.\\n                c. Ensure that the content is well-organized and flows logically.\\n                d. Use appropriate transitions between subsections and main sections.\\n\\n                4. Throughout the writing process, keep track of the sources you've used. For each source, note:\\n                - The title of the webpage or article\\n                - The author (if available)\\n                - The website name\\n                - The URL\\n                - The date you accessed the information\"))\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:web_deep_researcher:Node: CallToolsNode(model_response=ModelResponse(parts=[ToolCallPart(tool_name='final_result', args={'sections': [{'references': 'https://www.geeky-gadgets.com/pydantic-ai-app-development/, https://towardsdatascience.com/applied-python-chronicles-a-gentle-intro-to-pydantic-a630f797e933/, https://www.civo.com/learn/simplifying-ai-development-with-pydanticai, https://www.datacamp.com/tutorial/pydantic, https://ai.pydantic.dev/', 'title': 'Summary', 'content': 'Pydantic has emerged as a crucial library in modern AI development, primarily by providing robust data validation and structuring capabilities. Its integration offers key benefits such as improved type safety, reduced debugging time, and enhanced reliability of AI systems. Pydantic facilitates the creation of predictable data flows, which is essential for complex AI/ML workflows. Practical implications span various use cases, including the validation of inputs and outputs for AI models, enforcing structured responses from Large Language Models (LLMs), defining tools for AI agents, and establishing clear data schemas for MLOps pipelines.'}, {'title': 'Introduction', 'content': 'This report explores the indispensable role of Pydantic in shaping the landscape of modern AI applications, addressing critical data management and workflow challenges.', 'references': ''}, {'title': '2.1 What is Pydantic?', 'references': 'https://www.datagluons.io/blog/pydantic-data-contract-manager, https://medium.com/@terabyte26/pydantic-v2-patterns-for-automation-validate-anything-everywhere-6d553504a25a, https://superjson.ai/blog/2025-08-14-pydantic-vs-zod-json-schema-generation-comparison, https://www.reddit.com/r/Python/comments/1n0ng7f/whats_your_favorite_python_trick_or_lesser_known/, https://docs.langchain.com/oss/python/langgraph/use-graph-api, https://betterstack.com/community/guides/scaling-python/pydantic-explained/, https://docs.pydantic.dev/latest/concepts/serialization/, https://stackoverflow.com/questions/65622045/pydantic-convert-to-jsonable-dict-not-full-json-string, https://github.com/pydantic/pydantic-core, https://pypi.org/project/pydantic-client/, https://github.com/orgs/pydantic/repositories, https://latenode.com/blog/11-open-source-ai-agent-frameworks-that-will-transform-your-development-2025-complete-guide, https://lawrencemcdaniel.com/data-science', 'content': \"Pydantic is a powerful Python library designed for data validation and settings management. It leverages Python's type hints to define schemas and validate data at runtime. Core features include automatic data parsing, robust serialization and deserialization capabilities, and runtime validation, which collectively ensure data integrity and consistency throughout an application's lifecycle.\"}, {'title': '2.2 The Challenge of Data in AI Systems', 'references': 'https://www.cloudtech.com/resources/data-integrity-importance-best-practices, https://spectrum.ieee.org/data-integrity, https://www.datajoint.com/news/ai-and-the-evolution-of-relational-schemas', 'content': 'In AI and Machine Learning (ML) workflows, data integrity and structure are paramount. The reliability of AI systems heavily depends on the quality and consistency of the data they process. Common challenges include handling unstructured inputs and outputs, managing schema mismatches between different system components, and the inherent complexity of debugging issues that arise from inconsistent data. These issues can lead to unreliable models, incorrect predictions, and significant development roadblocks.'}, {'references': 'https://latenode.com/blog/11-open-source-ai-agent-frameworks-that-will-transform-your-development-2025-complete-guide, https://medium.com/@judeaugustinej/pydantic-for-llm-workflows-631a7c01fdba, https://brightdata.com/blog/ai/pydantic-ai-with-web-mcp', 'content': 'Pydantic directly addresses the aforementioned data challenges in AI by enabling developers to define clear data models and enforce validation rules. By ensuring that data conforms to predefined schemas, Pydantic mitigates issues like unstructured inputs/outputs and schema mismatches, thereby simplifying debugging and increasing the overall robustness of AI systems. This report aims to delve into how Pydantic achieves this, its fundamental features, its practical applications, and the benefits and challenges associated with its integration into AI workflows.', 'title': '2.3 Why Pydantic for AI?'}, {'title': '3. Pydantic Fundamentals Relevant to AI', 'references': '', 'content': \"Pydantic's core strength lies in its ability to enforce data integrity and structure through a set of fundamental features that are particularly valuable in AI applications.\"}, {'references': 'https://www.ireneburresi.dev/p/the-modern-python-toolbox-an-engineered, https://moldstud.com/articles/p-designing-python-rest-apis-with-openapi-specification-a-comprehensive-guide, https://www.reddit.com/r/Python/comments/1n4hc9e/python_type_system/', 'title': '3.1 Data Models and Type Hinting', 'content': 'Pydantic allows developers to define complex data structures using standard Python type hints. By inheriting from `pydantic.BaseModel`, users can create classes where each attribute has a specified type. This approach significantly enhances code clarity, making it easier to understand the expected data format. It also improves maintainability by allowing IDEs to provide better autocompletion and static analysis, catching potential type-related errors early in the development cycle.'}, {'title': '3.2 Validation and Error Handling', 'references': 'https://dev.to/aws-builders/how-to-handle-form-data-in-aws-lambda-apis-with-powertools-openapi-support-4l40, https://medium.com/@shouke.wei/the-power-of-pydantic-bringing-structure-and-validation-to-your-python-data-9de124362a6f, https://medium.com/@terabyte26/pydantic-v2-patterns-for-automation-validate-anything-everywhere-6d553504a25a', 'content': \"One of Pydantic's most powerful features is its automatic data validation. When a Pydantic model is instantiated or data is assigned to its fields, Pydantic automatically validates the input against the defined types and constraints. If validation fails, it raises clear and informative `ValidationError` exceptions. Developers can also implement custom validators using decorators like `@field_validator` or `@model_validator` (in V2) to enforce more complex business logic or data rules, providing granular control over data quality and robust error reporting mechanisms.\"}, {'references': 'https://betterstack.com/community/guides/scaling-python/pydantic-explained/, https://docs.pydantic.dev/latest/concepts/serialization/, https://stackoverflow.com/questions/65622045/pydantic-convert-to-jsonable-dict-not-full-json-string', 'title': '3.3 Serialization and Deserialization', 'content': 'Pydantic models seamlessly handle the conversion of data to and from various formats, such as JSON strings and Python dictionaries. The `model_dump()` and `model_dump_json()` methods (in V2) allow for easy serialization, making it straightforward to prepare data for API responses, logging, or storage. Conversely, models can be instantiated directly from dictionaries or JSON strings (via `model_validate()` or `model_validate_json()`), facilitating deserialization of incoming data from external sources. This capability is vital for interacting with web APIs and ensuring data persistence in a structured manner.'}, {'title': '3.4 Schema Generation (JSON Schema)', 'content': 'Pydantic can automatically generate JSON Schemas from its data models. This feature is invaluable for documenting API contracts and ensuring interoperability with other systems that consume or produce JSON data. The generated JSON Schema provides a machine-readable description of the expected data structure, which can be used for client-side validation, code generation in other languages, or for integration with tools like OpenAPI/Swagger to provide interactive API documentation.', 'references': 'https://www.scaleway.com/en/docs/generative-apis/how-to/use-structured-outputs/, https://ai.pydantic.dev/api/tools/, https://python.useinstructor.com/architecture/'}, {'content': \"Pydantic's strong data validation and structuring capabilities make it an invaluable asset across various stages of AI data management and workflow orchestration.\", 'references': '', 'title': \"4. Pydantic's Role in AI Data Management and Workflows\"}, {'content': 'Ensuring the quality and correctness of data fed into machine learning models is critical for their performance and reliability. Pydantic enables rigorous input validation, allowing developers to define expected data types, ranges, and structures for diverse inputs such as text, numerical data, and categorical features. By validating inputs before they reach the model, Pydantic helps prevent common issues like malformed data, out-of-range values, and type mismatches, thus enhancing the robustness of AI models.', 'title': '4.1 Input Validation for AI Models', 'references': 'https://www.netguru.com/blog/data-validation-pydantic, https://codilime.com/blog/declarative-data-validation-pydantic/, https://dev.to/mechcloud_academy/practical-pydantic-data-transformation-aliases-and-validators-16c2'}, {'content': \"Just as with inputs, controlling the format and content of AI model outputs is crucial for downstream processing and application reliability. Pydantic allows developers to define explicit schemas for predictions, classifications, and generated content. This ensures that the AI model's output adheres to a predictable structure, making it easier for other parts of the application or external systems to consume and interpret the results. This is particularly beneficial in applications where AI outputs trigger further actions or are displayed to users.\", 'title': '4.2 Structuring and Validating AI Model Outputs', 'references': 'https://pydantic.dev/articles/llm-intro, https://dev.to/devasservice/a-practical-guide-on-structuring-llm-outputs-with-pydantic-50b4, https://medium.com/@sridevi.gogusetty/building-multi-agent-llm-systems-with-pydanticai-framework-a-step-by-step-guide-to-create-ai-595cfcdede92'}, {'title': '4.3 Pydantic in Large Language Model (LLM) Applications', 'references': '', 'content': 'Pydantic plays a particularly transformative role in the development of Large Language Model (LLM) applications by bringing structure and reliability to their often-unpredictable outputs.'}, {'content': \"One of the significant challenges with LLMs is obtaining reliable, structured data from their free-form text generation. Pydantic enables developers to define a target JSON schema that LLMs must adhere to when generating responses. By providing the LLM with a Pydantic model's JSON schema, tools and frameworks can guide the LLM to produce outputs that are directly parsable and validatable, ensuring that the generated data, such as a list of entities or a specific data object, is consistent and usable.\", 'title': '4.3.1 Enforcing Structured Output from LLMs', 'references': 'https://codecut.ai/enforce-structured-outputs-from-llms-with-pydanticai/, https://dev.to/devasservice/a-practical-guide-on-structuring-llm-outputs-with-pydantic-50b4, https://datachain.ai/blog/enforcing-json-outputs-in-commercial-llms'}, {'content': \"In the context of LLM agents and function calling (e.g., with OpenAI's API), Pydantic is used to clearly define the inputs and outputs of tools and functions that the LLM can invoke. By representing tool schemas as Pydantic models, developers can provide a precise, machine-readable description of what a tool expects and what it returns. This allows LLMs to understand how to interact with external systems more effectively, leading to more reliable and accurate tool execution within agentic workflows.\", 'references': 'https://www.reddit.com/r/LangChain/comments/1mynq4a/agents_are_just_llm_loop_tools_its_simpler_than_people_make_it/, https://ai.pydantic.dev/api/agent/, https://ai.pydantic.dev/api/tools/', 'title': '4.3.2 Defining Tools and Functions for LLM Agents'}, {'title': '4.3.3 Validating LLM Prompts and Responses', 'references': 'https://docs.databricks.com/aws/en/mlflow3/genai/tracing/integrations/pydantic-ai, https://pub.towardsai.net/how-to-ensure-reliability-in-llm-applications-13302bd7521a, https://medium.com/@albertoarrigoni/production-grade-ai-assisted-coding-python-first-patterns-for-data-ml-teams-6880bfc111a6', 'content': \"Pydantic can also be employed to validate the format and content of both LLM prompts and their subsequent responses. This ensures that prompts adhere to expected templates or contain necessary information, and that responses, even when unstructured, contain certain keywords or follow a general pattern. While more challenging for free-form text, Pydantic's custom validators can be adapted to check for adherence to expected formats, improving the overall quality and predictability of LLM interactions.\"}, {'references': 'https://www.linkedin.com/posts/shantanuladhwe_ive-set-up-10-ai-production-environments-activity-7368870388241510400-EzeU, https://omyeole.com/work/house_price_predictor, https://huggingface.co/papers?q=modular%20collaborative%20framework', 'content': 'In MLOps and complex data pipelines, defining clear data contracts is essential. Pydantic models can serve as these contracts, establishing explicit schemas for data flowing between different stages of an ML pipeline—from data ingestion and preprocessing to model training and deployment. This approach significantly improves data governance, ensures data consistency across teams, and enhances reproducibility by providing a verifiable standard for all data artifacts.', 'title': '4.4 Data Schemas for MLOps and Data Pipelines'}, {'references': '', 'content': \"Pydantic's versatility makes it applicable across a wide range of practical scenarios in AI development, from API design to agentic systems.\", 'title': '5. Practical Applications and Use Cases'}, {'title': '5.1 Building Robust AI APIs', 'content': \"Pydantic is a cornerstone for building robust and reliable AI APIs, particularly when integrated with modern Python web frameworks like FastAPI. FastAPI, in fact, uses Pydantic extensively for automatic request body parsing, validation, and serialization of responses. This ensures that incoming data to an AI service conforms to the expected schema and that the API's outputs are consistently structured. While Flask can also leverage Pydantic for validation, FastAPI's native integration streamlines the process, leading to more stable and maintainable AI-powered microservices.\", 'references': 'https://medium.com/@hadiyolworld007/from-prototype-to-production-fastapi-in-the-era-of-ai-native-apps-faf7b0b566f1, https://dev.to/atifwattoo/django-vs-flask-vs-fastapi, https://www.geeksforgeeks.org/python/rest-api-with-fastapi/, https://zenvanriel.nl/ai-engineer-blog/fastapi-vs-flask-for-ai-applications/, https://www.netguru.com/blog/python-flask-versus-fastapi'}, {'references': 'https://www.prefect.io/blog/what-is-pydantic-validating-data-in-python, https://medium.com/@durgarao3d6/clean-and-reliable-data-handling-in-python-using-pydantic-3aef7c42d2f8, https://www.reddit.com/r/Python/comments/16xnhim/what_problems_does_pydantic_solves_and_how_should/, https://data-ai.theodo.com/en/technical-blog/boost-your-data-projects-with-pydantic-validation-and-efficiency, https://medium.com/@CodeWithHannan/data-validation-and-schema-enforcement-in-python-using-pydantic-886ab64fecba', 'content': 'Before any machine learning model can be trained, the raw data must be thoroughly validated and cleaned. Pydantic models can be used to define the schema for training data, allowing for automatic validation of incoming data streams or datasets. This ensures that only high-quality, correctly formatted data is used for training, preventing subtle data errors from propagating through the model and impacting its performance. Custom validators can also be applied to enforce specific data cleaning rules during ingestion.', 'title': '5.2 Enhancing Data Ingestion for Training Data'}, {'references': 'https://latenode.com/blog/11-open-source-ai-agent-frameworks-that-will-transform-your-development-2025-complete-guide, https://medium.com/@tam.tamanna18/why-ai-frameworks-langchain-crewai-pydanticai-and-others-fail-in-production-c6292b98ee59, https://devot.team/blog/agentic-ai, https://huggingface.co/papers?q=agile%20methodologies, https://www.researchgate.net/publication/383032644_NLP_Chatbot_For_Order_Assistance_Using_Dialogflow', 'content': 'Pydantic is instrumental in developing reliable AI agents and chatbots by providing a structured way to manage conversational states, user inputs, and agent actions. By defining Pydantic models for various conversational elements, developers can ensure that user queries are correctly parsed, agent responses adhere to expected formats, and internal states are consistently maintained. This structured approach significantly improves the robustness and predictability of AI-driven conversational systems.', 'title': '5.3 Developing Reliable AI Agents and Chatbots'}, {'content': \"Pydantic's utility extends to seamless integration with popular AI frameworks and libraries. For instance, in LangChain and LlamaIndex, Pydantic models are widely used to define schemas for LLM outputs, tool definitions, and data structures within agents. The OpenAI Python client also leverages Pydantic for its function-calling capabilities, allowing developers to describe desired output formats using Pydantic models directly. This integration streamlines the development of complex AI applications by providing a consistent and type-safe way to handle data across different components.\", 'title': '5.4 Integrating Pydantic with Popular AI Frameworks/Libraries', 'references': 'https://docs.llamaindex.ai/en/stable/examples/output_parsing/openai_pydantic_program/, https://ai.pydantic.dev/, https://docs.llamaindex.ai/en/stable/examples/llm/openai/, https://logfire.pydantic.dev/docs/integrations/, https://github.com/pydantic/pydantic-ai'}, {'references': '', 'content': 'While Pydantic offers significant advantages for AI development, it also comes with certain considerations.', 'title': '6. Benefits and Challenges of Pydantic-AI Integration'}, {'title': '6.1 Key Benefits', 'content': 'Integrating Pydantic into AI applications yields numerous benefits: \\n*   **Increased reliability and robustness:** By enforcing strict data schemas and validation rules, Pydantic significantly reduces the likelihood of data-related errors and unexpected behavior in AI systems.\\n*   **Improved developer experience and productivity:** Type hints and automatic validation lead to cleaner, more predictable code, which in turn enhances developer productivity and reduces the mental overhead of data management.\\n*   **Better code readability, maintainability, and collaboration:** Clearly defined data models improve code readability and make it easier for teams to collaborate on projects, as data contracts are explicit.\\n*   **Reduced debugging time and fewer runtime errors:** Many common data issues are caught at validation time, often before runtime, leading to less time spent on debugging and a decrease in production errors.\\n*   **Enhanced API design and documentation:** Automatic JSON schema generation and clear data models contribute to well-defined and self-documenting APIs for AI services.', 'references': 'https://hrshdg8.medium.com/agno-vs-pydantic-ai-the-ultimate-showdown-for-building-ai-agents-79b2c975cbec, https://medium.com/@tam.tamanna18/why-ai-frameworks-langchain-crewai-pydanticai-and-others-fail-in-production-c6292b98ee59, https://skylarbpayne.com/posts/ai-reliability-patterns/, https://ai.pydantic.dev/temporal/, https://latenode.com/blog/11-open-source-ai-agent-frameworks-that-will-transform-your-development-2025-complete-guide'}, {'title': '6.2 Potential Challenges and Considerations', 'references': 'https://docs.pydantic.dev/2.0/usage/types/types/, https://docs.pydantic.dev/latest/concepts/types/, https://medium.com/@sulbha.jindal/pydantic-python-library-for-data-validation-cd8b9185fc4d, https://docs.pydantic.dev/1.10/usage/types/, https://stackoverflow.com/questions/75730322/type-check-pydantic-basemodel-as-type-hint-with-mypy, https://arxiv.org/html/2508.14123v1, https://medium.com/@patriwala/ai-augmented-development-the-complete-guide-bd39fc2316bd, https://www.tokenmetrics.com/blog/fastapi-high-performance-apis, https://www.tokenmetrics.com/blog/best-api-institutional-crypto-analytics?74e29fd5_page=2, https://www.reddit.com/r/Python/comments/1mqf3e6/we_rewrote_our_ingest_pipeline_from_python_to_go/, https://ai.pydantic.dev/changelog/, https://medium.com/@albertoarrigoni/production-grade-ai-assisted-coding-python-first-patterns-for-data-ml-teams-6880bfc111a6, https://news.ycombinator.com/item?id=45055439, https://martinfowler.com/articles/build-own-coding-agent.html, https://medium.com/@sharma31193/parallel-model-training-data-pipelines-on-databricks-foreach-tasks-asset-bundles-pydantic-991cfba11dc5', 'content': \"Despite its advantages, Pydantic integration presents a few challenges: \\n*   **Learning curve for Pydantic (especially for those new to type hints):** Developers new to Python type hints may experience a learning curve when adopting Pydantic, as its core functionality relies heavily on them.\\n*   **Performance considerations for very high-throughput systems (if complex validation):** For applications requiring extremely high throughput and complex, deeply nested validation, the overhead of Pydantic's runtime validation might introduce performance considerations that need careful optimization.\\n*   **Keeping up with Pydantic's evolving features (e.g., V1 vs. V2):** Pydantic is an actively developed library, and staying updated with its evolving features, particularly the significant changes between V1 and V2, can require effort. This is crucial for leveraging the latest optimizations and functionalities, especially in AI contexts where new patterns are constantly emerging.\"}, {'references': 'https://medium.com/@01coder/pydantic-ai-the-type-safe-ai-agent-framework-for-building-robust-llm-applications-ec0ce3366862, https://www.linkedin.com/pulse/leveraging-pydantic-robust-structured-outputs-from-large-amit-kohad-yrmkc, https://www.startuphub.ai/ai-news/ai-video/2025/pydantics-vision-for-robust-ai-type-safety-agents-and-observability/, https://ai.pydantic.dev/llms-full.txt, https://codecut.ai/enforce-structured-outputs-from-llms-with-pydanticai/', 'content': 'Pydantic has firmly established itself as a critical tool for building robust, reliable, and maintainable AI applications. Its emphasis on type safety and structured data addresses many of the inherent complexities in AI/ML workflows, from input validation to enforcing predictable outputs from advanced models like LLMs. As the AI landscape continues to evolve, the demand for structured data and strong type safety will only increase, making Pydantic an even more indispensable library. Developers leveraging AI are strongly recommended to adopt Pydantic to enhance data integrity, reduce development complexities, minimize runtime errors, and ultimately build more trustworthy and efficient AI systems.', 'title': '7. Conclusion'}]}, tool_call_id='pyd_ai_acbfbf4870a54608ba6fb6c29622a9ad')], usage=RequestUsage(input_tokens=28355, cache_read_tokens=16190, output_tokens=7721, details={'cached_content_tokens': 16190, 'thoughts_tokens': 1837, 'text_prompt_tokens': 28355, 'text_cache_tokens': 16190}), model_name='gemini-2.5-flash', timestamp=datetime.datetime(2025, 9, 3, 22, 31, 1, 644121, tzinfo=datetime.timezone.utc), provider_name='google-gla', provider_details={'finish_reason': 'STOP'}, provider_response_id='pcG4aPiAK8mmmtkPqtqUsA8'))\n",
      "INFO:web_deep_researcher:Node: End(data=FinalResult(output=documentationOutput(sections=[Section(title='Summary', content='Pydantic has emerged as a crucial library in modern AI development, primarily by providing robust data validation and structuring capabilities. Its integration offers key benefits such as improved type safety, reduced debugging time, and enhanced reliability of AI systems. Pydantic facilitates the creation of predictable data flows, which is essential for complex AI/ML workflows. Practical implications span various use cases, including the validation of inputs and outputs for AI models, enforcing structured responses from Large Language Models (LLMs), defining tools for AI agents, and establishing clear data schemas for MLOps pipelines.', references='https://www.geeky-gadgets.com/pydantic-ai-app-development/, https://towardsdatascience.com/applied-python-chronicles-a-gentle-intro-to-pydantic-a630f797e933/, https://www.civo.com/learn/simplifying-ai-development-with-pydanticai, https://www.datacamp.com/tutorial/pydantic, https://ai.pydantic.dev/'), Section(title='Introduction', content='This report explores the indispensable role of Pydantic in shaping the landscape of modern AI applications, addressing critical data management and workflow challenges.', references=''), Section(title='2.1 What is Pydantic?', content=\"Pydantic is a powerful Python library designed for data validation and settings management. It leverages Python's type hints to define schemas and validate data at runtime. Core features include automatic data parsing, robust serialization and deserialization capabilities, and runtime validation, which collectively ensure data integrity and consistency throughout an application's lifecycle.\", references='https://www.datagluons.io/blog/pydantic-data-contract-manager, https://medium.com/@terabyte26/pydantic-v2-patterns-for-automation-validate-anything-everywhere-6d553504a25a, https://superjson.ai/blog/2025-08-14-pydantic-vs-zod-json-schema-generation-comparison, https://www.reddit.com/r/Python/comments/1n0ng7f/whats_your_favorite_python_trick_or_lesser_known/, https://docs.langchain.com/oss/python/langgraph/use-graph-api, https://betterstack.com/community/guides/scaling-python/pydantic-explained/, https://docs.pydantic.dev/latest/concepts/serialization/, https://stackoverflow.com/questions/65622045/pydantic-convert-to-jsonable-dict-not-full-json-string, https://github.com/pydantic/pydantic-core, https://pypi.org/project/pydantic-client/, https://github.com/orgs/pydantic/repositories, https://latenode.com/blog/11-open-source-ai-agent-frameworks-that-will-transform-your-development-2025-complete-guide, https://lawrencemcdaniel.com/data-science'), Section(title='2.2 The Challenge of Data in AI Systems', content='In AI and Machine Learning (ML) workflows, data integrity and structure are paramount. The reliability of AI systems heavily depends on the quality and consistency of the data they process. Common challenges include handling unstructured inputs and outputs, managing schema mismatches between different system components, and the inherent complexity of debugging issues that arise from inconsistent data. These issues can lead to unreliable models, incorrect predictions, and significant development roadblocks.', references='https://www.cloudtech.com/resources/data-integrity-importance-best-practices, https://spectrum.ieee.org/data-integrity, https://www.datajoint.com/news/ai-and-the-evolution-of-relational-schemas'), Section(title='2.3 Why Pydantic for AI?', content='Pydantic directly addresses the aforementioned data challenges in AI by enabling developers to define clear data models and enforce validation rules. By ensuring that data conforms to predefined schemas, Pydantic mitigates issues like unstructured inputs/outputs and schema mismatches, thereby simplifying debugging and increasing the overall robustness of AI systems. This report aims to delve into how Pydantic achieves this, its fundamental features, its practical applications, and the benefits and challenges associated with its integration into AI workflows.', references='https://latenode.com/blog/11-open-source-ai-agent-frameworks-that-will-transform-your-development-2025-complete-guide, https://medium.com/@judeaugustinej/pydantic-for-llm-workflows-631a7c01fdba, https://brightdata.com/blog/ai/pydantic-ai-with-web-mcp'), Section(title='3. Pydantic Fundamentals Relevant to AI', content=\"Pydantic's core strength lies in its ability to enforce data integrity and structure through a set of fundamental features that are particularly valuable in AI applications.\", references=''), Section(title='3.1 Data Models and Type Hinting', content='Pydantic allows developers to define complex data structures using standard Python type hints. By inheriting from `pydantic.BaseModel`, users can create classes where each attribute has a specified type. This approach significantly enhances code clarity, making it easier to understand the expected data format. It also improves maintainability by allowing IDEs to provide better autocompletion and static analysis, catching potential type-related errors early in the development cycle.', references='https://www.ireneburresi.dev/p/the-modern-python-toolbox-an-engineered, https://moldstud.com/articles/p-designing-python-rest-apis-with-openapi-specification-a-comprehensive-guide, https://www.reddit.com/r/Python/comments/1n4hc9e/python_type_system/'), Section(title='3.2 Validation and Error Handling', content=\"One of Pydantic's most powerful features is its automatic data validation. When a Pydantic model is instantiated or data is assigned to its fields, Pydantic automatically validates the input against the defined types and constraints. If validation fails, it raises clear and informative `ValidationError` exceptions. Developers can also implement custom validators using decorators like `@field_validator` or `@model_validator` (in V2) to enforce more complex business logic or data rules, providing granular control over data quality and robust error reporting mechanisms.\", references='https://dev.to/aws-builders/how-to-handle-form-data-in-aws-lambda-apis-with-powertools-openapi-support-4l40, https://medium.com/@shouke.wei/the-power-of-pydantic-bringing-structure-and-validation-to-your-python-data-9de124362a6f, https://medium.com/@terabyte26/pydantic-v2-patterns-for-automation-validate-anything-everywhere-6d553504a25a'), Section(title='3.3 Serialization and Deserialization', content='Pydantic models seamlessly handle the conversion of data to and from various formats, such as JSON strings and Python dictionaries. The `model_dump()` and `model_dump_json()` methods (in V2) allow for easy serialization, making it straightforward to prepare data for API responses, logging, or storage. Conversely, models can be instantiated directly from dictionaries or JSON strings (via `model_validate()` or `model_validate_json()`), facilitating deserialization of incoming data from external sources. This capability is vital for interacting with web APIs and ensuring data persistence in a structured manner.', references='https://betterstack.com/community/guides/scaling-python/pydantic-explained/, https://docs.pydantic.dev/latest/concepts/serialization/, https://stackoverflow.com/questions/65622045/pydantic-convert-to-jsonable-dict-not-full-json-string'), Section(title='3.4 Schema Generation (JSON Schema)', content='Pydantic can automatically generate JSON Schemas from its data models. This feature is invaluable for documenting API contracts and ensuring interoperability with other systems that consume or produce JSON data. The generated JSON Schema provides a machine-readable description of the expected data structure, which can be used for client-side validation, code generation in other languages, or for integration with tools like OpenAPI/Swagger to provide interactive API documentation.', references='https://www.scaleway.com/en/docs/generative-apis/how-to/use-structured-outputs/, https://ai.pydantic.dev/api/tools/, https://python.useinstructor.com/architecture/'), Section(title=\"4. Pydantic's Role in AI Data Management and Workflows\", content=\"Pydantic's strong data validation and structuring capabilities make it an invaluable asset across various stages of AI data management and workflow orchestration.\", references=''), Section(title='4.1 Input Validation for AI Models', content='Ensuring the quality and correctness of data fed into machine learning models is critical for their performance and reliability. Pydantic enables rigorous input validation, allowing developers to define expected data types, ranges, and structures for diverse inputs such as text, numerical data, and categorical features. By validating inputs before they reach the model, Pydantic helps prevent common issues like malformed data, out-of-range values, and type mismatches, thus enhancing the robustness of AI models.', references='https://www.netguru.com/blog/data-validation-pydantic, https://codilime.com/blog/declarative-data-validation-pydantic/, https://dev.to/mechcloud_academy/practical-pydantic-data-transformation-aliases-and-validators-16c2'), Section(title='4.2 Structuring and Validating AI Model Outputs', content=\"Just as with inputs, controlling the format and content of AI model outputs is crucial for downstream processing and application reliability. Pydantic allows developers to define explicit schemas for predictions, classifications, and generated content. This ensures that the AI model's output adheres to a predictable structure, making it easier for other parts of the application or external systems to consume and interpret the results. This is particularly beneficial in applications where AI outputs trigger further actions or are displayed to users.\", references='https://pydantic.dev/articles/llm-intro, https://dev.to/devasservice/a-practical-guide-on-structuring-llm-outputs-with-pydantic-50b4, https://medium.com/@sridevi.gogusetty/building-multi-agent-llm-systems-with-pydanticai-framework-a-step-by-step-guide-to-create-ai-595cfcdede92'), Section(title='4.3 Pydantic in Large Language Model (LLM) Applications', content='Pydantic plays a particularly transformative role in the development of Large Language Model (LLM) applications by bringing structure and reliability to their often-unpredictable outputs.', references=''), Section(title='4.3.1 Enforcing Structured Output from LLMs', content=\"One of the significant challenges with LLMs is obtaining reliable, structured data from their free-form text generation. Pydantic enables developers to define a target JSON schema that LLMs must adhere to when generating responses. By providing the LLM with a Pydantic model's JSON schema, tools and frameworks can guide the LLM to produce outputs that are directly parsable and validatable, ensuring that the generated data, such as a list of entities or a specific data object, is consistent and usable.\", references='https://codecut.ai/enforce-structured-outputs-from-llms-with-pydanticai/, https://dev.to/devasservice/a-practical-guide-on-structuring-llm-outputs-with-pydantic-50b4, https://datachain.ai/blog/enforcing-json-outputs-in-commercial-llms'), Section(title='4.3.2 Defining Tools and Functions for LLM Agents', content=\"In the context of LLM agents and function calling (e.g., with OpenAI's API), Pydantic is used to clearly define the inputs and outputs of tools and functions that the LLM can invoke. By representing tool schemas as Pydantic models, developers can provide a precise, machine-readable description of what a tool expects and what it returns. This allows LLMs to understand how to interact with external systems more effectively, leading to more reliable and accurate tool execution within agentic workflows.\", references='https://www.reddit.com/r/LangChain/comments/1mynq4a/agents_are_just_llm_loop_tools_its_simpler_than_people_make_it/, https://ai.pydantic.dev/api/agent/, https://ai.pydantic.dev/api/tools/'), Section(title='4.3.3 Validating LLM Prompts and Responses', content=\"Pydantic can also be employed to validate the format and content of both LLM prompts and their subsequent responses. This ensures that prompts adhere to expected templates or contain necessary information, and that responses, even when unstructured, contain certain keywords or follow a general pattern. While more challenging for free-form text, Pydantic's custom validators can be adapted to check for adherence to expected formats, improving the overall quality and predictability of LLM interactions.\", references='https://docs.databricks.com/aws/en/mlflow3/genai/tracing/integrations/pydantic-ai, https://pub.towardsai.net/how-to-ensure-reliability-in-llm-applications-13302bd7521a, https://medium.com/@albertoarrigoni/production-grade-ai-assisted-coding-python-first-patterns-for-data-ml-teams-6880bfc111a6'), Section(title='4.4 Data Schemas for MLOps and Data Pipelines', content='In MLOps and complex data pipelines, defining clear data contracts is essential. Pydantic models can serve as these contracts, establishing explicit schemas for data flowing between different stages of an ML pipeline—from data ingestion and preprocessing to model training and deployment. This approach significantly improves data governance, ensures data consistency across teams, and enhances reproducibility by providing a verifiable standard for all data artifacts.', references='https://www.linkedin.com/posts/shantanuladhwe_ive-set-up-10-ai-production-environments-activity-7368870388241510400-EzeU, https://omyeole.com/work/house_price_predictor, https://huggingface.co/papers?q=modular%20collaborative%20framework'), Section(title='5. Practical Applications and Use Cases', content=\"Pydantic's versatility makes it applicable across a wide range of practical scenarios in AI development, from API design to agentic systems.\", references=''), Section(title='5.1 Building Robust AI APIs', content=\"Pydantic is a cornerstone for building robust and reliable AI APIs, particularly when integrated with modern Python web frameworks like FastAPI. FastAPI, in fact, uses Pydantic extensively for automatic request body parsing, validation, and serialization of responses. This ensures that incoming data to an AI service conforms to the expected schema and that the API's outputs are consistently structured. While Flask can also leverage Pydantic for validation, FastAPI's native integration streamlines the process, leading to more stable and maintainable AI-powered microservices.\", references='https://medium.com/@hadiyolworld007/from-prototype-to-production-fastapi-in-the-era-of-ai-native-apps-faf7b0b566f1, https://dev.to/atifwattoo/django-vs-flask-vs-fastapi, https://www.geeksforgeeks.org/python/rest-api-with-fastapi/, https://zenvanriel.nl/ai-engineer-blog/fastapi-vs-flask-for-ai-applications/, https://www.netguru.com/blog/python-flask-versus-fastapi'), Section(title='5.2 Enhancing Data Ingestion for Training Data', content='Before any machine learning model can be trained, the raw data must be thoroughly validated and cleaned. Pydantic models can be used to define the schema for training data, allowing for automatic validation of incoming data streams or datasets. This ensures that only high-quality, correctly formatted data is used for training, preventing subtle data errors from propagating through the model and impacting its performance. Custom validators can also be applied to enforce specific data cleaning rules during ingestion.', references='https://www.prefect.io/blog/what-is-pydantic-validating-data-in-python, https://medium.com/@durgarao3d6/clean-and-reliable-data-handling-in-python-using-pydantic-3aef7c42d2f8, https://www.reddit.com/r/Python/comments/16xnhim/what_problems_does_pydantic_solves_and_how_should/, https://data-ai.theodo.com/en/technical-blog/boost-your-data-projects-with-pydantic-validation-and-efficiency, https://medium.com/@CodeWithHannan/data-validation-and-schema-enforcement-in-python-using-pydantic-886ab64fecba'), Section(title='5.3 Developing Reliable AI Agents and Chatbots', content='Pydantic is instrumental in developing reliable AI agents and chatbots by providing a structured way to manage conversational states, user inputs, and agent actions. By defining Pydantic models for various conversational elements, developers can ensure that user queries are correctly parsed, agent responses adhere to expected formats, and internal states are consistently maintained. This structured approach significantly improves the robustness and predictability of AI-driven conversational systems.', references='https://latenode.com/blog/11-open-source-ai-agent-frameworks-that-will-transform-your-development-2025-complete-guide, https://medium.com/@tam.tamanna18/why-ai-frameworks-langchain-crewai-pydanticai-and-others-fail-in-production-c6292b98ee59, https://devot.team/blog/agentic-ai, https://huggingface.co/papers?q=agile%20methodologies, https://www.researchgate.net/publication/383032644_NLP_Chatbot_For_Order_Assistance_Using_Dialogflow'), Section(title='5.4 Integrating Pydantic with Popular AI Frameworks/Libraries', content=\"Pydantic's utility extends to seamless integration with popular AI frameworks and libraries. For instance, in LangChain and LlamaIndex, Pydantic models are widely used to define schemas for LLM outputs, tool definitions, and data structures within agents. The OpenAI Python client also leverages Pydantic for its function-calling capabilities, allowing developers to describe desired output formats using Pydantic models directly. This integration streamlines the development of complex AI applications by providing a consistent and type-safe way to handle data across different components.\", references='https://docs.llamaindex.ai/en/stable/examples/output_parsing/openai_pydantic_program/, https://ai.pydantic.dev/, https://docs.llamaindex.ai/en/stable/examples/llm/openai/, https://logfire.pydantic.dev/docs/integrations/, https://github.com/pydantic/pydantic-ai'), Section(title='6. Benefits and Challenges of Pydantic-AI Integration', content='While Pydantic offers significant advantages for AI development, it also comes with certain considerations.', references=''), Section(title='6.1 Key Benefits', content='Integrating Pydantic into AI applications yields numerous benefits: \\n*   **Increased reliability and robustness:** By enforcing strict data schemas and validation rules, Pydantic significantly reduces the likelihood of data-related errors and unexpected behavior in AI systems.\\n*   **Improved developer experience and productivity:** Type hints and automatic validation lead to cleaner, more predictable code, which in turn enhances developer productivity and reduces the mental overhead of data management.\\n*   **Better code readability, maintainability, and collaboration:** Clearly defined data models improve code readability and make it easier for teams to collaborate on projects, as data contracts are explicit.\\n*   **Reduced debugging time and fewer runtime errors:** Many common data issues are caught at validation time, often before runtime, leading to less time spent on debugging and a decrease in production errors.\\n*   **Enhanced API design and documentation:** Automatic JSON schema generation and clear data models contribute to well-defined and self-documenting APIs for AI services.', references='https://hrshdg8.medium.com/agno-vs-pydantic-ai-the-ultimate-showdown-for-building-ai-agents-79b2c975cbec, https://medium.com/@tam.tamanna18/why-ai-frameworks-langchain-crewai-pydanticai-and-others-fail-in-production-c6292b98ee59, https://skylarbpayne.com/posts/ai-reliability-patterns/, https://ai.pydantic.dev/temporal/, https://latenode.com/blog/11-open-source-ai-agent-frameworks-that-will-transform-your-development-2025-complete-guide'), Section(title='6.2 Potential Challenges and Considerations', content=\"Despite its advantages, Pydantic integration presents a few challenges: \\n*   **Learning curve for Pydantic (especially for those new to type hints):** Developers new to Python type hints may experience a learning curve when adopting Pydantic, as its core functionality relies heavily on them.\\n*   **Performance considerations for very high-throughput systems (if complex validation):** For applications requiring extremely high throughput and complex, deeply nested validation, the overhead of Pydantic's runtime validation might introduce performance considerations that need careful optimization.\\n*   **Keeping up with Pydantic's evolving features (e.g., V1 vs. V2):** Pydantic is an actively developed library, and staying updated with its evolving features, particularly the significant changes between V1 and V2, can require effort. This is crucial for leveraging the latest optimizations and functionalities, especially in AI contexts where new patterns are constantly emerging.\", references='https://docs.pydantic.dev/2.0/usage/types/types/, https://docs.pydantic.dev/latest/concepts/types/, https://medium.com/@sulbha.jindal/pydantic-python-library-for-data-validation-cd8b9185fc4d, https://docs.pydantic.dev/1.10/usage/types/, https://stackoverflow.com/questions/75730322/type-check-pydantic-basemodel-as-type-hint-with-mypy, https://arxiv.org/html/2508.14123v1, https://medium.com/@patriwala/ai-augmented-development-the-complete-guide-bd39fc2316bd, https://www.tokenmetrics.com/blog/fastapi-high-performance-apis, https://www.tokenmetrics.com/blog/best-api-institutional-crypto-analytics?74e29fd5_page=2, https://www.reddit.com/r/Python/comments/1mqf3e6/we_rewrote_our_ingest_pipeline_from_python_to_go/, https://ai.pydantic.dev/changelog/, https://medium.com/@albertoarrigoni/production-grade-ai-assisted-coding-python-first-patterns-for-data-ml-teams-6880bfc111a6, https://news.ycombinator.com/item?id=45055439, https://martinfowler.com/articles/build-own-coding-agent.html, https://medium.com/@sharma31193/parallel-model-training-data-pipelines-on-databricks-foreach-tasks-asset-bundles-pydantic-991cfba11dc5'), Section(title='7. Conclusion', content='Pydantic has firmly established itself as a critical tool for building robust, reliable, and maintainable AI applications. Its emphasis on type safety and structured data addresses many of the inherent complexities in AI/ML workflows, from input validation to enforcing predictable outputs from advanced models like LLMs. As the AI landscape continues to evolve, the demand for structured data and strong type safety will only increase, making Pydantic an even more indispensable library. Developers leveraging AI are strongly recommended to adopt Pydantic to enhance data integrity, reduce development complexities, minimize runtime errors, and ultimately build more trustworthy and efficient AI systems.', references='https://medium.com/@01coder/pydantic-ai-the-type-safe-ai-agent-framework-for-building-robust-llm-applications-ec0ce3366862, https://www.linkedin.com/pulse/leveraging-pydantic-robust-structured-outputs-from-large-amit-kohad-yrmkc, https://www.startuphub.ai/ai-news/ai-video/2025/pydantics-vision-for-robust-ai-type-safety-agents-and-observability/, https://ai.pydantic.dev/llms-full.txt, https://codecut.ai/enforce-structured-outputs-from-llms-with-pydanticai/')]), tool_name='final_result', tool_call_id='pyd_ai_acbfbf4870a54608ba6fb6c29622a9ad'))\n",
      "INFO:web_deep_researcher:Getting the report for the information: [Section(title='Summary', content='Pydantic has emerged as a crucial library in modern AI development, primarily by providing robust data validation and structuring capabilities. Its integration offers key benefits such as improved type safety, reduced debugging time, and enhanced reliability of AI systems. Pydantic facilitates the creation of predictable data flows, which is essential for complex AI/ML workflows. Practical implications span various use cases, including the validation of inputs and outputs for AI models, enforcing structured responses from Large Language Models (LLMs), defining tools for AI agents, and establishing clear data schemas for MLOps pipelines.', references='https://www.geeky-gadgets.com/pydantic-ai-app-development/, https://towardsdatascience.com/applied-python-chronicles-a-gentle-intro-to-pydantic-a630f797e933/, https://www.civo.com/learn/simplifying-ai-development-with-pydanticai, https://www.datacamp.com/tutorial/pydantic, https://ai.pydantic.dev/'), Section(title='Introduction', content='This report explores the indispensable role of Pydantic in shaping the landscape of modern AI applications, addressing critical data management and workflow challenges.', references=''), Section(title='2.1 What is Pydantic?', content=\"Pydantic is a powerful Python library designed for data validation and settings management. It leverages Python's type hints to define schemas and validate data at runtime. Core features include automatic data parsing, robust serialization and deserialization capabilities, and runtime validation, which collectively ensure data integrity and consistency throughout an application's lifecycle.\", references='https://www.datagluons.io/blog/pydantic-data-contract-manager, https://medium.com/@terabyte26/pydantic-v2-patterns-for-automation-validate-anything-everywhere-6d553504a25a, https://superjson.ai/blog/2025-08-14-pydantic-vs-zod-json-schema-generation-comparison, https://www.reddit.com/r/Python/comments/1n0ng7f/whats_your_favorite_python_trick_or_lesser_known/, https://docs.langchain.com/oss/python/langgraph/use-graph-api, https://betterstack.com/community/guides/scaling-python/pydantic-explained/, https://docs.pydantic.dev/latest/concepts/serialization/, https://stackoverflow.com/questions/65622045/pydantic-convert-to-jsonable-dict-not-full-json-string, https://github.com/pydantic/pydantic-core, https://pypi.org/project/pydantic-client/, https://github.com/orgs/pydantic/repositories, https://latenode.com/blog/11-open-source-ai-agent-frameworks-that-will-transform-your-development-2025-complete-guide, https://lawrencemcdaniel.com/data-science'), Section(title='2.2 The Challenge of Data in AI Systems', content='In AI and Machine Learning (ML) workflows, data integrity and structure are paramount. The reliability of AI systems heavily depends on the quality and consistency of the data they process. Common challenges include handling unstructured inputs and outputs, managing schema mismatches between different system components, and the inherent complexity of debugging issues that arise from inconsistent data. These issues can lead to unreliable models, incorrect predictions, and significant development roadblocks.', references='https://www.cloudtech.com/resources/data-integrity-importance-best-practices, https://spectrum.ieee.org/data-integrity, https://www.datajoint.com/news/ai-and-the-evolution-of-relational-schemas'), Section(title='2.3 Why Pydantic for AI?', content='Pydantic directly addresses the aforementioned data challenges in AI by enabling developers to define clear data models and enforce validation rules. By ensuring that data conforms to predefined schemas, Pydantic mitigates issues like unstructured inputs/outputs and schema mismatches, thereby simplifying debugging and increasing the overall robustness of AI systems. This report aims to delve into how Pydantic achieves this, its fundamental features, its practical applications, and the benefits and challenges associated with its integration into AI workflows.', references='https://latenode.com/blog/11-open-source-ai-agent-frameworks-that-will-transform-your-development-2025-complete-guide, https://medium.com/@judeaugustinej/pydantic-for-llm-workflows-631a7c01fdba, https://brightdata.com/blog/ai/pydantic-ai-with-web-mcp'), Section(title='3. Pydantic Fundamentals Relevant to AI', content=\"Pydantic's core strength lies in its ability to enforce data integrity and structure through a set of fundamental features that are particularly valuable in AI applications.\", references=''), Section(title='3.1 Data Models and Type Hinting', content='Pydantic allows developers to define complex data structures using standard Python type hints. By inheriting from `pydantic.BaseModel`, users can create classes where each attribute has a specified type. This approach significantly enhances code clarity, making it easier to understand the expected data format. It also improves maintainability by allowing IDEs to provide better autocompletion and static analysis, catching potential type-related errors early in the development cycle.', references='https://www.ireneburresi.dev/p/the-modern-python-toolbox-an-engineered, https://moldstud.com/articles/p-designing-python-rest-apis-with-openapi-specification-a-comprehensive-guide, https://www.reddit.com/r/Python/comments/1n4hc9e/python_type_system/'), Section(title='3.2 Validation and Error Handling', content=\"One of Pydantic's most powerful features is its automatic data validation. When a Pydantic model is instantiated or data is assigned to its fields, Pydantic automatically validates the input against the defined types and constraints. If validation fails, it raises clear and informative `ValidationError` exceptions. Developers can also implement custom validators using decorators like `@field_validator` or `@model_validator` (in V2) to enforce more complex business logic or data rules, providing granular control over data quality and robust error reporting mechanisms.\", references='https://dev.to/aws-builders/how-to-handle-form-data-in-aws-lambda-apis-with-powertools-openapi-support-4l40, https://medium.com/@shouke.wei/the-power-of-pydantic-bringing-structure-and-validation-to-your-python-data-9de124362a6f, https://medium.com/@terabyte26/pydantic-v2-patterns-for-automation-validate-anything-everywhere-6d553504a25a'), Section(title='3.3 Serialization and Deserialization', content='Pydantic models seamlessly handle the conversion of data to and from various formats, such as JSON strings and Python dictionaries. The `model_dump()` and `model_dump_json()` methods (in V2) allow for easy serialization, making it straightforward to prepare data for API responses, logging, or storage. Conversely, models can be instantiated directly from dictionaries or JSON strings (via `model_validate()` or `model_validate_json()`), facilitating deserialization of incoming data from external sources. This capability is vital for interacting with web APIs and ensuring data persistence in a structured manner.', references='https://betterstack.com/community/guides/scaling-python/pydantic-explained/, https://docs.pydantic.dev/latest/concepts/serialization/, https://stackoverflow.com/questions/65622045/pydantic-convert-to-jsonable-dict-not-full-json-string'), Section(title='3.4 Schema Generation (JSON Schema)', content='Pydantic can automatically generate JSON Schemas from its data models. This feature is invaluable for documenting API contracts and ensuring interoperability with other systems that consume or produce JSON data. The generated JSON Schema provides a machine-readable description of the expected data structure, which can be used for client-side validation, code generation in other languages, or for integration with tools like OpenAPI/Swagger to provide interactive API documentation.', references='https://www.scaleway.com/en/docs/generative-apis/how-to/use-structured-outputs/, https://ai.pydantic.dev/api/tools/, https://python.useinstructor.com/architecture/'), Section(title=\"4. Pydantic's Role in AI Data Management and Workflows\", content=\"Pydantic's strong data validation and structuring capabilities make it an invaluable asset across various stages of AI data management and workflow orchestration.\", references=''), Section(title='4.1 Input Validation for AI Models', content='Ensuring the quality and correctness of data fed into machine learning models is critical for their performance and reliability. Pydantic enables rigorous input validation, allowing developers to define expected data types, ranges, and structures for diverse inputs such as text, numerical data, and categorical features. By validating inputs before they reach the model, Pydantic helps prevent common issues like malformed data, out-of-range values, and type mismatches, thus enhancing the robustness of AI models.', references='https://www.netguru.com/blog/data-validation-pydantic, https://codilime.com/blog/declarative-data-validation-pydantic/, https://dev.to/mechcloud_academy/practical-pydantic-data-transformation-aliases-and-validators-16c2'), Section(title='4.2 Structuring and Validating AI Model Outputs', content=\"Just as with inputs, controlling the format and content of AI model outputs is crucial for downstream processing and application reliability. Pydantic allows developers to define explicit schemas for predictions, classifications, and generated content. This ensures that the AI model's output adheres to a predictable structure, making it easier for other parts of the application or external systems to consume and interpret the results. This is particularly beneficial in applications where AI outputs trigger further actions or are displayed to users.\", references='https://pydantic.dev/articles/llm-intro, https://dev.to/devasservice/a-practical-guide-on-structuring-llm-outputs-with-pydantic-50b4, https://medium.com/@sridevi.gogusetty/building-multi-agent-llm-systems-with-pydanticai-framework-a-step-by-step-guide-to-create-ai-595cfcdede92'), Section(title='4.3 Pydantic in Large Language Model (LLM) Applications', content='Pydantic plays a particularly transformative role in the development of Large Language Model (LLM) applications by bringing structure and reliability to their often-unpredictable outputs.', references=''), Section(title='4.3.1 Enforcing Structured Output from LLMs', content=\"One of the significant challenges with LLMs is obtaining reliable, structured data from their free-form text generation. Pydantic enables developers to define a target JSON schema that LLMs must adhere to when generating responses. By providing the LLM with a Pydantic model's JSON schema, tools and frameworks can guide the LLM to produce outputs that are directly parsable and validatable, ensuring that the generated data, such as a list of entities or a specific data object, is consistent and usable.\", references='https://codecut.ai/enforce-structured-outputs-from-llms-with-pydanticai/, https://dev.to/devasservice/a-practical-guide-on-structuring-llm-outputs-with-pydantic-50b4, https://datachain.ai/blog/enforcing-json-outputs-in-commercial-llms'), Section(title='4.3.2 Defining Tools and Functions for LLM Agents', content=\"In the context of LLM agents and function calling (e.g., with OpenAI's API), Pydantic is used to clearly define the inputs and outputs of tools and functions that the LLM can invoke. By representing tool schemas as Pydantic models, developers can provide a precise, machine-readable description of what a tool expects and what it returns. This allows LLMs to understand how to interact with external systems more effectively, leading to more reliable and accurate tool execution within agentic workflows.\", references='https://www.reddit.com/r/LangChain/comments/1mynq4a/agents_are_just_llm_loop_tools_its_simpler_than_people_make_it/, https://ai.pydantic.dev/api/agent/, https://ai.pydantic.dev/api/tools/'), Section(title='4.3.3 Validating LLM Prompts and Responses', content=\"Pydantic can also be employed to validate the format and content of both LLM prompts and their subsequent responses. This ensures that prompts adhere to expected templates or contain necessary information, and that responses, even when unstructured, contain certain keywords or follow a general pattern. While more challenging for free-form text, Pydantic's custom validators can be adapted to check for adherence to expected formats, improving the overall quality and predictability of LLM interactions.\", references='https://docs.databricks.com/aws/en/mlflow3/genai/tracing/integrations/pydantic-ai, https://pub.towardsai.net/how-to-ensure-reliability-in-llm-applications-13302bd7521a, https://medium.com/@albertoarrigoni/production-grade-ai-assisted-coding-python-first-patterns-for-data-ml-teams-6880bfc111a6'), Section(title='4.4 Data Schemas for MLOps and Data Pipelines', content='In MLOps and complex data pipelines, defining clear data contracts is essential. Pydantic models can serve as these contracts, establishing explicit schemas for data flowing between different stages of an ML pipeline—from data ingestion and preprocessing to model training and deployment. This approach significantly improves data governance, ensures data consistency across teams, and enhances reproducibility by providing a verifiable standard for all data artifacts.', references='https://www.linkedin.com/posts/shantanuladhwe_ive-set-up-10-ai-production-environments-activity-7368870388241510400-EzeU, https://omyeole.com/work/house_price_predictor, https://huggingface.co/papers?q=modular%20collaborative%20framework'), Section(title='5. Practical Applications and Use Cases', content=\"Pydantic's versatility makes it applicable across a wide range of practical scenarios in AI development, from API design to agentic systems.\", references=''), Section(title='5.1 Building Robust AI APIs', content=\"Pydantic is a cornerstone for building robust and reliable AI APIs, particularly when integrated with modern Python web frameworks like FastAPI. FastAPI, in fact, uses Pydantic extensively for automatic request body parsing, validation, and serialization of responses. This ensures that incoming data to an AI service conforms to the expected schema and that the API's outputs are consistently structured. While Flask can also leverage Pydantic for validation, FastAPI's native integration streamlines the process, leading to more stable and maintainable AI-powered microservices.\", references='https://medium.com/@hadiyolworld007/from-prototype-to-production-fastapi-in-the-era-of-ai-native-apps-faf7b0b566f1, https://dev.to/atifwattoo/django-vs-flask-vs-fastapi, https://www.geeksforgeeks.org/python/rest-api-with-fastapi/, https://zenvanriel.nl/ai-engineer-blog/fastapi-vs-flask-for-ai-applications/, https://www.netguru.com/blog/python-flask-versus-fastapi'), Section(title='5.2 Enhancing Data Ingestion for Training Data', content='Before any machine learning model can be trained, the raw data must be thoroughly validated and cleaned. Pydantic models can be used to define the schema for training data, allowing for automatic validation of incoming data streams or datasets. This ensures that only high-quality, correctly formatted data is used for training, preventing subtle data errors from propagating through the model and impacting its performance. Custom validators can also be applied to enforce specific data cleaning rules during ingestion.', references='https://www.prefect.io/blog/what-is-pydantic-validating-data-in-python, https://medium.com/@durgarao3d6/clean-and-reliable-data-handling-in-python-using-pydantic-3aef7c42d2f8, https://www.reddit.com/r/Python/comments/16xnhim/what_problems_does_pydantic_solves_and_how_should/, https://data-ai.theodo.com/en/technical-blog/boost-your-data-projects-with-pydantic-validation-and-efficiency, https://medium.com/@CodeWithHannan/data-validation-and-schema-enforcement-in-python-using-pydantic-886ab64fecba'), Section(title='5.3 Developing Reliable AI Agents and Chatbots', content='Pydantic is instrumental in developing reliable AI agents and chatbots by providing a structured way to manage conversational states, user inputs, and agent actions. By defining Pydantic models for various conversational elements, developers can ensure that user queries are correctly parsed, agent responses adhere to expected formats, and internal states are consistently maintained. This structured approach significantly improves the robustness and predictability of AI-driven conversational systems.', references='https://latenode.com/blog/11-open-source-ai-agent-frameworks-that-will-transform-your-development-2025-complete-guide, https://medium.com/@tam.tamanna18/why-ai-frameworks-langchain-crewai-pydanticai-and-others-fail-in-production-c6292b98ee59, https://devot.team/blog/agentic-ai, https://huggingface.co/papers?q=agile%20methodologies, https://www.researchgate.net/publication/383032644_NLP_Chatbot_For_Order_Assistance_Using_Dialogflow'), Section(title='5.4 Integrating Pydantic with Popular AI Frameworks/Libraries', content=\"Pydantic's utility extends to seamless integration with popular AI frameworks and libraries. For instance, in LangChain and LlamaIndex, Pydantic models are widely used to define schemas for LLM outputs, tool definitions, and data structures within agents. The OpenAI Python client also leverages Pydantic for its function-calling capabilities, allowing developers to describe desired output formats using Pydantic models directly. This integration streamlines the development of complex AI applications by providing a consistent and type-safe way to handle data across different components.\", references='https://docs.llamaindex.ai/en/stable/examples/output_parsing/openai_pydantic_program/, https://ai.pydantic.dev/, https://docs.llamaindex.ai/en/stable/examples/llm/openai/, https://logfire.pydantic.dev/docs/integrations/, https://github.com/pydantic/pydantic-ai'), Section(title='6. Benefits and Challenges of Pydantic-AI Integration', content='While Pydantic offers significant advantages for AI development, it also comes with certain considerations.', references=''), Section(title='6.1 Key Benefits', content='Integrating Pydantic into AI applications yields numerous benefits: \\n*   **Increased reliability and robustness:** By enforcing strict data schemas and validation rules, Pydantic significantly reduces the likelihood of data-related errors and unexpected behavior in AI systems.\\n*   **Improved developer experience and productivity:** Type hints and automatic validation lead to cleaner, more predictable code, which in turn enhances developer productivity and reduces the mental overhead of data management.\\n*   **Better code readability, maintainability, and collaboration:** Clearly defined data models improve code readability and make it easier for teams to collaborate on projects, as data contracts are explicit.\\n*   **Reduced debugging time and fewer runtime errors:** Many common data issues are caught at validation time, often before runtime, leading to less time spent on debugging and a decrease in production errors.\\n*   **Enhanced API design and documentation:** Automatic JSON schema generation and clear data models contribute to well-defined and self-documenting APIs for AI services.', references='https://hrshdg8.medium.com/agno-vs-pydantic-ai-the-ultimate-showdown-for-building-ai-agents-79b2c975cbec, https://medium.com/@tam.tamanna18/why-ai-frameworks-langchain-crewai-pydanticai-and-others-fail-in-production-c6292b98ee59, https://skylarbpayne.com/posts/ai-reliability-patterns/, https://ai.pydantic.dev/temporal/, https://latenode.com/blog/11-open-source-ai-agent-frameworks-that-will-transform-your-development-2025-complete-guide'), Section(title='6.2 Potential Challenges and Considerations', content=\"Despite its advantages, Pydantic integration presents a few challenges: \\n*   **Learning curve for Pydantic (especially for those new to type hints):** Developers new to Python type hints may experience a learning curve when adopting Pydantic, as its core functionality relies heavily on them.\\n*   **Performance considerations for very high-throughput systems (if complex validation):** For applications requiring extremely high throughput and complex, deeply nested validation, the overhead of Pydantic's runtime validation might introduce performance considerations that need careful optimization.\\n*   **Keeping up with Pydantic's evolving features (e.g., V1 vs. V2):** Pydantic is an actively developed library, and staying updated with its evolving features, particularly the significant changes between V1 and V2, can require effort. This is crucial for leveraging the latest optimizations and functionalities, especially in AI contexts where new patterns are constantly emerging.\", references='https://docs.pydantic.dev/2.0/usage/types/types/, https://docs.pydantic.dev/latest/concepts/types/, https://medium.com/@sulbha.jindal/pydantic-python-library-for-data-validation-cd8b9185fc4d, https://docs.pydantic.dev/1.10/usage/types/, https://stackoverflow.com/questions/75730322/type-check-pydantic-basemodel-as-type-hint-with-mypy, https://arxiv.org/html/2508.14123v1, https://medium.com/@patriwala/ai-augmented-development-the-complete-guide-bd39fc2316bd, https://www.tokenmetrics.com/blog/fastapi-high-performance-apis, https://www.tokenmetrics.com/blog/best-api-institutional-crypto-analytics?74e29fd5_page=2, https://www.reddit.com/r/Python/comments/1mqf3e6/we_rewrote_our_ingest_pipeline_from_python_to_go/, https://ai.pydantic.dev/changelog/, https://medium.com/@albertoarrigoni/production-grade-ai-assisted-coding-python-first-patterns-for-data-ml-teams-6880bfc111a6, https://news.ycombinator.com/item?id=45055439, https://martinfowler.com/articles/build-own-coding-agent.html, https://medium.com/@sharma31193/parallel-model-training-data-pipelines-on-databricks-foreach-tasks-asset-bundles-pydantic-991cfba11dc5'), Section(title='7. Conclusion', content='Pydantic has firmly established itself as a critical tool for building robust, reliable, and maintainable AI applications. Its emphasis on type safety and structured data addresses many of the inherent complexities in AI/ML workflows, from input validation to enforcing predictable outputs from advanced models like LLMs. As the AI landscape continues to evolve, the demand for structured data and strong type safety will only increase, making Pydantic an even more indispensable library. Developers leveraging AI are strongly recommended to adopt Pydantic to enhance data integrity, reduce development complexities, minimize runtime errors, and ultimately build more trustworthy and efficient AI systems.', references='https://medium.com/@01coder/pydantic-ai-the-type-safe-ai-agent-framework-for-building-robust-llm-applications-ec0ce3366862, https://www.linkedin.com/pulse/leveraging-pydantic-robust-structured-outputs-from-large-amit-kohad-yrmkc, https://www.startuphub.ai/ai-news/ai-video/2025/pydantics-vision-for-robust-ai-type-safety-agents-and-observability/, https://ai.pydantic.dev/llms-full.txt, https://codecut.ai/enforce-structured-outputs-from-llms-with-pydanticai/')]\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n"
     ]
    }
   ],
   "source": [
    "web_deep_researcher = web_deep_researcher()\n",
    "result = await web_deep_researcher.execute(\"how can I use pydantic-ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "113dc69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report Outline: Pydantic in AI Applications\n",
      "\n",
      "**1. Summary**\n",
      "Pydantic has emerged as a crucial library in modern AI development, primarily by providing robust data validation and structuring capabilities. Its integration offers key benefits such as improved type safety, reduced debugging time, and enhanced reliability of AI systems. Pydantic facilitates the creation of predictable data flows, which is essential for complex AI/ML workflows. Practical implications span various use cases, including the validation of inputs and outputs for AI models, enforcing structured responses from Large Language Models (LLMs), defining tools for AI agents, and establishing clear data schemas for MLOps pipelines.\n",
      "\n",
      "**2. Introduction**\n",
      "This report explores the indispensable role of Pydantic in shaping the landscape of modern AI applications, addressing critical data management and workflow challenges.\n",
      "\n",
      "**2.1 What is Pydantic?**\n",
      "Pydantic is a powerful Python library designed for data validation and settings management. It leverages Python's type hints to define schemas and validate data at runtime. Core features include automatic data parsing, robust serialization and deserialization capabilities, and runtime validation, which collectively ensure data integrity and consistency throughout an application's lifecycle.\n",
      "\n",
      "**2.2 The Challenge of Data in AI Systems**\n",
      "In AI and Machine Learning (ML) workflows, data integrity and structure are paramount. The reliability of AI systems heavily depends on the quality and consistency of the data they process. Common challenges include handling unstructured inputs and outputs, managing schema mismatches between different system components, and the inherent complexity of debugging issues that arise from inconsistent data. These issues can lead to unreliable models, incorrect predictions, and significant development roadblocks.\n",
      "\n",
      "**2.3 Why Pydantic for AI?**\n",
      "Pydantic directly addresses the aforementioned data challenges in AI by enabling developers to define clear data models and enforce validation rules. By ensuring that data conforms to predefined schemas, Pydantic mitigates issues like unstructured inputs/outputs and schema mismatches, thereby simplifying debugging and increasing the overall robustness of AI systems. This report aims to delve into how Pydantic achieves this, its fundamental features, its practical applications, and the benefits and challenges associated with its integration into AI workflows.\n",
      "\n",
      "**3. Pydantic Fundamentals Relevant to AI**\n",
      "Pydantic's core strength lies in its ability to enforce data integrity and structure through a set of fundamental features that are particularly valuable in AI applications.\n",
      "\n",
      "**3.1 Data Models and Type Hinting**\n",
      "Pydantic allows developers to define complex data structures using standard Python type hints. By inheriting from `pydantic.BaseModel`, users can create classes where each attribute has a specified type. This approach significantly enhances code clarity, making it easier to understand the expected data format. It also improves maintainability by allowing IDEs to provide better autocompletion and static analysis, catching potential type-related errors early in the development cycle.\n",
      "\n",
      "**3.2 Validation and Error Handling**\n",
      "One of Pydantic's most powerful features is its automatic data validation. When a Pydantic model is instantiated or data is assigned to its fields, Pydantic automatically validates the input against the defined types and constraints. If validation fails, it raises clear and informative `ValidationError` exceptions. Developers can also implement custom validators using decorators like `@field_validator` or `@model_validator` (in V2) to enforce more complex business logic or data rules, providing granular control over data quality and robust error reporting mechanisms.\n",
      "\n",
      "**3.3 Serialization and Deserialization**\n",
      "Pydantic models seamlessly handle the conversion of data to and from various formats, such as JSON strings and Python dictionaries. The `model_dump()` and `model_dump_json()` methods (in V2) allow for easy serialization, making it straightforward to prepare data for API responses, logging, or storage. Conversely, models can be instantiated directly from dictionaries or JSON strings (via `model_validate()` or `model_validate_json()`), facilitating deserialization of incoming data from external sources. This capability is vital for interacting with web APIs and ensuring data persistence in a structured manner.\n",
      "\n",
      "**3.4 Schema Generation (JSON Schema)**\n",
      "Pydantic can automatically generate JSON Schemas from its data models. This feature is invaluable for documenting API contracts and ensuring interoperability with other systems that consume or produce JSON data. The generated JSON Schema provides a machine-readable description of the expected data structure, which can be used for client-side validation, code generation in other languages, or for integration with tools like OpenAPI/Swagger to provide interactive API documentation.\n",
      "\n",
      "**4. Pydantic's Role in AI Data Management and Workflows**\n",
      "Pydantic's strong data validation and structuring capabilities make it an invaluable asset across various stages of AI data management and workflow orchestration.\n",
      "\n",
      "**4.1 Input Validation for AI Models**\n",
      "Ensuring the quality and correctness of data fed into machine learning models is critical for their performance and reliability. Pydantic enables rigorous input validation, allowing developers to define expected data types, ranges, and structures for diverse inputs such as text, numerical data, and categorical features. By validating inputs before they reach the model, Pydantic helps prevent common issues like malformed data, out-of-range values, and type mismatches, thus enhancing the robustness of AI models.\n",
      "\n",
      "**4.2 Structuring and Validating AI Model Outputs**\n",
      "Just as with inputs, controlling the format and content of AI model outputs is crucial for downstream processing and application reliability. Pydantic allows developers to define explicit schemas for predictions, classifications, and generated content. This ensures that the AI model's output adheres to a predictable structure, making it easier for other parts of the application or external systems to consume and interpret the results. This is particularly beneficial in applications where AI outputs trigger further actions or are displayed to users.\n",
      "\n",
      "**4.3 Pydantic in Large Language Model (LLM) Applications**\n",
      "Pydantic plays a particularly transformative role in the development of Large Language Model (LLM) applications by bringing structure and reliability to their often-unpredictable outputs.\n",
      "\n",
      "**4.3.1 Enforcing Structured Output from LLMs**\n",
      "One of the significant challenges with LLMs is obtaining reliable, structured data from their free-form text generation. Pydantic enables developers to define a target JSON schema that LLMs must adhere to when generating responses. By providing the LLM with a Pydantic model's JSON schema, tools and frameworks can guide the LLM to produce outputs that are directly parsable and validatable, ensuring that the generated data, such as a list of entities or a specific data object, is consistent and usable.\n",
      "\n",
      "**4.3.2 Defining Tools and Functions for LLM Agents**\n",
      "In the context of LLM agents and function calling (e.g., with OpenAI's API), Pydantic is used to clearly define the inputs and outputs of tools and functions that the LLM can invoke. By representing tool schemas as Pydantic models, developers can provide a precise, machine-readable description of what a tool expects and what it returns. This allows LLMs to understand how to interact with external systems more effectively, leading to more reliable and accurate tool execution within agentic workflows.\n",
      "\n",
      "**4.3.3 Validating LLM Prompts and Responses**\n",
      "Pydantic can also be employed to validate the format and content of both LLM prompts and their subsequent responses. This ensures that prompts adhere to expected templates or contain necessary information, and that responses, even when unstructured, contain certain keywords or follow a general pattern. While more challenging for free-form text, Pydantic's custom validators can be adapted to check for adherence to expected formats, improving the overall quality and predictability of LLM interactions.\n",
      "\n",
      "**4.4 Data Schemas for MLOps and Data Pipelines**\n",
      "In MLOps and complex data pipelines, defining clear data contracts is essential. Pydantic models can serve as these contracts, establishing explicit schemas for data flowing between different stages of an ML pipeline—from data ingestion and preprocessing to model training and deployment. This approach significantly improves data governance, ensures data consistency across teams, and enhances reproducibility by providing a verifiable standard for all data artifacts.\n",
      "\n",
      "**5. Practical Applications and Use Cases**\n",
      "Pydantic's versatility makes it applicable across a wide range of practical scenarios in AI development, from API design to agentic systems.\n",
      "\n",
      "**5.1 Building Robust AI APIs**\n",
      "Pydantic is a cornerstone for building robust and reliable AI APIs, particularly when integrated with modern Python web frameworks like FastAPI. FastAPI, in fact, uses Pydantic extensively for automatic request body parsing, validation, and serialization of responses. This ensures that incoming data to an AI service conforms to the expected schema and that the API's outputs are consistently structured. While Flask can also leverage Pydantic for validation, FastAPI's native integration streamlines the process, leading to more stable and maintainable AI-powered microservices.\n",
      "\n",
      "**5.2 Enhancing Data Ingestion for Training Data**\n",
      "Before any machine learning model can be trained, the raw data must be thoroughly validated and cleaned. Pydantic models can be used to define the schema for training data, allowing for automatic validation of incoming data streams or datasets. This ensures that only high-quality, correctly formatted data is used for training, preventing subtle data errors from propagating through the model and impacting its performance. Custom validators can also be applied to enforce specific data cleaning rules during ingestion.\n",
      "\n",
      "**5.3 Developing Reliable AI Agents and Chatbots**\n",
      "Pydantic is instrumental in developing reliable AI agents and chatbots by providing a structured way to manage conversational states, user inputs, and agent actions. By defining Pydantic models for various conversational elements, developers can ensure that user queries are correctly parsed, agent responses adhere to expected formats, and internal states are consistently maintained. This structured approach significantly improves the robustness and predictability of AI-driven conversational systems.\n",
      "\n",
      "**5.4 Integrating Pydantic with Popular AI Frameworks/Libraries**\n",
      "Pydantic's utility extends to seamless integration with popular AI frameworks and libraries. For instance, in LangChain and LlamaIndex, Pydantic models are widely used to define schemas for LLM outputs, tool definitions, and data structures within agents. The OpenAI Python client also leverages Pydantic for its function-calling capabilities, allowing developers to describe desired output formats using Pydantic models directly. This integration streamlines the development of complex AI applications by providing a consistent and type-safe way to handle data across different components.\n",
      "\n",
      "**6. Benefits and Challenges of Pydantic-AI Integration**\n",
      "While Pydantic offers significant advantages for AI development, it also comes with certain considerations.\n",
      "\n",
      "**6.1 Key Benefits**\n",
      "Integrating Pydantic into AI applications yields numerous benefits:\n",
      "*   **Increased reliability and robustness:** By enforcing strict data schemas and validation rules, Pydantic significantly reduces the likelihood of data-related errors and unexpected behavior in AI systems.\n",
      "*   **Improved developer experience and productivity:** Type hints and automatic validation lead to cleaner, more predictable code, which in turn enhances developer productivity and reduces the mental overhead of data management.\n",
      "*   **Better code readability, maintainability, and collaboration:** Clearly defined data models improve code readability and make it easier for teams to collaborate on projects, as data contracts are explicit.\n",
      "*   **Reduced debugging time and fewer runtime errors:** Many common data issues are caught at validation time, often before runtime, leading to less time spent on debugging and a decrease in production errors.\n",
      "*   **Enhanced API design and documentation:** Automatic JSON schema generation and clear data models contribute to well-defined and self-documenting APIs for AI services.\n",
      "\n",
      "**6.2 Potential Challenges and Considerations**\n",
      "Despite its advantages, Pydantic integration presents a few challenges:\n",
      "*   **Learning curve for Pydantic (especially for those new to type hints):** Developers new to Python type hints may experience a learning curve when adopting Pydantic, as its core functionality relies heavily on them.\n",
      "*   **Performance considerations for very high-throughput systems (if complex validation):** For applications requiring extremely high throughput and complex, deeply nested validation, the overhead of Pydantic's runtime validation might introduce performance considerations that need careful optimization.\n",
      "*   **Keeping up with Pydantic's evolving features (e.g., V1 vs. V2):** Pydantic is an actively developed library, and staying updated with its evolving features, particularly the significant changes between V1 and V2, can require effort. This is crucial for leveraging the latest optimizations and functionalities, especially in AI contexts where new patterns are constantly emerging.\n",
      "\n",
      "**7. Conclusion**\n",
      "Pydantic has firmly established itself as a critical tool for building robust, reliable, and maintainable AI applications. Its emphasis on type safety and structured data addresses many of the inherent complexities in AI/ML workflows, from input validation to enforcing predictable outputs from advanced models like LLMs. As the AI landscape continues to evolve, the demand for structured data and strong type safety will only increase, making Pydantic an even more indispensable library. Developers leveraging AI are strongly recommended to adopt Pydantic to enhance data integrity, reduce development complexities, minimize runtime errors, and ultimately build more trustworthy and efficient AI systems.\n",
      "\n",
      "**8. References**\n",
      "*   https://www.geeky-gadgets.com/pydantic-ai-app-development/\n",
      "*   https://towardsdatascience.com/applied-python-chronicles-a-gentle-intro-to-pydantic-a630f797e933/\n",
      "*   https://www.civo.com/learn/simplifying-ai-development-with-pydanticai\n",
      "*   https://www.datacamp.com/tutorial/pydantic\n",
      "*   https://ai.pydantic.dev/\n",
      "*   https://www.datagluons.io/blog/pydantic-data-contract-manager\n",
      "*   https://medium.com/@terabyte26/pydantic-v2-patterns-for-automation-validate-anything-everywhere-6d553504a25a\n",
      "*   https://superjson.ai/blog/2025-08-14-pydantic-vs-zod-json-schema-generation-comparison\n",
      "*   https://www.reddit.com/r/Python/comments/1n0ng7f/whats_your_favorite_python_trick_or_lesser_known/\n",
      "*   https://docs.langchain.com/oss/python/langgraph/use-graph-api\n",
      "*   https://betterstack.com/community/guides/scaling-python/pydantic-explained/\n",
      "*   https://docs.pydantic.dev/latest/concepts/serialization/\n",
      "*   https://stackoverflow.com/questions/65622045/pydantic-convert-to-jsonable-dict-not-full-json-string\n",
      "*   https://github.com/pydantic/pydantic-core\n",
      "*   https://pypi.org/project/pydantic-client/\n",
      "*   https://github.com/orgs/pydantic/repositories\n",
      "*   https://latenode.com/blog/11-open-source-ai-agent-frameworks-that-will-transform-your-development-2025-complete-guide\n",
      "*   https://lawrencemcdaniel.com/data-science\n",
      "*   https://www.cloudtech.com/resources/data-integrity-importance-best-practices\n",
      "*   https://spectrum.ieee.org/data-integrity\n",
      "*   https://www.datajoint.com/news/ai-and-the-evolution-of-relational-schemas\n",
      "*   https://medium.com/@judeaugustinej/pydantic-for-llm-workflows-631a7c01fdba\n",
      "*   https://brightdata.com/blog/ai/pydantic-ai-with-web-mcp\n",
      "*   https://www.ireneburresi.dev/p/the-modern-python-toolbox-an-engineered\n",
      "*   https://moldstud.com/articles/p-designing-python-rest-apis-with-openapi-specification-a-comprehensive-guide\n",
      "*   https://www.reddit.com/r/Python/comments/1n4hc9e/python_type_system/\n",
      "*   https://dev.to/aws-builders/how-to-handle-form-data-in-aws-lambda-apis-with-powertools-openapi-support-4l40\n",
      "*   https://medium.com/@shouke.wei/the-power-of-pydantic-bringing-structure-and-validation-to-your-python-data-9de124362a6f\n",
      "*   https://www.scaleway.com/en/docs/generative-apis/how-to/use-structured-outputs/\n",
      "*   https://ai.pydantic.dev/api/tools/\n",
      "*   https://python.useinstructor.com/architecture/\n",
      "*   https://www.netguru.com/blog/data-validation-pydantic\n",
      "*   https://codilime.com/blog/declarative-data-validation-pydantic/\n",
      "*   https://dev.to/mechcloud_academy/practical-pydantic-data-transformation-aliases-and-validators-16c2\n",
      "*   https://pydantic.dev/articles/llm-intro\n",
      "*   https://dev.to/devasservice/a-practical-guide-on-structuring-llm-outputs-with-pydantic-50b4\n",
      "*   https://medium.com/@sridevi.gogusetty/building-multi-agent-llm-systems-with-pydanticai-framework-a-step-by-step-guide-to-create-ai-595cfcdede92\n",
      "*   https://codecut.ai/enforce-structured-outputs-from-llms-with-pydanticai/\n",
      "*   https://datachain.ai/blog/enforcing-json-outputs-in-commercial-llms\n",
      "*   https://www.reddit.com/r/LangChain/comments/1mynq4a/agents_are_just_llm_loop_tools_its_simpler_than_people_make_it/\n",
      "*   https://ai.pydantic.dev/api/agent/\n",
      "*   https://docs.databricks.com/aws/en/mlflow3/genai/tracing/integrations/pydantic-ai\n",
      "*   https://pub.towardsai.net/how-to-ensure-reliability-in-llm-applications-13302bd7521a\n",
      "*   https://medium.com/@albertoarrigoni/production-grade-ai-assisted-coding-python-first-patterns-for-data-ml-teams-6880bfc111a6\n",
      "*   https://www.linkedin.com/posts/shantanuladhwe_ive-set-up-10-ai-production-environments-activity-7368870388241510400-EzeU\n",
      "*   https://omyeole.com/work/house_price_predictor\n",
      "*   https://huggingface.co/papers?q=modular%20collaborative%20framework\n",
      "*   https://medium.com/@hadiyolworld007/from-prototype-to-production-fastapi-in-the-era-of-ai-native-apps-faf7b0b566f1\n",
      "*   https://dev.to/atifwattoo/django-vs-flask-vs-fastapi\n",
      "*   https://www.geeksforgeeks.org/python/rest-api-with-fastapi/\n",
      "*   https://zenvanriel.nl/ai-engineer-blog/fastapi-vs-flask-for-ai-applications/\n",
      "*   https://www.netguru.com/blog/python-flask-versus-fastapi\n",
      "*   https://www.prefect.io/blog/what-is-pydantic-validating-data-in-python\n",
      "*   https://medium.com/@durgarao3d6/clean-and-reliable-data-handling-in-python-using-pydantic-3aef7c42d2f8\n",
      "*   https://www.reddit.com/r/Python/comments/16xnhim/what_problems_does_pydantic_solves_and_how_should/\n",
      "*   https://data-ai.theodo.com/en/technical-blog/boost-your-data-projects-with-pydantic-validation-and-efficiency\n",
      "*   https://medium.com/@CodeWithHannan/data-validation-and-schema-enforcement-in-python-using-pydantic-886ab64fecba\n",
      "*   https://medium.com/@tam.tamanna18/why-ai-frameworks-langchain-crewai-pydanticai-and-others-fail-in-production-c6292b98ee59\n",
      "*   https://devot.team/blog/agentic-ai\n",
      "*   https://huggingface.co/papers?q=agile%20methodologies\n",
      "*   https://www.researchgate.net/publication/383032644_NLP_Chatbot_For_Order_Assistance_Using_Dialogflow\n",
      "*   https://docs.llamaindex.ai/en/stable/examples/output_parsing/openai_pydantic_program/\n",
      "*   https://ai.pydantic.dev/\n",
      "*   https://docs.llamaindex.ai/en/stable/examples/llm/openai/\n",
      "*   https://logfire.pydantic.dev/docs/integrations/\n",
      "*   https://github.com/pydantic/pydantic-ai\n",
      "*   https://hrshdg8.medium.com/agno-vs-pydantic-ai-the-ultimate-showdown-for-building-ai-agents-79b2c975cbec\n",
      "*   https://skylarbpayne.com/posts/ai-reliability-patterns/\n",
      "*   https://ai.pydantic.dev/temporal/\n",
      "*   https://docs.pydantic.dev/2.0/usage/types/types/\n",
      "*   https://docs.pydantic.dev/latest/concepts/types/\n",
      "*   https://medium.com/@sulbha.jindal/pydantic-python-library-for-data-validation-cd8b9185fc4d\n",
      "*   https://docs.pydantic.dev/1.10/usage/types/\n",
      "*   https://stackoverflow.com/questions/75730322/type-check-pydantic-basemodel-as-type-hint-with-mypy\n",
      "*   https://arxiv.org/html/2508.14123v1\n",
      "*   https://www.tokenmetrics.com/blog/fastapi-high-performance-apis\n",
      "*   https://www.tokenmetrics.com/blog/best-api-institutional-crypto-analytics?74e29fd5_page=2\n",
      "*   https://www.reddit.com/r/Python/comments/1mqf3e6/we_rewrote_our_ingest_pipeline_from_python_to_go/\n",
      "*   https://ai.pydantic.dev/changelog/\n",
      "*   https://news.ycombinator.com/item?id=45055439\n",
      "*   https://martinfowler.com/articles/build-own-coding-agent.html\n",
      "*   https://medium.com/@sharma31193/parallel-model-training-data-pipelines-on-databricks-foreach-tasks-asset-bundles-pydantic-991cfba11dc5\n",
      "*   https://medium.com/@01coder/pydantic-ai-the-type-safe-ai-agent-framework-for-building-robust-llm-applications-ec0ce3366862\n",
      "*   https://www.linkedin.com/pulse/leveraging-pydantic-robust-structured-outputs-from-large-amit-kohad-yrmkc/\n",
      "*   https://www.startuphub.ai/ai-news/ai-video/2025/pydantics-vision-for-robust-ai-type-safety-agents-and-observability/\n",
      "*   https://ai.pydantic.dev/llms-full.txt\n",
      "*   https://codecut.ai/enforce-structured-outputs-from-llms-with-pydanticai/\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
