{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f66c58e",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "The goal of this Lab is to demonstrate how to create a simple Agent using Pydantic AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142fbb13",
   "metadata": {},
   "source": [
    "# LLM Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa70278",
   "metadata": {},
   "source": [
    "We define the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f3cf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_settings import BaseSettings\n",
    "import os\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    GOOGLE_API_KEY: str\n",
    "    MODEL_NAME: str = \"google-gla:gemini-2.5-flash\"\n",
    "    CONTEXT7_API_KEY: str\n",
    "    class Config:\n",
    "        #ignore extra fields\n",
    "        extra = \"ignore\"\n",
    "        env_file = \".env\"\n",
    "\n",
    "settings = Settings()\n",
    "os.environ[\"GOOGLE_API_KEY\"] = settings.GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1718c818",
   "metadata": {},
   "source": [
    "# Simple LLM Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d084b4",
   "metadata": {},
   "source": [
    "Simple LLM call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9200e763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's one for you:\n",
      "\n",
      "Why don't scientists trust atoms?\n",
      "\n",
      "...Because they make up *everything*!\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai import Agent\n",
    "joke_agent = Agent(settings.MODEL_NAME,\n",
    "                instructions=\"You are a comedian, you tell jokes\",\n",
    ")\n",
    "result = await joke_agent.run(\"Tell me a joke\")\n",
    "print(result.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c719f4",
   "metadata": {},
   "source": [
    "## Structured LLM Call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21d1cc1",
   "metadata": {},
   "source": [
    "We define the Pydantic model for the structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0016ac2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joke=\"Why don't scientists trust atoms? Because they make up everything!\"\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai import Agent\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class joke_output(BaseModel):\n",
    "    joke: str\n",
    "\n",
    "joke_agent = Agent(settings.MODEL_NAME,\n",
    "                instructions=\"You are a comedian, you tell jokes\",\n",
    "                output_type=joke_output\n",
    ")\n",
    "result = await joke_agent.run(\"Tell me a joke\")\n",
    "print(result.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af88d346",
   "metadata": {},
   "source": [
    "what if we want 4 jokes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e20204a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Why don't scientists trust atoms? Because they make up everything!\",\n",
       " \"Why don't scientists trust atoms? Because they make up everything!\",\n",
       " 'Why did the computer go to the doctor? Because it had a virus!',\n",
       " \"Why don't scientists trust atoms? Because they make up everything!\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokes_history = []\n",
    "for i in range(4):\n",
    "    result = await joke_agent.run(\"Tell me a joke\")\n",
    "    jokes_history.append(result.output.joke)\n",
    "\n",
    "jokes_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5cd2f5",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8894f806",
   "metadata": {},
   "source": [
    "Add the JokeDependencies to keep track of the jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0952887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joke=\"Why don't scientists trust atoms? Because they make up everything!\"\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from pydantic_ai import Agent, RunContext\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# We use dataclass because it allows us to add any object as a dependency, in this case a list of strings\n",
    "@dataclass\n",
    "class JokeDependencies:\n",
    "    previous_jokes: list[str]\n",
    "\n",
    "# Output defined by Pydantic\n",
    "class joke_output(BaseModel):\n",
    "    joke: str\n",
    "\n",
    "joke_agent = Agent(settings.MODEL_NAME,\n",
    "                instructions=\"You are a comedian. You are given a list of previous jokes. You need to tell a new joke that is not in the list\",\n",
    "                deps_type=JokeDependencies,\n",
    "                output_type=joke_output)\n",
    "\n",
    "@joke_agent.system_prompt  \n",
    "async def add_previous_jokes(ctx: RunContext[JokeDependencies]) -> str:\n",
    "    return f\"The previous jokes are {ctx.deps.previous_jokes}\"\n",
    "\n",
    "result = await joke_agent.run(\"Tell me a joke\", deps=JokeDependencies(previous_jokes=[]))\n",
    "print(result.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04a6ff2",
   "metadata": {},
   "source": [
    "Now, what if we want 4 jokes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40d1a678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Why don't scientists trust atoms? Because they make up everything!\",\n",
       " 'Why did the scarecrow win an award? Because he was outstanding in his field!',\n",
       " 'What do you call a fake noodle? An impasta!',\n",
       " 'What do you call a snowman with a six-pack? An abdominal snowman!']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokes_history = JokeDependencies(previous_jokes=[])\n",
    "for i in range(4):\n",
    "    result = await joke_agent.run(\"Tell me a joke\", deps=jokes_history)\n",
    "    jokes_history.previous_jokes.append(result.output.joke)\n",
    "\n",
    "jokes_history.previous_jokes\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acd6cc2",
   "metadata": {},
   "source": [
    "# Agent with Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7165dd5",
   "metadata": {},
   "source": [
    "Definition of tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a88c0e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_settings import BaseSettings\n",
    "from random import randint\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "# We use dataclass because it allows us to add any object as a dependency, in this case a list of strings\n",
    "@dataclass\n",
    "class JokeDependencies:\n",
    "    previous_jokes: list[str]\n",
    "\n",
    "# Output defined by Pydantic\n",
    "class joke_output(BaseModel):\n",
    "    joke: str\n",
    "\n",
    "joke_agent = Agent(settings.MODEL_NAME,\n",
    "                instructions=\"You are a comedian. You are given a list of previous jokes. You need to tell a new joke that is not in the list\",\n",
    "                deps_type=JokeDependencies,\n",
    "                output_type=joke_output)\n",
    "\n",
    "@joke_agent.system_prompt  \n",
    "async def add_previous_joke(ctx: RunContext[JokeDependencies]) -> str:\n",
    "    return f\"The previous jokes are {ctx.deps.previous_jokes}\"\n",
    "\n",
    "async def tell_a_joke(previous_jokes: list[str]=[]) -> str:\n",
    "    \"\"\"Creates a new joke, provide a list of previous jokes, if there are no previous jokes, just provide an empty list\"\"\"\n",
    "    result = await joke_agent.run(\"Tell me a joke\", deps=JokeDependencies(previous_jokes=previous_jokes))\n",
    "    return result.output.joke\n",
    "\n",
    "def random_number(min_val: int = 1, max_val: int = 100) -> int:\n",
    "    \"\"\"Generates a random number in a given range.\"\"\"\n",
    "    return randint(min_val, max_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98901928",
   "metadata": {},
   "source": [
    "## Agent that uses tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7362b1",
   "metadata": {},
   "source": [
    "We define the agent with the new output model and the tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df857ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserPromptNode(user_prompt='Please give me 3 random jokes and 5 random numbers. Then write a small poem about them.', instructions=None, instructions_functions=[], system_prompts=(), system_prompt_functions=[], system_prompt_dynamic_functions={})\n",
      "ModelRequestNode(request=ModelRequest(parts=[UserPromptPart(content='Please give me 3 random jokes and 5 random numbers. Then write a small poem about them.', timestamp=datetime.datetime(2025, 9, 3, 22, 19, 7, 348015, tzinfo=datetime.timezone.utc))]))\n",
      "CallToolsNode(model_response=ModelResponse(parts=[ToolCallPart(tool_name='tell_a_joke', args={'previous_jokes': []}, tool_call_id='pyd_ai_79f54daf28b44a87b269e00fab60beaa')], usage=RequestUsage(input_tokens=228, output_tokens=253, details={'thoughts_tokens': 235, 'text_prompt_tokens': 228}), model_name='gemini-2.5-flash', timestamp=datetime.datetime(2025, 9, 3, 22, 19, 9, 372284, tzinfo=datetime.timezone.utc), provider_name='google-gla', provider_details={'finish_reason': 'STOP'}, provider_response_id='3b64aPr_IP-vmtkPkdOSkA0'))\n",
      "ModelRequestNode(request=ModelRequest(parts=[ToolReturnPart(tool_name='tell_a_joke', content=\"Why don't scientists trust atoms? Because they make up everything!\", tool_call_id='pyd_ai_79f54daf28b44a87b269e00fab60beaa', timestamp=datetime.datetime(2025, 9, 3, 22, 19, 10, 450848, tzinfo=datetime.timezone.utc))]))\n",
      "CallToolsNode(model_response=ModelResponse(parts=[ToolCallPart(tool_name='tell_a_joke', args={'previous_jokes': [\"Why don't scientists trust atoms? Because they make up everything!\"]}, tool_call_id='pyd_ai_5d7d44bc5d824090b93c95384043789d')], usage=RequestUsage(input_tokens=278, output_tokens=108, details={'thoughts_tokens': 76, 'text_prompt_tokens': 278}), model_name='gemini-2.5-flash', timestamp=datetime.datetime(2025, 9, 3, 22, 19, 11, 489481, tzinfo=datetime.timezone.utc), provider_name='google-gla', provider_details={'finish_reason': 'STOP'}, provider_response_id='3764aKeXKLu1qtsP_eXBsA8'))\n",
      "ModelRequestNode(request=ModelRequest(parts=[ToolReturnPart(tool_name='tell_a_joke', content='I told my wife she was drawing her eyebrows too high. She looked surprised.', tool_call_id='pyd_ai_5d7d44bc5d824090b93c95384043789d', timestamp=datetime.datetime(2025, 9, 3, 22, 19, 12, 458035, tzinfo=datetime.timezone.utc))]))\n",
      "CallToolsNode(model_response=ModelResponse(parts=[ToolCallPart(tool_name='tell_a_joke', args={'previous_jokes': [\"Why don't scientists trust atoms? Because they make up everything!\", 'I told my wife she was drawing her eyebrows too high. She looked surprised.']}, tool_call_id='pyd_ai_709003257f514328ac8347ef0afd1f16')], usage=RequestUsage(input_tokens=344, output_tokens=117, details={'thoughts_tokens': 68, 'text_prompt_tokens': 344}), model_name='gemini-2.5-flash', timestamp=datetime.datetime(2025, 9, 3, 22, 19, 13, 461238, tzinfo=datetime.timezone.utc), provider_name='google-gla', provider_details={'finish_reason': 'STOP'}, provider_response_id='4b64aJrAJsPg8QHk47bwDw'))\n",
      "ModelRequestNode(request=ModelRequest(parts=[ToolReturnPart(tool_name='tell_a_joke', content='What do you call a fake noodle? An impasta!', tool_call_id='pyd_ai_709003257f514328ac8347ef0afd1f16', timestamp=datetime.datetime(2025, 9, 3, 22, 19, 14, 385407, tzinfo=datetime.timezone.utc))]))\n",
      "CallToolsNode(model_response=ModelResponse(parts=[ToolCallPart(tool_name='random_number', args={}, tool_call_id='pyd_ai_c99ddacdcaaa4b4d9cd27964555b110b')], usage=RequestUsage(input_tokens=423, output_tokens=106, details={'thoughts_tokens': 96, 'text_prompt_tokens': 423}), model_name='gemini-2.5-flash', timestamp=datetime.datetime(2025, 9, 3, 22, 19, 15, 429086, tzinfo=datetime.timezone.utc), provider_name='google-gla', provider_details={'finish_reason': 'STOP'}, provider_response_id='4764aKW2JMyCmtkPztGTgAM'))\n",
      "ModelRequestNode(request=ModelRequest(parts=[ToolReturnPart(tool_name='random_number', content=64, tool_call_id='pyd_ai_c99ddacdcaaa4b4d9cd27964555b110b', timestamp=datetime.datetime(2025, 9, 3, 22, 19, 15, 430180, tzinfo=datetime.timezone.utc))]))\n",
      "CallToolsNode(model_response=ModelResponse(parts=[ToolCallPart(tool_name='random_number', args={}, tool_call_id='pyd_ai_d55a60b4e89749e190e6169184bd08f3')], usage=RequestUsage(input_tokens=451, output_tokens=75, details={'thoughts_tokens': 65, 'text_prompt_tokens': 451}), model_name='gemini-2.5-flash', timestamp=datetime.datetime(2025, 9, 3, 22, 19, 16, 297730, tzinfo=datetime.timezone.utc), provider_name='google-gla', provider_details={'finish_reason': 'STOP'}, provider_response_id='5L64aOG8HPXzqtsPidWeuA8'))\n",
      "ModelRequestNode(request=ModelRequest(parts=[ToolReturnPart(tool_name='random_number', content=41, tool_call_id='pyd_ai_d55a60b4e89749e190e6169184bd08f3', timestamp=datetime.datetime(2025, 9, 3, 22, 19, 16, 298334, tzinfo=datetime.timezone.utc))]))\n",
      "CallToolsNode(model_response=ModelResponse(parts=[ToolCallPart(tool_name='random_number', args={}, tool_call_id='pyd_ai_f65f4dbbe118466da0ea03e3a6805768')], usage=RequestUsage(input_tokens=479, output_tokens=84, details={'thoughts_tokens': 74, 'text_prompt_tokens': 479}), model_name='gemini-2.5-flash', timestamp=datetime.datetime(2025, 9, 3, 22, 19, 17, 738784, tzinfo=datetime.timezone.utc), provider_name='google-gla', provider_details={'finish_reason': 'STOP'}, provider_response_id='5b64aJS5N-WrqtsPg6rt2Q8'))\n",
      "ModelRequestNode(request=ModelRequest(parts=[ToolReturnPart(tool_name='random_number', content=15, tool_call_id='pyd_ai_f65f4dbbe118466da0ea03e3a6805768', timestamp=datetime.datetime(2025, 9, 3, 22, 19, 17, 739910, tzinfo=datetime.timezone.utc))]))\n",
      "CallToolsNode(model_response=ModelResponse(parts=[ToolCallPart(tool_name='random_number', args={}, tool_call_id='pyd_ai_2bbfa5a3a2e94f9a86ddced80c71aeab')], usage=RequestUsage(input_tokens=507, output_tokens=73, details={'thoughts_tokens': 63, 'text_prompt_tokens': 507}), model_name='gemini-2.5-flash', timestamp=datetime.datetime(2025, 9, 3, 22, 19, 18, 750757, tzinfo=datetime.timezone.utc), provider_name='google-gla', provider_details={'finish_reason': 'STOP'}, provider_response_id='5r64aKyZOKDSqtsPh5fr6A8'))\n",
      "ModelRequestNode(request=ModelRequest(parts=[ToolReturnPart(tool_name='random_number', content=45, tool_call_id='pyd_ai_2bbfa5a3a2e94f9a86ddced80c71aeab', timestamp=datetime.datetime(2025, 9, 3, 22, 19, 18, 752819, tzinfo=datetime.timezone.utc))]))\n",
      "CallToolsNode(model_response=ModelResponse(parts=[ToolCallPart(tool_name='random_number', args={}, tool_call_id='pyd_ai_a65bdc3fa0964d1ea5b5c955ff4ef445')], usage=RequestUsage(input_tokens=535, output_tokens=79, details={'thoughts_tokens': 69, 'text_prompt_tokens': 535}), model_name='gemini-2.5-flash', timestamp=datetime.datetime(2025, 9, 3, 22, 19, 19, 953943, tzinfo=datetime.timezone.utc), provider_name='google-gla', provider_details={'finish_reason': 'STOP'}, provider_response_id='6L64aNa6B_ekmtkPoPWuqA0'))\n",
      "ModelRequestNode(request=ModelRequest(parts=[ToolReturnPart(tool_name='random_number', content=41, tool_call_id='pyd_ai_a65bdc3fa0964d1ea5b5c955ff4ef445', timestamp=datetime.datetime(2025, 9, 3, 22, 19, 19, 955158, tzinfo=datetime.timezone.utc))]))\n",
      "CallToolsNode(model_response=ModelResponse(parts=[ToolCallPart(tool_name='final_result', args={'poem': \"Three jokes I've shared, with laughter in mind,\\nOf atoms, high brows, and noodles you'll find.\\nThen numbers emerged, from a mystical hand,\\nSixty-four, forty-one, fifteen, across the land.\\nWith forty-five and forty-one, a pair anew,\\nA whimsical mix, for me and for you.\", 'jokes': [\"Why don't scientists trust atoms? Because they make up everything!\", 'I told my wife she was drawing her eyebrows too high. She looked surprised.', 'What do you call a fake noodle? An impasta!'], 'numbers': [64, 41, 15, 45, 41]}, tool_call_id='pyd_ai_e9dafe1439e84ee9899cad60f662a479')], usage=RequestUsage(input_tokens=563, output_tokens=435, details={'thoughts_tokens': 276, 'text_prompt_tokens': 563}), model_name='gemini-2.5-flash', timestamp=datetime.datetime(2025, 9, 3, 22, 19, 22, 492269, tzinfo=datetime.timezone.utc), provider_name='google-gla', provider_details={'finish_reason': 'STOP'}, provider_response_id='6r64aLiKKPP5qtsPo9WjsA0'))\n",
      "End(data=FinalResult(output=PoemOutput(jokes=[\"Why don't scientists trust atoms? Because they make up everything!\", 'I told my wife she was drawing her eyebrows too high. She looked surprised.', 'What do you call a fake noodle? An impasta!'], numbers=[64, 41, 15, 45, 41], poem=\"Three jokes I've shared, with laughter in mind,\\nOf atoms, high brows, and noodles you'll find.\\nThen numbers emerged, from a mystical hand,\\nSixty-four, forty-one, fifteen, across the land.\\nWith forty-five and forty-one, a pair anew,\\nA whimsical mix, for me and for you.\"), tool_name='final_result', tool_call_id='pyd_ai_e9dafe1439e84ee9899cad60f662a479'))\n"
     ]
    }
   ],
   "source": [
    "class PoemOutput(BaseModel):\n",
    "    jokes: list[str]\n",
    "    numbers: list[int]\n",
    "    poem: str\n",
    "\n",
    "agent = Agent(settings.MODEL_NAME, tools=[tell_a_joke, random_number], output_type=PoemOutput)\n",
    "\n",
    "# Run agent\n",
    "nodes = []\n",
    "async with agent.iter(\n",
    "    \"Please give me 3 random jokes and 5 random numbers. Then write a small poem about them.\",\n",
    ") as agent_run:\n",
    "    async for node in agent_run:\n",
    "        # Each node represents a step in the agent's execution\n",
    "        print(node)\n",
    "        nodes.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3469212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"jokes\": [\n",
      "    \"Why don't scientists trust atoms? Because they make up everything!\",\n",
      "    \"I told my wife she was drawing her eyebrows too high. She looked surprised.\",\n",
      "    \"What do you call a fake noodle? An impasta!\"\n",
      "  ],\n",
      "  \"numbers\": [\n",
      "    64,\n",
      "    41,\n",
      "    15,\n",
      "    45,\n",
      "    41\n",
      "  ],\n",
      "  \"poem\": \"Three jokes I've shared, with laughter in mind,\\nOf atoms, high brows, and noodles you'll find.\\nThen numbers emerged, from a mystical hand,\\nSixty-four, forty-one, fifteen, across the land.\\nWith forty-five and forty-one, a pair anew,\\nA whimsical mix, for me and for you.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(nodes[-1].data.output.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62110c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three jokes I've shared, with laughter in mind,\n",
      "Of atoms, high brows, and noodles you'll find.\n",
      "Then numbers emerged, from a mystical hand,\n",
      "Sixty-four, forty-one, fifteen, across the land.\n",
      "With forty-five and forty-one, a pair anew,\n",
      "A whimsical mix, for me and for you.\n"
     ]
    }
   ],
   "source": [
    "print(nodes[-1].data.output.poem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455c1109",
   "metadata": {},
   "source": [
    "# MCP Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072eeb24",
   "metadata": {},
   "source": [
    "## API MCPs: Context7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f61e040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserPromptNode(user_prompt='Give me the code for a simple agent that is connected to an MCP, using ', instructions='You are a helpful assistant that can answer questions about code, you use Context7 to search first for the library and then ask some questions to context7 about that library, based on the results, you give an answer to the user', instructions_functions=[], system_prompts=(), system_prompt_functions=[], system_prompt_dynamic_functions={})\n",
      "ModelRequestNode(request=ModelRequest(parts=[UserPromptPart(content='Give me the code for a simple agent that is connected to an MCP, using ', timestamp=datetime.datetime(2025, 9, 3, 22, 20, 33, 862527, tzinfo=datetime.timezone.utc))], instructions='You are a helpful assistant that can answer questions about code, you use Context7 to search first for the library and then ask some questions to context7 about that library, based on the results, you give an answer to the user'))\n",
      "CallToolsNode(model_response=ModelResponse(parts=[ToolCallPart(tool_name='resolve-library-id', args={'libraryName': 'MCP'}, tool_call_id='pyd_ai_e97f9103e3e9431da2e24cb7273d1741')], usage=RequestUsage(input_tokens=629, output_tokens=136, details={'thoughts_tokens': 118, 'text_prompt_tokens': 629}), model_name='gemini-2.5-flash', timestamp=datetime.datetime(2025, 9, 3, 22, 20, 36, 218419, tzinfo=datetime.timezone.utc), provider_name='google-gla', provider_details={'finish_reason': 'STOP'}, provider_response_id='NL-4aLGjF7a7qtsPzvuTwA8'))\n",
      "ModelRequestNode(request=ModelRequest(parts=[ToolReturnPart(tool_name='resolve-library-id', content=\"Available Libraries (top matches):\\n\\nEach result includes:\\n- Library ID: Context7-compatible identifier (format: /org/project)\\n- Name: Library or package name\\n- Description: Short summary\\n- Code Snippets: Number of available code examples\\n- Trust Score: Authority indicator\\n- Versions: List of versions if available. Use one of those versions if and only if the user explicitly provides a version in their query.\\n\\nFor best results, select libraries based on name match, trust score, snippet coverage, and relevance to your use case.\\n\\n----------\\n\\n- Title: Memento MCP\\n- Context7-compatible library ID: /gannonh/memento-mcp\\n- Description: Memento MCP is a scalable knowledge graph memory system for LLMs, offering semantic retrieval, contextual recall, and temporal awareness for persistent long-term memory.\\n- Code Snippets: 45\\n- Trust Score: 8.6\\n----------\\n- Title: MCP GitHub Projects\\n- Context7-compatible library ID: /taylor-lindores-reeves/mcp-github-projects\\n- Description: \\n- Code Snippets: 186\\n- Trust Score: 8.7\\n----------\\n- Title: MCP-Use\\n- Context7-compatible library ID: /mcp-use/mcp-use\\n- Description: MCP-Use is an open-source solution to connect any LLM to any MCP server, enabling the creation of custom MCP agents with tool access without relying on closed-source clients.\\n- Code Snippets: 435\\n- Trust Score: 5.6\\n----------\\n- Title: Memory Bank MCP\\n- Context7-compatible library ID: /movibe/memory-bank-mcp\\n- Description: Memory Bank MCP is a Model Context Protocol (MCP) server for managing Memory Banks, allowing AI assistants to store and retrieve information across sessions with features like file operations, progress tracking, and decision logging.\\n- Code Snippets: 406\\n- Trust Score: 8.5\\n----------\\n- Title: MCP HubSpot\\n- Context7-compatible library ID: /ajbmachon/mcp-hubspot\\n- Description: A Model Context Protocol (MCP) server that connects AI assistants to HubSpot CRM data, enabling direct interaction with contacts, companies, and engagements, with built-in vector storage and caching for improved performance.\\n- Code Snippets: 9\\n- Trust Score: 5\\n----------\\n- Title: MCP Link\\n- Context7-compatible library ID: /automation-ai-labs/mcp-link\\n- Description: MCP Link automatically converts any OpenAPI V3 API into an MCP Server, enabling seamless integration with AI Agent calling standards without code modification.\\n- Code Snippets: 3\\n- Trust Score: 4.5\\n----------\\n- Title: MCP Router\\n- Context7-compatible library ID: /mcp-router/mcp-router\\n- Description: MCP Router is a free desktop application for Windows and macOS that simplifies the management of Model Context Protocol (MCP) servers, offering secure, universal connectivity, and AI agent features.\\n- Code Snippets: 65\\n- Trust Score: 5.2\\n----------\\n- Title: Playwright MCP\\n- Context7-compatible library ID: /microsoft/playwright-mcp\\n- Description: A Model Context Protocol (MCP) server that provides browser automation capabilities using Playwright, enabling LLMs to interact with web pages through structured accessibility snapshots without needing vision models.\\n- Code Snippets: 23\\n- Trust Score: 9.9\\n----------\\n- Title: Windows MCP\\n- Context7-compatible library ID: /cursortouch/windows-mcp\\n- Description: Windows MCP is a lightweight, open-source project that enables seamless integration between AI agents and the Windows operating system, allowing agents to perform tasks like file navigation, application control, and UI interaction.\\n- Code Snippets: 6\\n- Trust Score: 6.8\\n----------\\n- Title: MCP Trader\\n- Context7-compatible library ID: /wshobson/mcp-trader\\n- Description: A simplified Model Context Protocol (MCP) server for stock and cryptocurrency analysis, featuring technical analysis tools and FastMCP resources for direct market data access.\\n- Code Snippets: 24\\n- Trust Score: 9.5\\n----------\\n- Title: MCP Client\\n- Context7-compatible library ID: /punkpeye/mcp-client\\n- Description: A Node.js client for the Model Context Protocol (MCP) that simplifies interactions by abstracting lower-level details and providing a more convenient API.\\n- Code Snippets: 15\\n- Trust Score: 9.6\\n----------\\n- Title: Magic MCP\\n- Context7-compatible library ID: /21st-dev/magic-mcp\\n- Description: Magic Component Platform (MCP) is an AI-driven tool that instantly generates UI components from natural language descriptions, integrating seamlessly with popular IDEs for streamlined UI development.\\n- Code Snippets: 8\\n- Trust Score: 6.5\\n----------\\n- Title: Postgres MCP Pro\\n- Context7-compatible library ID: /crystaldba/postgres-mcp\\n- Description: Postgres MCP Pro is an open-source Model Context Protocol server that enhances PostgreSQL development with features like index tuning, query plan analysis, health checks, and safe SQL execution.\\n- Code Snippets: 15\\n- Trust Score: 5.9\\n----------\\n- Title: MCP Framework\\n- Context7-compatible library ID: /websites/mcp-framework\\n- Description: MCP Framework is a powerful tool for building and managing MCP (Model Context Protocol) servers, enabling easy creation of custom tools, resources, and prompts for AI models with features like authentication and multiple transports.\\n- Code Snippets: 120\\n- Trust Score: 7.5\\n----------\\n- Title: MCP\\n- Context7-compatible library ID: /m-yoshiro/storybook-mcp\\n- Description: A custom Model Context Protocol (MCP) server that integrates with Storybook to support UI implementation from Figma designs, enabling AI tools to query components and assist non-developers in frontend workflows.\\n- Code Snippets: 20\\n- Trust Score: 7.1\\n----------\\n- Title: 1MCP Agent\\n- Context7-compatible library ID: /1mcp-app/agent\\n- Description: 1MCP (One MCP) is a unified Model Context Protocol server that aggregates multiple MCP servers into one, simplifying AI assistant configuration and reducing resource usage.\\n- Code Snippets: 549\\n- Trust Score: 3.7\\n----------\\n- Title: monday.com MCP\\n- Context7-compatible library ID: /mondaycom/mcp\\n- Description: monday.com's open framework for connecting AI agents into your work OS, providing secure access to structured data and tools for reliable operation in real workflows.\\n- Code Snippets: 19\\n- Trust Score: 8.1\\n----------\\n- Title: Browser MCP\\n- Context7-compatible library ID: /websites/docs_browsermcp_io-welcome\\n- Description: Browser MCP is an MCP server designed to automate browser interactions. It allows AI applications to navigate the web, fill out forms, and perform other browser-based tasks.\\n- Code Snippets: 2\\n- Trust Score: 7.5\\n----------\\n- Title: MCP Unity\\n- Context7-compatible library ID: /codergamester/mcp-unity\\n- Description: MCP Unity is a Unity Editor implementation of the Model Context Protocol, enabling AI assistants to interact with Unity projects via a Node.js server.\\n- Code Snippets: 689\\n- Trust Score: 8.7\\n----------\\n- Title: MCP\\n- Context7-compatible library ID: /findleyr/mcp\\n- Description: \\n- Code Snippets: 68\\n- Trust Score: 5.3\\n----------\\n- Title: Grok MCP\\n- Context7-compatible library ID: /bob-lance/grok-mcp\\n- Description: A Model Context Protocol (MCP) plugin that provides seamless access to Grok AI's chat completion, image understanding, and function calling capabilities directly from Cline.\\n- Code Snippets: 13\\n- Trust Score: 4.3\\n----------\\n- Title: Unity MCP\\n- Context7-compatible library ID: /justinpbarnett/unity-mcp\\n- Description: Unity MCP enables AI assistants to interact directly with the Unity Editor, allowing LLMs to manage assets, control scenes, edit scripts, and automate tasks via a local MCP client.\\n- Code Snippets: 7\\n- Trust Score: 9.2\\n----------\\n- Title: Browser MCP\\n- Context7-compatible library ID: /websites/docs_browsermcp_io\\n- Description: Browser MCP is an MCP server that enables AI applications to automate web browsers, allowing them to navigate the web, fill out forms, and interact with online services.\\n- Code Snippets: 2\\n- Trust Score: 7.5\\n----------\\n- Title: Tableau MCP\\n- Context7-compatible library ID: /tableau/tableau-mcp\\n- Description: Tableau MCP is a suite of developer primitives, including tools, resources, and prompts, designed to simplify the creation of AI applications that integrate with Tableau.\\n- Code Snippets: 16\\n- Trust Score: 8.4\\n----------\\n- Title: Docker MCP Gateway\\n- Context7-compatible library ID: /docker/mcp-gateway\\n- Description: The Docker MCP Gateway is a CLI plugin that facilitates the secure running and deployment of MCP servers, enabling AI applications to connect to external data sources and tools.\\n- Code Snippets: 1206\\n- Trust Score: 9.9\\n----------\\n- Title: MCP Server\\n- Context7-compatible library ID: /echelon-ai-labs/servicenow-mcp\\n- Description: A Model Completion Protocol (MCP) server implementation for ServiceNow, enabling Claude to interact with ServiceNow instances by retrieving data and performing actions via its API.\\n- Code Snippets: 137\\n- Trust Score: 6\\n----------\\n- Title: Unity MCP\\n- Context7-compatible library ID: /ivanmurzak/unity-mcp\\n- Description: Unity MCP is a bridge between LLM and Unity, exposing and explaining Unity's tools to LLMs for them to utilize as requested by users.\\n- Code Snippets: 42\\n- Trust Score: 9.3\\n----------\\n- Title: MCP Terminal\\n- Context7-compatible library ID: /sichang824/mcp-terminal\\n- Description: MCP Terminal is a terminal control server based on the Model Context Protocol (MCP), designed for integration with Large Language Models (LLMs) and AI assistants, enabling AI to execute terminal commands and retrieve output.\\n- Code Snippets: 10\\n----------\\n- Title: MCP Toolbox for Databases\\n- Context7-compatible library ID: /googleapis/genai-toolbox\\n- Description: MCP Toolbox for Databases is an open-source MCP server that simplifies the development of AI tools for databases by handling complexities like connection pooling, authentication, and observability.\\n- Code Snippets: 588\\n- Trust Score: 8.5\\n----------\\n- Title: MCP Tools\\n- Context7-compatible library ID: /clerk/mcp-tools\\n- Description: A library built on the MCP Typescript SDK to simplify implementing MCP with authentication in your AI application's client or server.\\n- Code Snippets: 46\\n- Trust Score: 8.4\", tool_call_id='pyd_ai_e97f9103e3e9431da2e24cb7273d1741', timestamp=datetime.datetime(2025, 9, 3, 22, 20, 37, 574325, tzinfo=datetime.timezone.utc))], instructions='You are a helpful assistant that can answer questions about code, you use Context7 to search first for the library and then ask some questions to context7 about that library, based on the results, you give an answer to the user'))\n",
      "CallToolsNode(model_response=ModelResponse(parts=[ToolCallPart(tool_name='get-library-docs', args={'topic': 'simple agent', 'context7CompatibleLibraryID': '/mcp-use/mcp-use'}, tool_call_id='pyd_ai_97d3aa46303145d8831b23cb3b06eb6c')], usage=RequestUsage(input_tokens=3330, cache_read_tokens=907, output_tokens=289, details={'cached_content_tokens': 907, 'thoughts_tokens': 254, 'text_prompt_tokens': 3330, 'text_cache_tokens': 907}), model_name='gemini-2.5-flash', timestamp=datetime.datetime(2025, 9, 3, 22, 20, 39, 865053, tzinfo=datetime.timezone.utc), provider_name='google-gla', provider_details={'finish_reason': 'STOP'}, provider_response_id='OL-4aP7rAdOwqtsPjfrAuA0'))\n",
      "ModelRequestNode(request=ModelRequest(parts=[ToolReturnPart(tool_name='get-library-docs', content='========================\\nCODE SNIPPETS\\n========================\\nTITLE: Stream Agent Output with Basic `astream` Method (Python)\\nDESCRIPTION: Demonstrates how to use the `astream` method of `MCPAgent` to receive real-time, incremental output from an agent. This method provides a simple way to get the agent\\'s response as it is generated, suitable for displaying continuous output.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/guides/streaming.mdx#_snippet_0\\n\\nLANGUAGE: python\\nCODE:\\n```\\nimport asyncio\\nfrom langchain_openai import ChatOpenAI\\nfrom mcp_use import MCPAgent, MCPClient\\n\\nasync def basic_streaming_example():\\n    # Setup agent\\n    config = {\\n        \"mcpServers\": {\\n            \"playwright\": {\\n                \"command\": \"npx\",\\n                \"args\": [\"@playwright/mcp@latest\"]\\n            }\\n        }\\n    }\\n\\n    client = MCPClient.from_dict(config)\\n    llm = ChatOpenAI(model=\"gpt-4\")\\n    agent = MCPAgent(llm=llm, client=client)\\n\\n    # Stream the agent\\'s response\\n    print(\"Agent is working...\")\\n    async for chunk in agent.astream(\"Search for the latest Python news and summarize it\"):\\n        print(chunk, end=\"\", flush=True)\\n\\n    print(\"\\\\n\\\\nDone!\")\\n\\nif __name__ == \"__main__\":\\n    asyncio.run(basic_streaming_example())\\n```\\n\\n----------------------------------------\\n\\nTITLE: Dynamically Update Disallowed Tools\\nDESCRIPTION: Demonstrates how to change the disallowed tools for an existing MCPAgent instance and then reinitialize the agent to apply these changes. This allows for dynamic adjustment of agent capabilities.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/agent/agent-configuration.mdx#_snippet_5\\n\\nLANGUAGE: python\\nCODE:\\n```\\n# Update restrictions after initialization\\nagent.set_disallowed_tools([\"file_system\", \"network\", \"shell\", \"database\"])\\nawait agent.initialize()  # Reinitialize to apply changes\\n```\\n\\n----------------------------------------\\n\\nTITLE: LangChain Custom Agent Example\\nDESCRIPTION: Demonstrates how to initialize MCPClient, create a LangChainAdapter, convert MCP tools to LangChain tools, set up a LangChain agent with a prompt and language model, and execute the agent. This example showcases the simplified tool creation process provided by the adapter.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/advanced/building-custom-agents.mdx#_snippet_0\\n\\nLANGUAGE: python\\nCODE:\\n```\\nimport asyncio\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent\\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\\n\\nfrom mcp_use.client import MCPClient\\nfrom mcp_use.adapters import LangChainAdapter\\n\\nasync def main():\\n    # Initialize the MCP client\\n    client = MCPClient.from_config_file(\"path/to/config.json\")\\n\\n    # Create adapter instance\\n    adapter = LangChainAdapter()\\n\\n    # Get LangChain tools directly from the client with a single line\\n    tools = await adapter.create_tools(client)\\n\\n    # Initialize your language model\\n    llm = ChatOpenAI(model=\"gpt-4o\")\\n\\n    # Create a prompt template\\n    prompt = ChatPromptTemplate.from_messages([\\n        (\"system\", \"You are a helpful assistant with access to powerful tools.\"),\\n        MessagesPlaceholder(variable_name=\"chat_history\"),\\n        (\"human\", \"{input}\"),\\n        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\\n    ])\\n\\n    # Create the agent\\n    agent = create_tool_calling_agent(llm=llm, tools=tools, prompt=prompt)\\n\\n    # Create the agent executor\\n    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\\n\\n    # Run the agent\\n    result = await agent_executor.ainvoke({\"input\": \"What can you do?\"})\\n    print(result[\"output\"])\\n\\nif __name__ == \"__main__\":\\n    asyncio.run(main())\\n```\\n\\n----------------------------------------\\n\\nTITLE: Creating a LangChain Agent with MCP Tools\\nDESCRIPTION: This snippet demonstrates how to initialize MCPClient, use LangChainAdapter to convert MCP tools into LangChain-compatible tools, and then integrate them into a langchain_openai agent. It shows the setup of the LLM, prompt, agent, and executor for running a query, illustrating a complete custom agent workflow.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/building-custom-agents.mdx#_snippet_0\\n\\nLANGUAGE: python\\nCODE:\\n```\\nimport asyncio\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent\\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\\n\\nfrom mcp_use.client import MCPClient\\nfrom mcp_use.adapters import LangChainAdapter\\n\\nasync def main():\\n    # Initialize the MCP client\\n    client = MCPClient.from_config_file(\"path/to/config.json\")\\n\\n    # Create adapter instance\\n    adapter = LangChainAdapter()\\n\\n    # Get LangChain tools directly from the client with a single line\\n    tools = await adapter.create_tools(client)\\n\\n    # Initialize your language model\\n    llm = ChatOpenAI(model=\"gpt-4o\")\\n\\n    # Create a prompt template\\n    prompt = ChatPromptTemplate.from_messages([\\n        (\"system\", \"You are a helpful assistant with access to powerful tools.\"),\\n        MessagesPlaceholder(variable_name=\"chat_history\"),\\n        (\"human\", \"{input}\"),\\n        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\\n    ])\\n\\n    # Create the agent\\n    agent = create_tool_calling_agent(llm=llm, tools=tools, prompt=prompt)\\n\\n    # Create the agent executor\\n    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\\n\\n    # Run the agent\\n    result = await agent_executor.ainvoke({\"input\": \"What can you do?\"})\\n    print(result[\"output\"])\\n\\nif __name__ == \"__main__\":\\n    asyncio.run(main())\\n```\\n\\n----------------------------------------\\n\\nTITLE: Basic MCPAgent Initialization\\nDESCRIPTION: Initializes an MCPAgent with a specified LangChain LLM and MCPClient, setting a maximum number of steps for the agent\\'s execution. This is a fundamental setup for agent operation.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/agent/agent-configuration.mdx#_snippet_2\\n\\nLANGUAGE: python\\nCODE:\\n```\\nfrom mcp_use import MCPAgent, MCPClient\\nfrom langchain_openai import ChatOpenAI\\n\\nagent = MCPAgent(\\n    llm=ChatOpenAI(model=\"gpt-4o\", temperature=0.7),\\n    client=MCPClient.from_config_file(\"config.json\"),\\n    max_steps=30\\n)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Creating a Custom Agent with LangChain Adapter\\nDESCRIPTION: Demonstrates how to initialize MCPClient, create a LangChainAdapter, convert MCP tools to LangChain tools, and integrate them into a LangChain agent executor using ChatOpenAI and a custom prompt template.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/guides/building-custom-agents.mdx#_snippet_0\\n\\nLANGUAGE: python\\nCODE:\\n```\\nimport asyncio\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent\\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\\n\\nfrom mcp_use.client import MCPClient\\nfrom mcp_use.adapters import LangChainAdapter\\n\\nasync def main():\\n    # Initialize the MCP client\\n    client = MCPClient.from_config_file(\"path/to/config.json\")\\n\\n    # Create adapter instance\\n    adapter = LangChainAdapter()\\n\\n    # Get LangChain tools directly from the client with a single line\\n    tools = await adapter.create_tools(client)\\n\\n    # Initialize your language model\\n    llm = ChatOpenAI(model=\"gpt-4o\")\\n\\n    # Create a prompt template\\n    prompt = ChatPromptTemplate.from_messages([\\n        (\"system\", \"You are a helpful assistant with access to powerful tools.\"),\\n        MessagesPlaceholder(variable_name=\"chat_history\"),\\n        (\"human\", \"{input}\"),\\n        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\\n    ])\\n\\n    # Create the agent\\n    agent = create_tool_calling_agent(llm=llm, tools=tools, prompt=prompt)\\n\\n    # Create the agent executor\\n    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\\n\\n    # Run the agent\\n    result = await agent_executor.ainvoke({\"input\": \"What can you do?\"})\\n    print(result[\"output\"])\\n\\nif __name__ == \"__main__\":\\n    asyncio.run(main())\\n```\\n\\n----------------------------------------\\n\\nTITLE: Basic Example: Get Structured Weather Data\\nDESCRIPTION: Demonstrates how to use MCPAgent to retrieve weather information and parse it into a Pydantic model. It includes setting up the agent, defining a Pydantic schema, running the agent with the schema, and printing the structured output.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/agent/structured-output.mdx#_snippet_0\\n\\nLANGUAGE: python\\nCODE:\\n```\\nimport asyncio\\nfrom pydantic import BaseModel, Field\\nfrom langchain_openai import ChatOpenAI\\nfrom mcp_use import MCPAgent, MCPClient\\n\\nclass WeatherInfo(BaseModel):\\n    \"\"\"Weather information for a location\"\"\"\\n    city: str = Field(description=\"City name\")\\n    temperature: float = Field(description=\"Temperature in Celsius\")\\n    condition: str = Field(description=\"Weather condition\")\\n    humidity: int = Field(description=\"Humidity percentage\")\\n\\nasync def main():\\n    # Setup client and agent\\n    client = MCPClient(config={\"mcpServers\": {...}})\\n    llm = ChatOpenAI(model=\"gpt-4o\")\\n    agent = MCPAgent(llm=llm, client=client)\\n\\n    # Get structured output\\n    weather: WeatherInfo = await agent.run(\\n        \"Get the current weather in San Francisco\",\\n        output_schema=WeatherInfo\\n    )\\n\\n    print(f\"Temperature in {weather.city}: {weather.temperature}°C\")\\n    print(f\"Condition: {weather.condition}\")\\n    print(f\"Humidity: {weather.humidity}%\")\\n\\nasyncio.run(main())\\n```\\n\\n----------------------------------------\\n\\nTITLE: Step-by-Step Agent Streaming\\nDESCRIPTION: Demonstrates how to stream agent execution step-by-step, showing tool calls and their results. This is useful for tracking progress and processing intermediate actions.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/agent/streaming.mdx#_snippet_0\\n\\nLANGUAGE: python\\nCODE:\\n```\\nimport asyncio\\nfrom langchain_openai import ChatOpenAI\\nfrom mcp_use import MCPAgent, MCPClient\\n\\nasync def step_streaming_example():\\n    # Setup agent\\n    config = {\\n        \"mcpServers\": {\\n            \"playwright\": {\\n                \"command\": \"npx\",\\n                \"args\": [\"@playwright/mcp@latest\"]\\n            }\\n        }\\n    }\\n\\n    client = MCPClient.from_dict(config)\\n    llm = ChatOpenAI(model=\"gpt-4\")\\n    agent = MCPAgent(llm=llm, client=client)\\n\\n    # Stream the agent\\'s steps\\n    print(\"🤖 Agent is working...\")\\n    print(\"-\" * 50)\\n\\n    async for item in agent.stream(\"Search for the latest Python news and summarize it\"):\\n        if isinstance(item, str):\\n            # Final result\\n            print(f\"\\\\n✅ Final Result:\\\\n{item}\")\\n        else:\\n            # Intermediate step (action, observation)\\n            action, observation = item\\n            print(f\"\\\\n🔧 Tool: {action.tool}\")\\n            print(f\"📝 Input: {action.tool_input}\")\\n            print(f\"📄 Result: {observation[:100]}{\\'...\\' if len(observation) > 100 else \\'\\'}\")\\n\\n    print(\"\\\\n🎉 Done!\")\\n\\nif __name__ == \"__main__\":\\n    asyncio.run(step_streaming_example())\\n```\\n\\n----------------------------------------\\n\\nTITLE: Initialization: Manual Initialization\\nDESCRIPTION: Disables auto-initialization, allowing for manual control over when the agent is initialized. This is useful for scenarios requiring more explicit control over the agent\\'s lifecycle.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/agent/agent-configuration.mdx#_snippet_20\\n\\nLANGUAGE: python\\nCODE:\\n```\\n# Manual initialization for more control\\nagent = MCPAgent(\\n    llm=llm,\\n    client=client,\\n    auto_initialize=False\\n)\\n\\n# Initialize manually when ready\\nawait agent.initialize()\\n```\\n\\n----------------------------------------\\n\\nTITLE: Check Current Restrictions\\nDESCRIPTION: Retrieves and prints the list of currently disallowed tools for the agent. This is useful for understanding and managing agent capabilities.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/agent/agent-configuration.mdx#_snippet_6\\n\\nLANGUAGE: python\\nCODE:\\n```\\nrestricted_tools = agent.get_disallowed_tools()\\nprint(f\"Restricted tools: {restricted_tools}\")\\n```\\n\\n----------------------------------------\\n\\nTITLE: Process Agent Events Incrementally to Avoid Memory Issues\\nDESCRIPTION: Addresses potential memory issues with long streams by demonstrating how to process agent events incrementally. This approach avoids storing all events in memory, preventing memory exhaustion and ensuring efficient handling of continuous data.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/guides/streaming.mdx#_snippet_9\\n\\nLANGUAGE: python\\nCODE:\\n```\\n# Don\\'t store all events in memory\\nasync for event in agent.astream_events(query, version=\"v1\"):\\n    process_event_immediately(event)\\n    # Don\\'t append to a list\\n```\\n\\n----------------------------------------\\n\\nTITLE: Streaming Agent Output with astream in Python\\nDESCRIPTION: Demonstrates how to use the `astream` method of an agent object to asynchronously iterate over streaming output chunks. Each chunk is a dictionary providing incremental results, actions, steps, and the final output.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/introduction.mdx#_snippet_0\\n\\nLANGUAGE: python\\nCODE:\\n```\\nasync for chunk in agent.astream(\"your query here\"):\\n    print(chunk[\"messages\"], end=\"\", flush=True)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Implement Real-time Agent Progress Tracking with Python\\nDESCRIPTION: This Python code defines a `ProgressTracker` class to monitor an agent\\'s execution, updating on tool starts and displaying a summary at the end. It integrates with `MCPAgent` and `ChatOpenAI` to capture `on_tool_start`, `on_chat_model_stream`, and `on_chain_end` events, providing real-time feedback on the agent\\'s actions and overall performance.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/guides/streaming.mdx#_snippet_2\\n\\nLANGUAGE: python\\nCODE:\\n```\\nimport asyncio\\nimport time\\nfrom langchain_openai import ChatOpenAI\\nfrom mcp_use import MCPAgent, MCPClient\\n\\nclass ProgressTracker:\\n    def __init__(self):\\n        self.start_time = time.time()\\n        self.current_tool = None\\n        self.step_count = 0\\n        self.tools_used = []\\n\\n    def update_tool(self, tool_name):\\n        if tool_name != self.current_tool:\\n            self.current_tool = tool_name\\n            self.step_count += 1\\n            self.tools_used.append(tool_name)\\n            elapsed = time.time() - self.start_time\\n            print(f\"\\\\n[{elapsed:.1f}s] Step {self.step_count}: Using {tool_name}\")\\n\\n    def show_summary(self):\\n        elapsed = time.time() - self.start_time\\n        print(f\"\\\\n{\\'=\\'*50}\")\\n        print(f\"Task completed in {elapsed:.1f} seconds\")\\n        print(f\"Steps taken: {self.step_count}\")\\n        print(f\"Tools used: {\\', \\'.join(set(self.tools_used))}\")\\n        print(f\"{\\'=\\'*50}\")\\n\\nasync def streaming_with_progress():\\n    config = {\\n        \"mcpServers\": {\\n            \"playwright\": {\\n                \"command\": \"npx\",\\n                \"args\": [\"@playwright/mcp@latest\"]\\n            }\\n        }\\n    }\\n\\n    client = MCPClient.from_dict(config)\\n    llm = ChatOpenAI(model=\"gpt-4\", streaming=True)\\n    agent = MCPAgent(llm=llm, client=client)\\n\\n    tracker = ProgressTracker()\\n\\n    query = \"Find information about the latest iPhone model and its key features\"\\n\\n    print(\"🚀 Starting agent with real-time progress tracking...\")\\n\\n    async for event in agent.astream_events(query, version=\"v1\"):\\n        event_type = event.get(\"event\")\\n        data = event.get(\"data\", {})\\n\\n        if event_type == \"on_tool_start\":\\n            tool_name = data.get(\"input\", {}).get(\"tool_name\")\\n            if tool_name:\\n                tracker.update_tool(tool_name)\\n\\n        elif event_type == \"on_chat_model_stream\":\\n            chunk = data.get(\"chunk\", {})\\n            if hasattr(chunk, \\'content\\') and chunk.content:\\n                print(chunk.content, end=\"\", flush=True)\\n\\n        elif event_type == \"on_chain_end\":\\n            tracker.show_summary()\\n\\nif __name__ == \"__main__\":\\n    asyncio.run(streaming_with_progress())\\n```\\n\\n----------------------------------------\\n\\nTITLE: Custom System Prompt\\nDESCRIPTION: Sets a custom system prompt for the agent, defining its persona and behavior. This prompt guides the agent\\'s responses and task execution.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/agent/agent-configuration.mdx#_snippet_11\\n\\nLANGUAGE: python\\nCODE:\\n```\\ncustom_prompt = \"\"\"\\nYou are a helpful assistant specialized in data analysis.\\nAlways provide detailed explanations for your reasoning.\\nWhen working with data, prioritize accuracy over speed.\\n\"\"\"\\n\\nagent = MCPAgent(\\n    llm=llm,\\n    client=client,\\n    system_prompt=custom_prompt\\n)\\n```\\n\\n----------------------------------------\\n\\nTITLE: LangChain Integration with MCP Adapters\\nDESCRIPTION: Demonstrates how to use the LangChainAdapter to create tools from an MCPClient and integrate them with a LangChain agent. It covers initializing the client, creating the adapter, generating tools, setting up a LangChain prompt and agent, and executing a query.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/agent/agent-configuration.mdx#_snippet_7\\n\\nLANGUAGE: python\\nCODE:\\n```\\nimport asyncio\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent\\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\\n\\nfrom mcp_use.client import MCPClient\\nfrom mcp_use.adapters import LangChainAdapter\\n\\nasync def main():\\n    # Initialize client\\n    client = MCPClient.from_config_file(\"browser_mcp.json\")\\n\\n    # Create an adapter instance\\n    adapter = LangChainAdapter()\\n\\n    # Get tools directly from the client\\n    tools = await adapter.create_tools(client)\\n\\n    # Use the tools with any LangChain agent\\n    llm = ChatOpenAI(model=\"gpt-4o\")\\n    prompt = ChatPromptTemplate.from_messages([\\n        (\"system\", \"You are a helpful assistant with access to powerful tools.\"),\\n        MessagesPlaceholder(variable_name=\"chat_history\"),\\n        (\"human\", \"{input}\"),\\n        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\\n    ])\\n\\n    agent = create_tool_calling_agent(llm=llm, tools=tools, prompt=prompt)\\n    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\\n\\n    result = await agent_executor.ainvoke({\"input\": \"Search for information about climate change\"})\\n    print(result[\"output\"])\\n\\nif __name__ == \"__main__\":\\n    asyncio.run(main())\\n```\\n\\n----------------------------------------\\n\\nTITLE: Additional Instructions\\nDESCRIPTION: Adds task-specific instructions to the agent without replacing the base system prompt. This allows for dynamic adjustments to the agent\\'s focus.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/agent/agent-configuration.mdx#_snippet_12\\n\\nLANGUAGE: python\\nCODE:\\n```\\nagent = MCPAgent(\\n    llm=llm,\\n    client=client,\\n    additional_instructions=\"Focus on finding recent information from the last 6 months.\"\\n)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Dynamically Update MCPAgent Tool Restrictions\\nDESCRIPTION: Illustrates how to change the list of disallowed tools for an MCPAgent after its initial creation and then reinitialize the agent to apply these new restrictions.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/essentials/agent-configuration.mdx#_snippet_4\\n\\nLANGUAGE: python\\nCODE:\\n```\\n# Update restrictions after initialization\\nagent.set_disallowed_tools([\"file_system\", \"network\", \"shell\", \"database\"])\\nawait agent.initialize()  # Reinitialize to apply changes\\n```\\n\\n----------------------------------------\\n\\nTITLE: Debugging: Enable Verbose Logging\\nDESCRIPTION: Enables verbose logging and debugging features for the agent, which is highly recommended during development to understand the agent\\'s internal processes.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/agent/agent-configuration.mdx#_snippet_17\\n\\nLANGUAGE: python\\nCODE:\\n```\\n# Enable verbose logging\\nagent = MCPAgent(\\n    llm=llm,\\n    client=client,\\n    verbose=True,\\n    debug=True\\n)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Stream Agent Output with FastAPI using Server-Sent Events\\nDESCRIPTION: Demonstrates how to build a FastAPI application to stream real-time agent output using Server-Sent Events (SSE). It includes setting up an MCPAgent with ChatOpenAI for streaming, and an endpoint that asynchronously generates and formats agent events as SSE data.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/guides/streaming.mdx#_snippet_4\\n\\nLANGUAGE: python\\nCODE:\\n```\\nfrom fastapi import FastAPI\\nfrom fastapi.responses import StreamingResponse\\nimport asyncio\\nimport json\\nfrom langchain_openai import ChatOpenAI\\nfrom mcp_use import MCPAgent, MCPClient\\n\\napp = FastAPI()\\n\\nasync def create_agent():\\n    config = {\\n        \"mcpServers\": {\\n            \"playwright\": {\\n                \"command\": \"npx\",\\n                \"args\": [\"@playwright/mcp@latest\"]\\n            }\\n        }\\n    }\\n    client = MCPClient.from_dict(config)\\n    llm = ChatOpenAI(model=\"gpt-4\", streaming=True)\\n    return MCPAgent(llm=llm, client=client)\\n\\n@app.get(\"/stream/{query}\")\\nasync def stream_agent_response(query: str):\\n    \"\"\"Stream agent response using Server-Sent Events\"\"\"\\n\\n    async def event_generator():\\n        agent = await create_agent()\\n\\n        async for event in agent.astream_events(query, version=\"v1\"):\\n            event_type = event.get(\"event\")\\n            data = event.get(\"data\", {})\\n\\n            # Format as SSE\\n            sse_data = {\\n                \"type\": event_type,\\n                \"timestamp\": time.time(),\\n                \"data\": data\\n            }\\n\\n            yield f\"data: {json.dumps(sse_data)}\\\\n\\\\n\"\\n\\n        yield \"data: [DONE]\\\\n\\\\n\"\\n\\n    return StreamingResponse(\\n        event_generator(),\\n        media_type=\"text/plain\",\\n        headers={\\n            \"Cache-Control\": \"no-cache\",\\n            \"Connection\": \"keep-alive\"\\n        }\\n    )\\n\\n@app.get(\"/\")\\nasync def root():\\n    return {\"message\": \"MCP Agent Streaming API\"}\\n\\nif __name__ == \"__main__\":\\n    import uvicorn\\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Initialization: Auto-Initialize\\nDESCRIPTION: Configures the agent to automatically initialize upon creation. This simplifies the setup process by performing initialization tasks immediately.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/agent/agent-configuration.mdx#_snippet_19\\n\\nLANGUAGE: python\\nCODE:\\n```\\n# Auto-initialize on creation\\nagent = MCPAgent(\\n    llm=llm,\\n    client=client,\\n    auto_initialize=True\\n)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Stream Agent Output\\nDESCRIPTION: Demonstrates how to stream agent responses chunk by chunk as they are generated, providing real-time feedback to the user.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/getting-started/quickstart.mdx#_snippet_7\\n\\nLANGUAGE: python\\nCODE:\\n```\\nasync for chunk in agent.astream(\"your query here\"):\\n    print(chunk, end=\"\", flush=True)\\n```\\n\\n----------------------------------------\\n\\nTITLE: LangChain Agent Integration Example\\nDESCRIPTION: Demonstrates how to integrate MCP tools with a LangChain agent. This example shows the setup for creating an agent executor using LangChain\\'s tool calling capabilities, OpenAI\\'s chat models, and a custom prompt template.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/api-reference/adapters.mdx#_snippet_14\\n\\nLANGUAGE: python\\nCODE:\\n```\\nfrom langchain.agents import create_tool_calling_agent, AgentExecutor\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain_openai import ChatOpenAI\\n\\nasync def create_langchain_agent():\\n    # Create MCP client and adapter\\n    client = MCPClient.from_config_file(\"config.json\")\\n    tools = await LangChainAdapter.create_tools(client)\\n\\n    # Create LLM and prompt\\n    llm = ChatOpenAI(model=\"gpt-4\")\\n    prompt = ChatPromptTemplate.from_messages([\\n        (\"system\", \"You are a helpful assistant with access to tools.\"),\\n        (\"human\", \"{input}\"),\\n        (\"placeholder\", \"{agent_scratchpad}\"),\\n    ])\\n\\n    # Create agent\\n    agent = create_tool_calling_agent(llm, tools, prompt)\\n    agent_executor = AgentExecutor(agent=agent, tools=tools)\\n\\n    return agent_executor\\n\\n# Usage\\nagent = await create_langchain_agent()\\nresult = await agent.ainvoke({\"input\": \"Read the contents of README.md\"})\\n```\\n\\n----------------------------------------\\n\\nTITLE: Buffer Agent Streaming Output for Performance\\nDESCRIPTION: Illustrates an asynchronous Python function for buffering agent stream output. This technique collects multiple smaller chunks into a larger buffer before yielding, which can improve performance by reducing the number of individual writes.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/guides/streaming.mdx#_snippet_5\\n\\nLANGUAGE: python\\nCODE:\\n```\\nasync def buffered_streaming(agent, query, buffer_size=10):\\n    buffer = []\\n\\n    async for chunk in agent.astream(query):\\n        buffer.append(chunk)\\n\\n        if len(buffer) >= buffer_size:\\n            yield \\'\\'.join(buffer)\\n            buffer = []\\n\\n    # Yield remaining buffer\\n    if buffer:\\n        yield \\'\\'.join(buffer)\\n\\n# Usage\\nasync for buffered_chunk in buffered_streaming(agent, \"Your query here\"):\\n    print(buffered_chunk, end=\"\", flush=True)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Development Setup\\nDESCRIPTION: Illustrates a simple development configuration using a JSON config file for the client and setting up the agent with specific parameters like max steps and verbosity.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/getting-started/configuration.mdx#_snippet_3\\n\\nLANGUAGE: python\\nCODE:\\n```\\n# Simple development configuration\\nfrom dotenv import load_dotenv\\nload_dotenv()\\n\\nclient = MCPClient.from_config_file(\"dev-config.json\")\\nagent = MCPAgent(\\n    llm=ChatOpenAI(model=\"gpt-4o\"),\\n    client=client,\\n    max_steps=10,\\n    verbose=True\\n)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Enable LLM Streaming for Agent Output\\nDESCRIPTION: Ensures that the underlying Large Language Model (LLM) is configured to support streaming. This is a critical prerequisite for the agent\\'s streaming functionality to work correctly.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/guides/streaming.mdx#_snippet_7\\n\\nLANGUAGE: python\\nCODE:\\n```\\n# Enable streaming in your LLM\\nllm = ChatOpenAI(model=\"gpt-4\", streaming=True)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Low-Level Event Streaming\\nDESCRIPTION: Shows how to stream agent output at a low level using `stream_events`, providing real-time output events for more granular control. This is suitable for custom interfaces and detailed event handling.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/agent/streaming.mdx#_snippet_1\\n\\nLANGUAGE: python\\nCODE:\\n```\\nimport asyncio\\nfrom langchain_openai import ChatOpenAI\\nfrom mcp_use import MCPAgent, MCPClient\\n\\nasync def basic_streaming_example():\\n    # Setup agent\\n    config = {\\n        \"mcpServers\": {\\n            \"playwright\": {\\n                \"command\": \"npx\",\\n                \"args\": [\"@playwright/mcp@latest\"]\\n            }\\n        }\\n    }\\n\\n    client = MCPClient.from_dict(config)\\n    llm = ChatOpenAI(model=\"gpt-4\")\\n    agent = MCPAgent(llm=llm, client=client)\\n\\n    # Stream the agent\\'s response\\n    print(\"Agent is working...\")\\n    async for chunk in agent.stream_events(\"Search for the latest Python news and summarize it\"):\\n        print(chunk, end=\"\", flush=True)\\n\\n    print(\"\\\\n\\\\nDone!\")\\n\\nif __name__ == \"__main__\":\\n    asyncio.run(basic_streaming_example())\\n```\\n\\n----------------------------------------\\n\\nTITLE: Initializing MCPClient and Creating Tools with Adapter (Python)\\nDESCRIPTION: This snippet demonstrates the initial steps for integrating MCP-Use with an agent framework. It initializes the `MCPClient` from a configuration file, creates an instance of a custom `YourFrameworkAdapter`, and then uses the adapter to asynchronously create tools from the client. Finally, these tools are passed to the `your_framework.create_agent` method to build an agent.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/building-custom-agents.mdx#_snippet_4\\n\\nLANGUAGE: Python\\nCODE:\\n```\\n# Initialize the client\\nclient = MCPClient.from_config_file(\"config.json\")\\n\\n# Create an adapter instance\\nadapter = YourFrameworkAdapter()\\n\\n# Get tools with a single line\\ntools = await adapter.create_tools(client)\\n\\n# Use the tools with your framework\\nagent = your_framework.create_agent(tools=tools)\\n```\\n\\n----------------------------------------\\n\\nTITLE: MCP Agent Tool Usage Example\\nDESCRIPTION: Illustrates how an agent, with the Server Manager enabled, can execute complex, multi-step tasks that require discovering and utilizing tools from different servers. The agent automatically handles tool discovery, server connection, and execution.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/agent/server-manager.mdx#_snippet_1\\n\\nLANGUAGE: python\\nCODE:\\n```\\n# Agent automatically discovers and uses the right tools\\nresult = await agent.run(\"\"\"\\nI need to:\\n1. Find tools for web scraping\\n2. Connect to the right server\\n3. Scrape data from https://example.com\\n4. Save it to a file\\n\\nStart by searching for relevant tools.\\n\"\"\")\\n```\\n\\n----------------------------------------\\n\\nTITLE: Configure MCPAgent Timeout and Retry Behavior (Python)\\nDESCRIPTION: Explains how to set a timeout for agent operations using the `timeout` parameter and how to configure retry behavior for the underlying LLM (e.g., `ChatOpenAI`) to handle transient errors.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/essentials/agent-configuration.mdx#_snippet_17\\n\\nLANGUAGE: python\\nCODE:\\n```\\n# Set timeout for agent operations\\nagent = MCPAgent(\\n    llm=llm,\\n    client=client,\\n    timeout=60  # 60 seconds timeout\\n)\\n\\n# Configure retry behavior (if supported by LLM)\\nllm = ChatOpenAI(\\n    model=\"gpt-4o\",\\n    max_retries=3,\\n    retry_delay=2\\n)\\n\\nagent = MCPAgent(llm=llm, client=client)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Configure MCPAgent with Basic and Advanced Parameters\\nDESCRIPTION: Shows how to instantiate an MCPAgent with essential parameters like LLM and client, and also demonstrates advanced configuration options including max_steps, server_name, memory_enabled, custom prompts, and disallowed_tools.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/essentials/agent-configuration.mdx#_snippet_2\\n\\nLANGUAGE: python\\nCODE:\\n```\\nfrom mcp_use import MCPAgent, MCPClient\\nfrom langchain_openai import ChatOpenAI\\n\\n# Basic configuration\\nagent = MCPAgent(\\n    llm=ChatOpenAI(model=\"gpt-4o\", temperature=0.7),\\n    client=MCPClient.from_config_file(\"config.json\"),\\n    max_steps=30\\n)\\n\\n# Advanced configuration\\nagent = MCPAgent(\\n    llm=ChatOpenAI(model=\"gpt-4o\", temperature=0.7),\\n    client=MCPClient.from_config_file(\"config.json\"),\\n    max_steps=30,\\n    server_name=None,\\n    auto_initialize=True,\\n    memory_enabled=True,\\n    system_prompt=\"Custom instructions for the agent\",\\n    additional_instructions=\"Additional guidelines for specific tasks\",\\n    disallowed_tools=[\"file_system\", \"network\", \"shell\"]  # Restrict potentially dangerous tools\\n)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Simplified Tool Creation with LangChainAdapter\\nDESCRIPTION: This snippet highlights the simplified API for creating LangChain tools from MCPClient using the LangChainAdapter. It emphasizes that the adapter handles sessions, connectors, and initialization automatically, streamlining the process of obtaining tools without manual configuration.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/building-custom-agents.mdx#_snippet_1\\n\\nLANGUAGE: python\\nCODE:\\n```\\nadapter = LangChainAdapter()\\ntools = await adapter.create_tools(client)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Simplified Tool Creation with LangChain Adapter\\nDESCRIPTION: Illustrates the streamlined API for creating LangChain tools from MCPClient using the LangChainAdapter\\'s `create_tools` method, highlighting automatic session and connector management.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/guides/building-custom-agents.mdx#_snippet_1\\n\\nLANGUAGE: python\\nCODE:\\n```\\nadapter = LangChainAdapter()\\ntools = await adapter.create_tools(client)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Update Agent Tool Restrictions\\nDESCRIPTION: Demonstrates how to update and check the disallowed tools for an MCP agent. This is useful for restricting agent capabilities for security or task-specific focus.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/api-reference/introduction.mdx#_snippet_14\\n\\nLANGUAGE: python\\nCODE:\\n```\\nagent.set_disallowed_tools([\"file_system\", \"network\", \"shell\", \"database\"])\\nawait agent.initialize()\\n\\nrestricted_tools = agent.get_disallowed_tools()\\nprint(f\"Restricted tools: {restricted_tools}\")\\n```\\n\\n----------------------------------------\\n\\nTITLE: Agent Core Methods\\nDESCRIPTION: Documentation for the essential methods of the MCP Agent, covering running queries, resetting the agent\\'s state, retrieving interaction history, and managing disallowed tools.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/api-reference/introduction.mdx#_snippet_6\\n\\nLANGUAGE: APIDOC\\nCODE:\\n```\\nrun:\\n  Runs the agent with a given query.\\n  Signature:\\n    await agent.run(query: str, max_steps: int = None, stop_on_first_result: bool = False, server_name: str = None, callbacks: list = None)\\n  Parameters:\\n    query (str): The query to run.\\n    max_steps (int, optional): Overrides the instance max_steps. Defaults to None.\\n    stop_on_first_result (bool, optional): Whether to stop at first result. Defaults to False.\\n    server_name (str, optional): Specific server to use. Defaults to None.\\n    callbacks (list, optional): Callback functions for events. Defaults to None.\\n  Usage Example:\\n    result = await agent.run(\\n        query=\"Find information about Python libraries\",\\n        max_steps=25,\\n        stop_on_first_result=False\\n    )\\n  When to use different parameters:\\n    - max_steps: Override the instance default for specific queries\\n    - stop_on_first_result: Use True for simple lookups, False for thorough exploration\\n    - server_name: Specify when using multiple servers for different tasks\\n    - callbacks: Add for monitoring or logging specific runs\\n\\nreset:\\n  Resets the agent state.\\n  Signature:\\n    agent.reset()\\n  Usage Example:\\n    agent.reset()\\n  When to use:\\n    - Between different tasks to clear context\\n    - When starting a new conversation thread\\n    - When agent gets stuck in a particular strategy\\n\\nget_history:\\n  Gets the agent\\'s interaction history.\\n  Signature:\\n    agent.get_history()\\n  Usage Example:\\n    history = agent.get_history()\\n  When to use:\\n    - For debugging agent behavior\\n    - When implementing custom logging\\n    - To provide context for follow-up queries\\n\\nset_disallowed_tools:\\n  Sets the list of tools that should not be available to the agent.\\n  Signature:\\n    agent.set_disallowed_tools(disallowed_tools: list[str])\\n  Parameters:\\n    disallowed_tools (list[str]): List of tool names that should not be available.\\n  Usage Example:\\n    agent.set_disallowed_tools([\"tool1\", \"tool2\"])\\n  When to use:\\n    - To restrict access to specific tools for security reasons\\n    - To limit agent capabilities for specific tasks\\n    - To prevent the agent from using potentially dangerous tools\\n    - Note: Changes take effect on next initialization\\n\\nget_disallowed_tools:\\n  Gets the list of tools that are not available to the agent.\\n  Signature:\\n    agent.get_disallowed_tools()\\n  Usage Example:\\n    disallowed = agent.get_disallowed_tools()\\n  When to use:\\n    - To check which tools are currently restricted\\n    - For debugging or auditing purposes\\n    - To verify tool restrictions before running the agent\\n```\\n\\n----------------------------------------\\n\\nTITLE: Restricting Agent Tool Access in MCPAgent\\nDESCRIPTION: Demonstrates how to enhance security and control agent behavior by restricting the set of tools available to an MCPAgent. This is achieved by providing a list of disallowed_tools during agent initialization.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/api-reference/introduction.mdx#_snippet_23\\n\\nLANGUAGE: python\\nCODE:\\n```\\nfrom mcp_use import MCPAgent, MCPClient\\nfrom langchain_openai import ChatOpenAI\\n\\n# Create agent with restricted tools\\nagent = MCPAgent(\\n    llm=ChatOpenAI(model=\"gpt-4o\"),\\n    client=client,\\n    disallowed_tools=[\"file_system\", \"network\", \"shell\"]  # Restrict potentially dangerous tools\\n)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Stream Agent Output\\nDESCRIPTION: Demonstrates how to use the `astream` method of MCPAgent for asynchronous streaming of agent output. It iterates over chunks, printing messages, actions, and steps in real-time.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/README.md#_snippet_9\\n\\nLANGUAGE: python\\nCODE:\\n```\\nasync for chunk in agent.astream(\"Find the best restaurant in San Francisco\"):\\n    print(chunk[\"messages\"], end=\"\", flush=True)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Performance: Limit Execution Steps\\nDESCRIPTION: Limits the maximum number of steps the agent can execute to prevent runaway processes. This is a crucial safety measure for controlling agent behavior.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/agent/agent-configuration.mdx#_snippet_14\\n\\nLANGUAGE: python\\nCODE:\\n```\\n# Limit execution steps\\nagent = MCPAgent(\\n    llm=llm,\\n    client=client,\\n    max_steps=10  # Prevent runaway execution\\n)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Complete MCPAgent Tool Access Control Example\\nDESCRIPTION: Provides a full asynchronous Python example demonstrating how to set up an MCPAgent, load environment variables, configure an MCPClient from a dictionary, and restrict specific tools like \\'file_system\\' and \\'network\\' before running an agent query.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/essentials/agent-configuration.mdx#_snippet_3\\n\\nLANGUAGE: python\\nCODE:\\n```\\nimport asyncio\\nimport os\\nfrom dotenv import load_dotenv\\nfrom langchain_openai import ChatOpenAI\\nfrom mcp_use import MCPAgent, MCPClient\\n\\nasync def main():\\n    # Load environment variables\\n    load_dotenv()\\n\\n    # Create configuration dictionary\\n    config = {\\n      \"mcpServers\": {\\n        \"playwright\": {\\n          \"command\": \"npx\",\\n          \"args\": [\"@playwright/mcp@latest\"],\\n          \"env\": {\\n            \"DISPLAY\": \":1\"\\n          }\\n        }\\n      }\\n    }\\n\\n    # Create MCPClient from configuration dictionary\\n    client = MCPClient.from_dict(config)\\n\\n    # Create LLM\\n    llm = ChatOpenAI(model=\"gpt-4o\")\\n\\n    # Create agent with restricted tools\\n    agent = MCPAgent(\\n        llm=llm,\\n        client=client,\\n        max_steps=30,\\n        disallowed_tools=[\"file_system\", \"network\"]  # Restrict potentially dangerous tools\\n    )\\n\\n    # Run the query\\n    result = await agent.run(\\n        \"Find the best restaurant in San Francisco USING GOOGLE SEARCH\",\\n    )\\n    print(f\"\\\\nResult: {result}\")\\n\\nif __name__ == \"__main__\":\\n    asyncio.run(main())\\n```\\n\\n----------------------------------------\\n\\nTITLE: Memory Configuration for MCPAgent\\nDESCRIPTION: Provides examples of configuring conversation memory for MCPAgent. It shows how to enable memory for context maintenance and disable it for stateless interactions.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/agent/agent-configuration.mdx#_snippet_10\\n\\nLANGUAGE: python\\nCODE:\\n```\\n# Enable memory (default)\\nagent = MCPAgent(\\n    llm=llm,\\n    client=client,\\n    memory_enabled=True\\n)\\n\\n# Disable memory for stateless interactions\\nagent = MCPAgent(\\n    llm=llm,\\n    client=client,\\n    memory_enabled=False\\n)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Agent Connection Pooling and Reuse\\nDESCRIPTION: A Python `AgentPool` class designed to manage a pool of MCP clients and agents. It demonstrates initializing clients, pre-creating sessions, and efficiently retrieving and returning agents to the pool for reuse.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/troubleshooting/performance.mdx#_snippet_13\\n\\nLANGUAGE: python\\nCODE:\\n```\\nclass AgentPool:\\n    def __init__(self, pool_size=3):\\n        self.pool_size = pool_size\\n        self.clients = []\\n        self.available_clients = asyncio.Queue()\\n\\n    async def initialize(self):\\n        for _ in range(self.pool_size):\\n            client = MCPClient.from_config_file(\"config.json\")\\n            await client.create_all_sessions()  # Pre-create sessions\\n            self.clients.append(client)\\n            await self.available_clients.put(client)\\n\\n    async def get_agent(self):\\n        client = await self.available_clients.get()\\n        llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\\n        return MCPAgent(llm=llm, client=client, use_server_manager=True)\\n\\n    async def return_client(self, agent):\\n        await self.available_clients.put(agent.client)\\n\\n# Usage\\npool = AgentPool(pool_size=3)\\nawait pool.initialize()\\n\\nagent = await pool.get_agent()\\nresult = await agent.run(\"Your query\")\\nawait pool.return_client(agent)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Multi-server Setup with Server Manager\\nDESCRIPTION: Illustrates a multi-server setup using MCPAgent with the Server Manager enabled. The agent automatically selects the appropriate server based on the tool usage, optimizing performance and resource consumption.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/agent/agent-configuration.mdx#_snippet_9\\n\\nLANGUAGE: python\\nCODE:\\n```\\n# Multi-server setup with server manager\\nclient = MCPClient.from_config_file(\"multi_server_config.json\")\\nagent = MCPAgent(\\n    llm=llm,\\n    client=client,\\n    use_server_manager=True\\n)\\n\\n# The agent automatically selects servers based on tool usage\\nresult = await agent.run(\\n    \"Search for a place in Barcelona on Airbnb, then Google nearby restaurants.\"\\n)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Agent Tool Search and Discovery\\nDESCRIPTION: This snippet demonstrates how an agent uses its `run` method to search for tools required to complete a multi-step task. The agent intelligently identifies relevant tools from different servers based on the task description, such as `navigate_browser`, `take_screenshot`, and `write_file`.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/essentials/server-manager.mdx#_snippet_9\\n\\nLANGUAGE: python\\nCODE:\\n```\\nresult = await agent.run(\\n    \"\"\"I need to perform the following tasks:\\n    1. Navigate to a website\\n    2. Take a screenshot of the page\\n    3. Save the screenshot to a file\\n\\n    First, search for tools that could help me with each of these tasks.\"\"\"\\n)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Simplified LangChain Tool Creation\\nDESCRIPTION: Illustrates the concise method for creating LangChain tools using the LangChainAdapter. This snippet highlights how the adapter abstracts away the complexities of session management, connector initialization, and tool conversion.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/advanced/building-custom-agents.mdx#_snippet_1\\n\\nLANGUAGE: python\\nCODE:\\n```\\nadapter = LangChainAdapter()\\ntools = await adapter.create_tools(client)\\n```\\n\\n----------------------------------------\\n\\nTITLE: MCP Agent Tool Filtering by Type\\nDESCRIPTION: Shows how to control the tools available to the MCP Agent by specifying allowed and disallowed tool types. This helps prevent confusion and ensures the agent uses only relevant functionalities.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/advanced/multi-server-setup.mdx#_snippet_14\\n\\nLANGUAGE: python\\nCODE:\\n```\\n# Restrict to specific tool types\\nagent = MCPAgent(\\n    llm=llm,\\n    client=client,\\n    allowed_tools=[\"file_read\", \"file_write\", \"web_search\"],\\n    disallowed_tools=[\"system_exec\", \"network_request\"]\\n)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Stream mcp-use Agent Output (Python)\\nDESCRIPTION: Demonstrates how to asynchronously stream responses from an mcp-use agent as they are generated, providing real-time feedback to the user.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/quickstart.mdx#_snippet_6\\n\\nLANGUAGE: python\\nCODE:\\n```\\nasync for chunk in agent.astream(\"your query here\"):\\n    print(chunk, end=\"\", flush=True)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Error Handling: Set Timeout\\nDESCRIPTION: Sets a timeout for agent operations, specifying the maximum duration in seconds before an operation is considered failed. This prevents indefinite waiting.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/agent/agent-configuration.mdx#_snippet_21\\n\\nLANGUAGE: python\\nCODE:\\n```\\n# Set timeout for agent operations\\nagent = MCPAgent(\\n    llm=llm,\\n    client=client,\\n    timeout=60  # 60 seconds timeout\\n)\\n```\\n\\n----------------------------------------\\n\\nTITLE: MCP Agent Tool Filtering by Server\\nDESCRIPTION: Demonstrates how to filter the tools available to the MCP Agent by specifying which servers should be active. This approach allows for granular control over the agent\\'s capabilities based on the configured servers.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/advanced/multi-server-setup.mdx#_snippet_15\\n\\nLANGUAGE: python\\nCODE:\\n```\\n# Or filter by server\\nagent = MCPAgent(\\n    llm=llm,\\n    client=client,\\n    allowed_servers=[\"filesystem\", \"playwright\"],\\n    use_server_manager=True\\n)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Configure MCPAgent Conversation Memory (Disabled)\\nDESCRIPTION: This snippet demonstrates how to disable conversation memory for the `MCPAgent` by setting `memory_enabled=False`. This is useful for creating stateless interactions where the agent does not need to retain context from previous turns.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/essentials/agent-configuration.mdx#_snippet_10\\n\\nLANGUAGE: python\\nCODE:\\n```\\n# Disable memory for stateless interactions\\nagent = MCPAgent(\\n    llm=llm,\\n    client=client,\\n    memory_enabled=False\\n)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Use Langchain ChatPromptTemplate for MCPAgent System Prompt (Python)\\nDESCRIPTION: Illustrates how to provide a more advanced, templated system prompt using `langchain.prompts.ChatPromptTemplate`. This allows for dynamic insertion of values like domain and instructions into the prompt.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/essentials/agent-configuration.mdx#_snippet_13\\n\\nLANGUAGE: python\\nCODE:\\n```\\nfrom langchain.prompts import ChatPromptTemplate\\n\\ncustom_template = ChatPromptTemplate.from_messages([\\n    (\"system\", \"You are an expert {domain} assistant. {instructions}\"),\\n    (\"human\", \"{input}\"),\\n    # ... other message templates\\n])\\n\\nagent = MCPAgent(\\n    llm=llm,\\n    client=client,\\n    system_prompt_template=custom_template\\n)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Filter Agent Streaming Events for Performance\\nDESCRIPTION: Provides an asynchronous Python function to selectively process agent events from a stream. By filtering for specific event types, this method helps reduce processing overhead and allows focus on relevant information, improving performance.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/guides/streaming.mdx#_snippet_6\\n\\nLANGUAGE: python\\nCODE:\\n```\\nasync def filtered_streaming(agent, query):\\n    interesting_events = [\\n        \"on_tool_start\",\\n        \"on_tool_end\",\\n        \"on_chat_model_stream\",\\n        \"on_chain_end\"\\n    ]\\n\\n    async for event in agent.astream_events(query, version=\"v1\"):\\n        if event.get(\"event\") in interesting_events:\\n            yield event\\n\\n# Usage\\nasync for event in filtered_streaming(agent, \"Your query\"):\\n    # Process only relevant events\\n    handle_event(event)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Python Streaming UI for MCP Agent\\nDESCRIPTION: This Python script implements a `StreamingUI` class to manage and display real-time updates from an MCP Agent. It includes methods for clearing lines, printing status messages, showing the agent\\'s thinking process, and displaying tool results with proper formatting and timestamps. The `streaming_ui_example` function demonstrates how to integrate this UI with an `MCPAgent` and `MCPClient` to process a query and stream events.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/agent/streaming.mdx#_snippet_2\\n\\nLANGUAGE: python\\nCODE:\\n```\\nimport asyncio\\nimport sys\\nfrom datetime import datetime\\nfrom langchain_openai import ChatOpenAI\\nfrom mcp_use import MCPAgent, MCPClient\\n\\n\\nclass StreamingUI:\\n    def __init__(self):\\n        self.current_thought = \"\"\\n        self.tool_outputs = []\\n        self.final_answer = \"\"\\n\\n    def clear_line(self):\\n        \"\"\"Clear the current line in terminal\"\"\"\\n        sys.stdout.write(\"\\\\r\\\\033[K\")\\n\\n    def print_status(self, status, tool=None):\\n        \"\"\"Print colored status updates\"\"\"\\n        timestamp = datetime.now().strftime(\"%H:%M:%S\")\\n        if tool:\\n            print(f\"\\\\033[94m[{timestamp}] {status}: {tool}\\\\033[0m\")\\n        else:\\n            print(f\"\\\\033[92m[{timestamp}] {status}\\\\033[0m\")\\n\\n    def print_thinking(self, text):\\n        \"\"\"Print agent\\'s reasoning in real-time\"\"\"\\n        self.clear_line()\\n        truncated = text[:80] + \"...\" if len(text) > 80 else text\\n        sys.stdout.write(f\"\\\\033[93m💭 Thinking: {truncated}\\\\033[0m\")\\n        sys.stdout.flush()\\n\\n    def print_tool_result(self, tool_name, result):\\n        \"\"\"Print tool execution results\"\"\"\\n        print(f\"\\\\n\\\\033[96m🔧 {tool_name} result:\\\\033[0m\")\\n        # Truncate long results\\n        display_result = result[:200] + \"...\" if len(result) > 200 else result\\n        print(f\"   {display_result}\")\\n\\n\\nasync def streaming_ui_example():\\n    config = {\"mcpServers\": {\"playwright\": {\"command\": \"npx\", \"args\": [\"@playwright/mcp@latest\"]}}}\\n\\n    client = MCPClient.from_dict(config)\\n    llm = ChatOpenAI(model=\"gpt-4\", streaming=True)\\n    agent = MCPAgent(llm=llm, client=client)\\n\\n    ui = StreamingUI()\\n\\n    query = \"What are the current trending topics on Hacker News?\"\\n\\n    print(\"🤖 MCP Agent - Interactive Session\")\\n    print(\"=\" * 50)\\n    print(f\"Query: {query}\")\\n    print(\"=\" * 50)\\n\\n    current_tool = None\\n    current_reasoning = \"\"\\n\\n    async for event in agent.stream_events(query):\\n        event_type = event.get(\"event\")\\n        data = event.get(\"data\", {})\\n\\n        if event_type == \"on_chat_model_start\":\\n            ui.print_status(\"Starting to plan\")\\n\\n        elif event_type == \"on_chat_model_stream\":\\n            chunk = data.get(\"chunk\", {})\\n            if hasattr(chunk, \"content\") and chunk.content:\\n                current_reasoning += chunk.content\\n                ui.print_thinking(current_reasoning)\\n\\n        elif event_type == \"on_tool_start\":\\n            current_tool = data.get(\"input\", {}).get(\"tool_name\")\\n            if current_tool:\\n                print(\"\\\\n\")  # New line after thinking\\n                ui.print_status(\"Executing tool\", current_tool)\\n                current_reasoning = \"\"  # Reset for next iteration\\n\\n        elif event_type == \"on_tool_end\":\\n            output = data.get(\"output\")\\n            if current_tool and output:\\n                ui.print_tool_result(current_tool, str(output))\\n\\n        elif event_type == \"on_chain_end\":\\n            print(\"\\\\n\")\\n            ui.print_status(\"Task completed!\")\\n\\n            # Extract final answer\\n            final_output = data.get(\"output\")\\n            if final_output:\\n                print(f\"\\\\n\\\\033[92m📋 Final Answer:\\\\033[0m\")\\n                print(f\"{final_output}\")\\n\\n\\nif __name__ == \"__main__\":\\n    asyncio.run(streaming_ui_example())\\n\\n```\\n\\n----------------------------------------\\n\\nTITLE: System Prompt Templates\\nDESCRIPTION: Provides a custom system prompt template using Langchain\\'s ChatPromptTemplate for more advanced customization. This allows for dynamic insertion of variables like domain and instructions.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/agent/agent-configuration.mdx#_snippet_13\\n\\nLANGUAGE: python\\nCODE:\\n```\\nfrom langchain.prompts import ChatPromptTemplate\\n\\ncustom_template = ChatPromptTemplate.from_messages([\\n    (\"system\", \"You are an expert {domain} assistant. {instructions}\"),\\n    (\"human\", \"{input}\"),\\n    # ... other message templates\\n])\\n\\nagent = MCPAgent(\\n    llm=llm,\\n    client=client,\\n    system_prompt_template=custom_template\\n)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Configure MCPAgent Conversation Memory (Enabled)\\nDESCRIPTION: This snippet shows how to explicitly enable conversation memory for the `MCPAgent` by setting `memory_enabled=True`. This allows the agent to maintain context across multiple interactions, which is the default behavior.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/essentials/agent-configuration.mdx#_snippet_9\\n\\nLANGUAGE: python\\nCODE:\\n```\\n# Enable memory (default)\\nagent = MCPAgent(\\n    llm=llm,\\n    client=client,\\n    memory_enabled=True\\n)\\n```\\n\\n----------------------------------------\\n\\nTITLE: Restricting Agent Tools with Playwright MCP Server (Python)\\nDESCRIPTION: This snippet demonstrates how to initialize an `MCPAgent` with specific tools disallowed, such as \\'file_system\\' and \\'network\\', to enhance security. It configures a Playwright MCP server and uses `ChatOpenAI` as the LLM, then runs a query with the restricted agent.\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/quickstart.mdx#_snippet_16\\n\\nLANGUAGE: python\\nCODE:\\n```\\nimport asyncio\\nimport os\\nfrom dotenv import load_dotenv\\nfrom langchain_openai import ChatOpenAI\\nfrom mcp_use import MCPAgent, MCPClient\\n\\nasync def main():\\n    # Load environment variables\\n    load_dotenv()\\n\\n    # Create configuration dictionary\\n    config = {\\n      \"mcpServers\": {\\n        \"playwright\": {\\n          \"command\": \"npx\",\\n          \"args\": [\"@playwright/mcp@latest\"],\\n          \"env\": {\\n            \"DISPLAY\": \":1\"\\n          }\\n        }\\n      }\\n    }\\n\\n    # Create MCPClient from configuration dictionary\\n    client = MCPClient.from_dict(config)\\n\\n    # Create LLM\\n    llm = ChatOpenAI(model=\"gpt-4o\")\\n\\n    # Create agent with restricted tools\\n    agent = MCPAgent(\\n        llm=llm,\\n        client=client,\\n        max_steps=30,\\n        disallowed_tools=[\"file_system\", \"network\"]  # Restrict potentially dangerous tools\\n    )\\n\\n    # Run the query\\n    result = await agent.run(\\n        \"Find the best restaurant in San Francisco USING GOOGLE SEARCH\",\\n    )\\n    print(f\"\\\\nResult: {result}\")\\n\\nif __name__ == \"__main__\":\\n    asyncio.run(main())\\n```\\n\\n========================\\nQUESTIONS AND ANSWERS\\n========================\\nTOPIC: Agent Structured Output - MCPAgent\\nQ: What is Agent Structured Output in the context of MCPAgent?\\nA: Agent Structured Output allows the MCPAgent to return strongly-typed Pydantic models instead of plain text. The agent becomes schema-aware and retries to gather missing information until all required fields are populated.\\n\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/agent/structured-output.mdx#_qa_0\\n\\n----------------------------------------\\n\\nTOPIC: Agent Configuration - MCPAgent\\nQ: What is the purpose of Agent Configuration in MCPAgent?\\nA: Agent Configuration in MCPAgent allows users to customize the behavior of the agent and its integration with Large Language Models (LLMs). This includes setting up API keys, defining agent parameters, and controlling tool access.\\n\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/agent/agent-configuration.mdx#_qa_0\\n\\n----------------------------------------\\n\\nTOPIC: Streaming Agent Output - MCP Use\\nQ: When should the `stream()` method be used with mcp-use?\\nA: The `stream()` method in mcp-use is recommended when you need to display step-by-step progress, process each tool call individually, build a workflow UI, or require simple and clean step tracking.\\n\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/agent/streaming.mdx#_qa_1\\n\\n----------------------------------------\\n\\nTOPIC: Building Custom Agents with MCP-Use\\nQ: What is the benefit of the simplified API provided by the LangChain adapter?\\nA: The LangChain adapter\\'s simplified API automatically handles the complexity of session management, connector initialization, and tool conversion, making it easier to integrate MCP tools with LangChain.\\n\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/advanced/building-custom-agents.mdx#_qa_4\\n\\n----------------------------------------\\n\\nTOPIC: Building Custom Agents with MCP-Use\\nQ: What is the process for contributing a new adapter for a different agent framework to MCP-Use?\\nA: To contribute a new adapter, you need to implement the `BaseAdapter` abstract class. This involves implementing the `_convert_tool` method to convert a single MCP tool into the format required by your target agent framework.\\n\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/advanced/building-custom-agents.mdx#_qa_6\\n\\n----------------------------------------\\n\\nTOPIC: Building Custom Agents with MCP-Use\\nQ: What is the primary purpose of building custom agents with MCP-Use?\\nA: Building custom agents with MCP-Use provides maximum flexibility to integrate with existing systems, implement specialized behavior, or use different agent frameworks, beyond the capabilities of the built-in MCPAgent class.\\n\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/advanced/building-custom-agents.mdx#_qa_0\\n\\n----------------------------------------\\n\\nTOPIC: Agent Structured Output - MCPAgent\\nQ: How does MCPAgent achieve structured output?\\nA: When an `output_schema` is provided, the MCPAgent understands the data requirements, attempts to format the output according to the schema, and intelligently retries to gather any missing information. It validates completeness before finishing.\\n\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/agent/structured-output.mdx#_qa_1\\n\\n----------------------------------------\\n\\nTOPIC: Building Custom Agents with MCP-Use\\nQ: What are the key capabilities offered by MCP-Use for building custom agents?\\nA: MCP-Use allows users to access MCP tools through flexible connectors, convert and adapt these tools for any agent framework using adapters, and build specialized agents tailored to specific use cases.\\n\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/advanced/building-custom-agents.mdx#_qa_1\\n\\n----------------------------------------\\n\\nTOPIC: Agent Structured Output - MCPAgent\\nQ: What are the key benefits of using Agent Structured Output with MCPAgent?\\nA: Key benefits include type safety with full IDE support and validation, intelligent data gathering where the agent ensures all required information is collected, automatic retries for missing fields, and built-in validation for data types and constraints.\\n\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/docs/agent/structured-output.mdx#_qa_2\\n\\n----------------------------------------\\n\\nTOPIC: MCP-Use: Connect LLMs to MCP Servers and Build Custom Agents\\nQ: What kind of tools can MCP-Use agents access?\\nA: MCP-Use agents can be equipped with access to various tools, including but not limited to web browsing and file operations.\\n\\n\\nSOURCE: https://github.com/mcp-use/mcp-use/blob/main/README.md#_qa_4', tool_call_id='pyd_ai_97d3aa46303145d8831b23cb3b06eb6c', timestamp=datetime.datetime(2025, 9, 3, 22, 20, 40, 786042, tzinfo=datetime.timezone.utc))], instructions='You are a helpful assistant that can answer questions about code, you use Context7 to search first for the library and then ask some questions to context7 about that library, based on the results, you give an answer to the user'))\n",
      "CallToolsNode(model_response=ModelResponse(parts=[TextPart(content='Here is the code for a simple agent connected to an MCP, using the `mcp-use` library. This example demonstrates how to initialize an `MCPAgent` with a `MCPClient` and an OpenAI LLM, and then run a simple query. It also shows how to restrict certain tools for security or task-specific reasons.\\n\\n'), TextPart(content='```python\\nimport asyncio\\nimport os\\nfrom dotenv import load_dotenv\\nfrom langchain_openai import ChatOpenAI\\nfrom mcp_use import MCPAgent, MCPClient\\n\\nasync def main():\\n    # Load environment variables (e.g., OPENAI_API_KEY)\\n    load_dotenv()\\n\\n    # Create a configuration dictionary for your MCP servers.\\n    # This example configures a \\'playwright\\' server for web automation.\\n    config = {\\n      \"mcpServers\": {\\n        \"playwright\": {\\n          \"command\": \"npx\",\\n          \"args\": [\"@playwright/mcp@latest\"],\\n          \"env\": {\\n            \"DISPLAY\": \":1\" # This might be needed for Playwright in some environments\\n          }\\n        }\\n      }\\n    }\\n\\n    # Create an MCPClient from the configuration dictionary.\\n    # The client manages the connection to your MCP servers.\\n    client = MCPClient.from_dict(config)\\n\\n    # Initialize your Large Language Model (LLM).\\n    # Here, we use ChatOpenAI with the \"gpt-4o\" model.\\n    llm = ChatOpenAI(model=\"gpt-4o\")\\n\\n    # Create the MCPAgent.\\n    # - llm: The language model to power the agent.\\n    # - client: The MCPClient instance to access MCP tools.\\n    # - max_steps: Limits the number of steps the agent can take to prevent runaway execution.\\n    # - disallowed_tools: A list of tool names that the agent is not allowed to use.\\n    #                     This is useful for security or to focus the agent on specific tasks.\\n    agent = MCPAgent(\\n        llm=llm,\\n        client=client,\\n        max_steps=30,\\n        disallowed_tools=[\"file_system\", \"network\"]  # Restrict potentially dangerous tools\\n    )\\n\\n    # Run a query with the agent.\\n    # The agent will use its available tools (excluding disallowed ones) to fulfill the query.\\n    print(\"Agent is running the query...\")\\n    result = await agent.run(\\n        \"Find the best restaurant in San Francisco USING GOOGLE SEARCH\",\\n    )\\n    print(f\"\\\\nResult: {result}\")\\n\\nif __name__ == \"__main__\":\\n    asyncio.run(main())\\n```'), TextPart(content='\\n\\n**To use this code:**\\n\\n1.  **Install necessary libraries:**\\n    '), TextPart(content='```bash\\n    pip install python-dotenv langchain-openai mcp-use\\n    npm install @playwright/mcp@latest\\n    ```'), TextPart(content='\\n2.  **Set your OpenAI API Key:** Create a `.env` file in the same directory as your Python script and add your OpenAI API key:\\n    '), TextPart(content='```\\n    OPENAI_API_KEY=\"your_openai_api_key_here\"\\n    ```'), TextPart(content='\\n3.  **Run the script:**\\n    '), TextPart(content='```bash\\n    python your_agent_script.py\\n    ```'), TextPart(content='\\n\\nThis agent is configured to use a Playwright MCP server, which allows it to interact with web browsers. The `disallowed_tools` parameter is used to prevent the agent from accessing file system or network tools directly, enhancing security. The agent will attempt to find the best restaurant in San Francisco by using a \"GOOGLE SEARCH\" tool (which would be provided by the Playwright MCP in this setup).')], usage=RequestUsage(input_tokens=20667, cache_read_tokens=3008, output_tokens=958, details={'cached_content_tokens': 3008, 'thoughts_tokens': 152, 'text_prompt_tokens': 20667, 'text_cache_tokens': 3008}), model_name='gemini-2.5-flash', timestamp=datetime.datetime(2025, 9, 3, 22, 20, 47, 186281, tzinfo=datetime.timezone.utc), provider_name='google-gla', provider_details={'finish_reason': 'STOP'}, provider_response_id='P7-4aOTEFPGsqtsPv4T7wAw'))\n",
      "End(data=FinalResult(output='Here is the code for a simple agent connected to an MCP, using the `mcp-use` library. This example demonstrates how to initialize an `MCPAgent` with a `MCPClient` and an OpenAI LLM, and then run a simple query. It also shows how to restrict certain tools for security or task-specific reasons.\\n\\n\\n\\n```python\\nimport asyncio\\nimport os\\nfrom dotenv import load_dotenv\\nfrom langchain_openai import ChatOpenAI\\nfrom mcp_use import MCPAgent, MCPClient\\n\\nasync def main():\\n    # Load environment variables (e.g., OPENAI_API_KEY)\\n    load_dotenv()\\n\\n    # Create a configuration dictionary for your MCP servers.\\n    # This example configures a \\'playwright\\' server for web automation.\\n    config = {\\n      \"mcpServers\": {\\n        \"playwright\": {\\n          \"command\": \"npx\",\\n          \"args\": [\"@playwright/mcp@latest\"],\\n          \"env\": {\\n            \"DISPLAY\": \":1\" # This might be needed for Playwright in some environments\\n          }\\n        }\\n      }\\n    }\\n\\n    # Create an MCPClient from the configuration dictionary.\\n    # The client manages the connection to your MCP servers.\\n    client = MCPClient.from_dict(config)\\n\\n    # Initialize your Large Language Model (LLM).\\n    # Here, we use ChatOpenAI with the \"gpt-4o\" model.\\n    llm = ChatOpenAI(model=\"gpt-4o\")\\n\\n    # Create the MCPAgent.\\n    # - llm: The language model to power the agent.\\n    # - client: The MCPClient instance to access MCP tools.\\n    # - max_steps: Limits the number of steps the agent can take to prevent runaway execution.\\n    # - disallowed_tools: A list of tool names that the agent is not allowed to use.\\n    #                     This is useful for security or to focus the agent on specific tasks.\\n    agent = MCPAgent(\\n        llm=llm,\\n        client=client,\\n        max_steps=30,\\n        disallowed_tools=[\"file_system\", \"network\"]  # Restrict potentially dangerous tools\\n    )\\n\\n    # Run a query with the agent.\\n    # The agent will use its available tools (excluding disallowed ones) to fulfill the query.\\n    print(\"Agent is running the query...\")\\n    result = await agent.run(\\n        \"Find the best restaurant in San Francisco USING GOOGLE SEARCH\",\\n    )\\n    print(f\"\\\\nResult: {result}\")\\n\\nif __name__ == \"__main__\":\\n    asyncio.run(main())\\n```\\n\\n\\n\\n**To use this code:**\\n\\n1.  **Install necessary libraries:**\\n    \\n\\n```bash\\n    pip install python-dotenv langchain-openai mcp-use\\n    npm install @playwright/mcp@latest\\n    ```\\n\\n\\n2.  **Set your OpenAI API Key:** Create a `.env` file in the same directory as your Python script and add your OpenAI API key:\\n    \\n\\n```\\n    OPENAI_API_KEY=\"your_openai_api_key_here\"\\n    ```\\n\\n\\n3.  **Run the script:**\\n    \\n\\n```bash\\n    python your_agent_script.py\\n    ```\\n\\n\\n\\nThis agent is configured to use a Playwright MCP server, which allows it to interact with web browsers. The `disallowed_tools` parameter is used to prevent the agent from accessing file system or network tools directly, enhancing security. The agent will attempt to find the best restaurant in San Francisco by using a \"GOOGLE SEARCH\" tool (which would be provided by the Playwright MCP in this setup).'))\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.mcp import MCPServerStreamableHTTP\n",
    "\n",
    "context7_server = MCPServerStreamableHTTP(\n",
    "    url ='https://mcp.context7.com/mcp',\n",
    "    headers={'CONTEXT7_API_KEY': settings.CONTEXT7_API_KEY}\n",
    ")  \n",
    "\n",
    "mcp_agent = Agent(\n",
    "    settings.MODEL_NAME, toolsets=[context7_server], \n",
    "    instructions=\"You are a helpful assistant that can answer questions about code, you use Context7 to search first for the library and then ask some questions to context7 about that library, based on the results, you give an answer to the user\"\n",
    ")  \n",
    "# Run the Agent\n",
    "async with mcp_agent:  \n",
    "    nodes = []\n",
    "    async with mcp_agent.iter(\n",
    "        \"Give me the code for a simple agent that is connected to an MCP, using \",\n",
    "    ) as agent_run:\n",
    "        async for node in agent_run:\n",
    "            # Each node represents a step in the agent's execution\n",
    "            print(node)\n",
    "            nodes.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "320513a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the code for a simple agent connected to an MCP, using the `mcp-use` library. This example demonstrates how to initialize an `MCPAgent` with a `MCPClient` and an OpenAI LLM, and then run a simple query. It also shows how to restrict certain tools for security or task-specific reasons.\n",
      "\n",
      "\n",
      "\n",
      "```python\n",
      "import asyncio\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "from langchain_openai import ChatOpenAI\n",
      "from mcp_use import MCPAgent, MCPClient\n",
      "\n",
      "async def main():\n",
      "    # Load environment variables (e.g., OPENAI_API_KEY)\n",
      "    load_dotenv()\n",
      "\n",
      "    # Create a configuration dictionary for your MCP servers.\n",
      "    # This example configures a 'playwright' server for web automation.\n",
      "    config = {\n",
      "      \"mcpServers\": {\n",
      "        \"playwright\": {\n",
      "          \"command\": \"npx\",\n",
      "          \"args\": [\"@playwright/mcp@latest\"],\n",
      "          \"env\": {\n",
      "            \"DISPLAY\": \":1\" # This might be needed for Playwright in some environments\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "\n",
      "    # Create an MCPClient from the configuration dictionary.\n",
      "    # The client manages the connection to your MCP servers.\n",
      "    client = MCPClient.from_dict(config)\n",
      "\n",
      "    # Initialize your Large Language Model (LLM).\n",
      "    # Here, we use ChatOpenAI with the \"gpt-4o\" model.\n",
      "    llm = ChatOpenAI(model=\"gpt-4o\")\n",
      "\n",
      "    # Create the MCPAgent.\n",
      "    # - llm: The language model to power the agent.\n",
      "    # - client: The MCPClient instance to access MCP tools.\n",
      "    # - max_steps: Limits the number of steps the agent can take to prevent runaway execution.\n",
      "    # - disallowed_tools: A list of tool names that the agent is not allowed to use.\n",
      "    #                     This is useful for security or to focus the agent on specific tasks.\n",
      "    agent = MCPAgent(\n",
      "        llm=llm,\n",
      "        client=client,\n",
      "        max_steps=30,\n",
      "        disallowed_tools=[\"file_system\", \"network\"]  # Restrict potentially dangerous tools\n",
      "    )\n",
      "\n",
      "    # Run a query with the agent.\n",
      "    # The agent will use its available tools (excluding disallowed ones) to fulfill the query.\n",
      "    print(\"Agent is running the query...\")\n",
      "    result = await agent.run(\n",
      "        \"Find the best restaurant in San Francisco USING GOOGLE SEARCH\",\n",
      "    )\n",
      "    print(f\"\\nResult: {result}\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    asyncio.run(main())\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "**To use this code:**\n",
      "\n",
      "1.  **Install necessary libraries:**\n",
      "    \n",
      "\n",
      "```bash\n",
      "    pip install python-dotenv langchain-openai mcp-use\n",
      "    npm install @playwright/mcp@latest\n",
      "    ```\n",
      "\n",
      "\n",
      "2.  **Set your OpenAI API Key:** Create a `.env` file in the same directory as your Python script and add your OpenAI API key:\n",
      "    \n",
      "\n",
      "```\n",
      "    OPENAI_API_KEY=\"your_openai_api_key_here\"\n",
      "    ```\n",
      "\n",
      "\n",
      "3.  **Run the script:**\n",
      "    \n",
      "\n",
      "```bash\n",
      "    python your_agent_script.py\n",
      "    ```\n",
      "\n",
      "\n",
      "\n",
      "This agent is configured to use a Playwright MCP server, which allows it to interact with web browsers. The `disallowed_tools` parameter is used to prevent the agent from accessing file system or network tools directly, enhancing security. The agent will attempt to find the best restaurant in San Francisco by using a \"GOOGLE SEARCH\" tool (which would be provided by the Playwright MCP in this setup).\n"
     ]
    }
   ],
   "source": [
    "print(nodes[-1].data.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d2a3a3",
   "metadata": {},
   "source": [
    "## Command MCPs: DuckDuckGo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ba7bf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserPromptNode(user_prompt='What is the weather in Tokyo?', instructions='You are a helpful assistant that can use the DuckDuckGo MCP server to search the web and obtain current information, you always search first and then fetch the content of the search results you think are relevant to the question, finally you answer the question based on the content of the search results', instructions_functions=[], system_prompts=(), system_prompt_functions=[], system_prompt_dynamic_functions={})\n",
      "ModelRequestNode(request=ModelRequest(parts=[UserPromptPart(content='What is the weather in Tokyo?', timestamp=datetime.datetime(2025, 9, 3, 22, 21, 23, 80422, tzinfo=datetime.timezone.utc))], instructions='You are a helpful assistant that can use the DuckDuckGo MCP server to search the web and obtain current information, you always search first and then fetch the content of the search results you think are relevant to the question, finally you answer the question based on the content of the search results'))\n",
      "CallToolsNode(model_response=ModelResponse(parts=[ToolCallPart(tool_name='search', args={'query': 'weather in Tokyo'}, tool_call_id='pyd_ai_422a8e6ad7394b2ca0d91c71751338c5')], usage=RequestUsage(input_tokens=233, output_tokens=55, details={'thoughts_tokens': 40, 'text_prompt_tokens': 233}), model_name='gemini-2.5-flash', timestamp=datetime.datetime(2025, 9, 3, 22, 21, 23, 868398, tzinfo=datetime.timezone.utc), provider_name='google-gla', provider_details={'finish_reason': 'STOP'}, provider_response_id='ZL-4aI7XAaPQz7IP7cLq8A4'))\n",
      "ModelRequestNode(request=ModelRequest(parts=[ToolReturnPart(tool_name='search', content=\"Found 10 search results:\\n\\n1. Tokyo, Tokyo, Japan Weather Forecast | AccuWeather\\n   URL: https://www.accuweather.com/en/jp/tokyo/226396/weather-forecast/226396\\n   Summary: Tokyo,Tokyo, JapanWeatherForecast, with current conditions, wind, air quality, and what to expect for the next 3 days.\\n\\n2. Tokyo, Japan 14 day weather forecast - timeanddate.com\\n   URL: https://www.timeanddate.com/weather/japan/tokyo/ext\\n   Summary: 2 Week Extended Forecast inTokyo, Japan ... Hour-by-hourweatherforTokyonext 7 days\\n\\n3. Tokyo, Tokyo Prefecture, Japan - The Weather Channel\\n   URL: https://weather.com/weather/today/l/4ba28384e2da53b2861f5b5c70b7332e4ba1dc83e75b948e6fbd2aaceeeceae3\\n   Summary: Today's and tonight'sTokyo,TokyoPrefecture, Japanweatherforecast,weatherconditions and Doppler radar from TheWeatherChannel andWeather.com\\n\\n4. Tokyo - BBC Weather\\n   URL: https://www.bbc.com/weather/1850147\\n   Summary: 14-dayweatherforecast forTokyo.BBCWeatherinassociation with MeteoGroup All times are Japan Standard Time (Asia/Tokyo, GMT+9) unless otherwise stated.\\n\\n5. Tokyo, 13, JP 14 Days Weather - The Weather Network\\n   URL: https://www.theweathernetwork.com/en/city/jp/tokyo/tokyo/14-days\\n   Summary: Tokyo, 13, JP temperature trend for the next 14 Days. Find daytime highs and nighttime lows from TheWeatherNetwork.com.\\n\\n6. Tokyo, Japan Weather Forecast\\n   URL: https://www.weather-forecast.com/locations/Tokyo-1/forecasts/latest\\n   Summary: 12 dayTokyo, JapanWeatherForecast. LiveWeatherWarnings, hourlyweatherupdates. AccurateTokyoweathertoday, forecast for sun, rain, wind and temperature.\\n\\n7. Weather in Tokyo, Japan - 7-Day Forecast & Live Conditions\\n   URL: https://weather.now/jp/tokyo\\n   Summary: Get the latestweatherforecast forTokyo, Japan. View live conditions, a 7-day forecast, hourly updates, air quality index, and more.\\n\\n8. Tokyo Weather - Tokyo, JP\\n   URL: https://www.worldweatheronline.com/tokyo-weather/tokyo/jp.aspx\\n   Summary: LatestweatherinTokyo,Tokyo, Japan for today, tomorrow and the next 14 days. Get hourlyweatherup to 14 days, meteograms, radar maps, historicalweather, FAQ andweatheraverages.\\n\\n9. Tokyo Prefecture, Tokyo Prefecture, Japan Weather - The Weather Channel\\n   URL: https://weather.com/en-NA/weather/tenday/l/Tokyo+Prefecture+Tokyo+Prefecture+Japan?placeId=ed4da556fdab676b346a10ebd9a8fc2b2f612e1194009a72350620a6ee6dfae8\\n   Summary: Be prepared with the most accurate 10-day forecast forTokyoPrefecture,TokyoPrefecture, Japan with highs, lows, chance of precipitation from TheWeatherChannel andWeather.com\\n\\n10. Tokyo, Japan 10-Day Weather Forecast | Weather Underground\\n   URL: https://www.wunderground.com/forecast/jp/tokyo\\n   Summary: TokyoWeatherForecasts.WeatherUnderground provides local & long-rangeweatherforecasts, weatherreports, maps & tropicalweatherconditions for theTokyoarea.\\n\", tool_call_id='pyd_ai_422a8e6ad7394b2ca0d91c71751338c5', timestamp=datetime.datetime(2025, 9, 3, 22, 21, 25, 125615, tzinfo=datetime.timezone.utc))], instructions='You are a helpful assistant that can use the DuckDuckGo MCP server to search the web and obtain current information, you always search first and then fetch the content of the search results you think are relevant to the question, finally you answer the question based on the content of the search results'))\n",
      "CallToolsNode(model_response=ModelResponse(parts=[ToolCallPart(tool_name='fetch_content', args={'url': 'https://www.accuweather.com/en/jp/tokyo/226396/weather-forecast/226396'}, tool_call_id='pyd_ai_6809d8ad501f4ef8beb4dafcbf673bf8')], usage=RequestUsage(input_tokens=1176, output_tokens=109, details={'thoughts_tokens': 61, 'text_prompt_tokens': 1176}), model_name='gemini-2.5-flash', timestamp=datetime.datetime(2025, 9, 3, 22, 21, 26, 33135, tzinfo=datetime.timezone.utc), provider_name='google-gla', provider_details={'finish_reason': 'STOP'}, provider_response_id='Zr-4aMfWC_qcz7IPiOzE0Ak'))\n",
      "ModelRequestNode(request=ModelRequest(parts=[ToolReturnPart(tool_name='fetch_content', content='Tokyo, Tokyo, Japan Weather Forecast | AccuWeather Go Back For Business | Warnings Data Suite Forensics Advertising Superior Accuracy™ Tokyo, Tokyo 80°F Location Chevron down Location News Videos Use Current Location Recent Tokyo Tokyo 80° No results found. Try searching for a city, zip code or point of interest. settings Tokyo, Tokyo Weather Today WinterCast Local {stormName} Tracker Hourly Daily Radar MinuteCast® Monthly Air Quality Health & Activities Around the Globe Hurricane Tracker Severe Weather Radar & Maps News News & Features Astronomy Business Climate Health Recreation Sports Travel For Business Warnings Data Suite Forensics Advertising Superior Accuracy™ Video Today Hourly Daily Radar MinuteCast® Monthly Air Quality Health & Activities 3 Heavy Rain Warning for Inundation Today\\'s Weather Thu, Sep 4 Cloudy, very humid and not as hot; occasional rain in the afternoon Hi: 83° Tonight: A thunderstorm around in the evening; otherwise, cloudy and very humid Lo: 77° Current Weather 7:21 AM 80°F RealFeel® 91° Mostly cloudy More Details RealFeel Shade™ 88° Wind NE 7 mph Wind Gusts 10 mph Air Quality Fair Looking Ahead Thunderstorms, some heavy, Friday Tokyo Weather Radar Static Radar Temporarily Unavailable Thank you for your patience as we work to get everything up and running again. Refresh Page Clouds Temperature Hourly Weather Chevron left 8 AM 81° 7% 9 AM 82° 6% 10 AM 83° 5% 11 AM 82° 5% 12 PM 81° 8% 1 PM 81° 55% 2 PM 81° 61% 3 PM 80° 49% 4 PM 80° 27% 5 PM 80° 27% 6 PM 80° 29% 7 PM 79° 34% Chevron right 10-Day Weather Forecast Today 9/4 83° 77° Humid; a little p.m. rain Night: Humid; a t-storm around early 75% Fri 9/5 81° 75° Heavy afternoon thunderstorms A few evening showers; humid 98% Sat 9/6 88° 75° Partly sunny and humid Partly cloudy and humid 1% Sun 9/7 91° 77° Humid; breezy in the p.m. Clear, warm and very humid 2% Mon 9/8 95° 79° Mostly sunny, hot and humid Humid; a shower in spots late 6% Tue 9/9 91° 77° Partly sunny, hot and humid Mainly clear, warm and humid 25% Wed 9/10 79° 69° Not as warm; a p.m. t-storm Some rain and a thunderstorm 55% Thu 9/11 78° 70° Cloudy with a stray t-storm Some rain and a t-storm late 55% Fri 9/12 78° 70° A couple of morning showers Humid with a bit of rain 57% Sat 9/13 88° 74° Very warm with rain Humid with periods of rain 65% Sun & Moon 12 hrs 51 mins Rise 5:14 AM Set 6:05 PM Waxing Gibbous Rise 4:08 PM Set 2:00 AM Air Quality See More Air Quality Fair The air quality is generally acceptable for most individuals. However, sensitive groups may experience minor to moderate symptoms from long-term exposure. Allergy Outlook See All Dust & Dander Extreme Data is not supported in this location Top Stories Hurricane Hurricane Kiko to impact Hawaii with wind, rain before mid-September 3 hours ago Weather Forecasts Fall foliage 2025: Where to expect the best color 11 hours ago Weather News October-like chill to grip central US, cause temperatures to tumble 3 hours ago Hurricane Tropical activity to ramp up before peak of Atlantic hurricane season 3 hours ago Astronomy 85% of world population may see total lunar eclipse on Sunday 1 day ago More Stories Featured Stories Weather News At least 1,400 killed as earthquake rocks eastern Afghanistan 12 hours ago Recreation Powerball win or a lightning strike: Which has better odds? 9 hours ago Weather News Scientists have created rechargeable, multicolored, glow-in-the-dark s... 7 hours ago Weather News How the iconic Titanic discovery unfolded 1 day ago Live Blog Clearing the air: Leaves are falling but it\\'s not autumn yet LATEST ENTRY Why are there so many leaves falling? Is this an early autumn? 1 week ago World Asia Japan Tokyo Tokyo Weather Near Tokyo: Funabashi-shi, Chiba Hachioji-shi, Tokyo Kawasaki-shi, Kanagawa Company Proven Superior Accuracy™ About AccuWeather Digital Advertising Careers Press Contact Us Products & Services For Business For Partners For Advertising AccuWeather APIs AccuWeather Connect RealFeel® and RealFeel Shade™ Personal Weather Stations Apps & Downloads iPhone App Android App See all Apps & Downloads Subscription Services AccuWeather Premium AccuWeather Professional More AccuWeather Ready Business Health Hurricane Leisure and Recreation Severe Weather Space and Astronomy Sports Travel Weather News Company Proven Superior Accuracy™ About AccuWeather Digital Advertising Careers Press Contact Us Products & Services For Business For Partners For Advertising AccuWeather APIs AccuWeather Connect RealFeel® and RealFeel Shade™ Personal Weather Stations Apps & Downloads iPhone App Android App See all Apps & Downloads Subscription Services AccuWeather Premium AccuWeather Professional More AccuWeather Ready Business Health Hurricane Leisure and Recreation Severe Weather Space and Astronomy Sports Travel Weather News © 2025 AccuWeather, Inc. \"AccuWeather\" and sun design are registered trademarks of AccuWeather, Inc. All Rights Reserved. Terms of Use | Privacy Policy | Cookie Policy | About Your Privacy Do Not Sell or Share My Personal Information ... ... ... ... ...', tool_call_id='pyd_ai_6809d8ad501f4ef8beb4dafcbf673bf8', timestamp=datetime.datetime(2025, 9, 3, 22, 21, 26, 813660, tzinfo=datetime.timezone.utc))], instructions='You are a helpful assistant that can use the DuckDuckGo MCP server to search the web and obtain current information, you always search first and then fetch the content of the search results you think are relevant to the question, finally you answer the question based on the content of the search results'))\n",
      "CallToolsNode(model_response=ModelResponse(parts=[TextPart(content='The weather in Tokyo, Japan is currently 80°F and mostly cloudy with a RealFeel® of 91°F. There is a heavy rain warning for inundation. The wind is from the NE at 7 mph with gusts up to 10 mph. The air quality is fair.\\n\\nFor today, Thursday, September 4th, it will be cloudy, very humid, and not as hot, with occasional rain in the afternoon. The high will be 83°F. Tonight, there will be a thunderstorm around in the evening, otherwise, it will be cloudy and very humid with a low of 77°F.\\n\\nLooking ahead, there are heavy afternoon thunderstorms expected on Friday, September 5th, with a high of 81°F and a low of 75°F.')], usage=RequestUsage(input_tokens=2532, cache_read_tokens=872, output_tokens=172, details={'cached_content_tokens': 872, 'text_prompt_tokens': 2532, 'text_cache_tokens': 872}), model_name='gemini-2.5-flash', timestamp=datetime.datetime(2025, 9, 3, 22, 21, 27, 925434, tzinfo=datetime.timezone.utc), provider_name='google-gla', provider_details={'finish_reason': 'STOP'}, provider_response_id='aL-4aPiVBYzUz7IP8r7H6Q4'))\n",
      "End(data=FinalResult(output='The weather in Tokyo, Japan is currently 80°F and mostly cloudy with a RealFeel® of 91°F. There is a heavy rain warning for inundation. The wind is from the NE at 7 mph with gusts up to 10 mph. The air quality is fair.\\n\\nFor today, Thursday, September 4th, it will be cloudy, very humid, and not as hot, with occasional rain in the afternoon. The high will be 83°F. Tonight, there will be a thunderstorm around in the evening, otherwise, it will be cloudy and very humid with a low of 77°F.\\n\\nLooking ahead, there are heavy afternoon thunderstorms expected on Friday, September 5th, with a high of 81°F and a low of 75°F.'))\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai.mcp import MCPServerStdio\n",
    "\n",
    "ddg_server = MCPServerStdio(  \n",
    "    'uvx',\n",
    "    args=[\n",
    "        'duckduckgo-mcp-server',\n",
    "    ]\n",
    ")\n",
    "mcp_agent = Agent(settings.MODEL_NAME, toolsets=[ddg_server], \n",
    "                  instructions=\"You are a helpful assistant that can use the DuckDuckGo MCP server to search the web and obtain current information, you always search first and then fetch the content of the search results you think are relevant to the question, finally you answer the question based on the content of the search results\")  \n",
    "# Run the Agent\n",
    "async with mcp_agent:  \n",
    "    nodes = []\n",
    "    async with mcp_agent.iter(\n",
    "        \"What is the weather in Tokyo?\",\n",
    "    ) as agent_run:\n",
    "        async for node in agent_run:\n",
    "            # Each node represents a step in the agent's execution\n",
    "            print(node)\n",
    "            nodes.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48464966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in Tokyo, Japan is currently 80°F and mostly cloudy with a RealFeel® of 91°F. There is a heavy rain warning for inundation. The wind is from the NE at 7 mph with gusts up to 10 mph. The air quality is fair.\n",
      "\n",
      "For today, Thursday, September 4th, it will be cloudy, very humid, and not as hot, with occasional rain in the afternoon. The high will be 83°F. Tonight, there will be a thunderstorm around in the evening, otherwise, it will be cloudy and very humid with a low of 77°F.\n",
      "\n",
      "Looking ahead, there are heavy afternoon thunderstorms expected on Friday, September 5th, with a high of 81°F and a low of 75°F.\n"
     ]
    }
   ],
   "source": [
    "print(nodes[-1].data.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd60cab2",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we explored the fundamentals of creating AI agents using Pydantic AI:\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Simple Agents**: Created basic agents that can generate text responses with customizable instructions\n",
    "2. **Structured Output**: Used Pydantic models to ensure agents return well-formatted, typed data instead of plain text\n",
    "3. **Dependencies**: Implemented stateful agents that can maintain context (like previous jokes) between interactions\n",
    "4. **Tool Integration**: Built agents that can use external tools (random number generation, joke creation) to perform complex tasks\n",
    "5. **MCP Integration**: Connected agents to external services through Model Context Protocol (MCP) servers:\n",
    "   - **API MCPs**: Used Context7 for code documentation and library search\n",
    "   - **Command MCPs**: Used DuckDuckGo for real-time web search and content fetching\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "- Pydantic AI makes it easy to build reliable agents with structured outputs\n",
    "- Dependencies allow agents to maintain state and context across interactions\n",
    "- Tools enable agents to perform actions beyond text generation\n",
    "- MCP servers provide a standardized way to connect agents to external services\n",
    "- Agents can automatically chain tool calls to complete complex, multi-step tasks\n",
    "\n",
    "This foundation prepares us for building more sophisticated agent architectures in the following notebooks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
